{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f278c7",
   "metadata": {},
   "source": [
    "# Benchmarking models against SoDa-RoBERTa\n",
    "\n",
    "In this document we want to benchmark the performance of different models in comparisson to [SoDa-RoBERTa](https://github.com/source-data/soda-roberta) in different `token classification` tasks. \n",
    "\n",
    "The goal behind this experiment is not to find the best overall models, but those that are more accurate in classifying text entities of interest in the [SourceData](https://sourcedata.embo.org/) context. This means in the field of molecular cell biology. With this goal in mind, we will not use typical benchmarking datasets, but the [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) and [`sd-nlp-non-tokenized`](https://huggingface.co/datasets/EMBO/sd-nlp-non-tokenized) datasets. These datasets contain annotated image captions extracted from papers on molecular biology. The data has been curated and annotated by profesionals in the field. \n",
    "\n",
    "This notebook is intended to be used with the [🤗 Datasets](https://huggingface.co/) library. \n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [Chapter 1 - SoDa-RoBERTa](#chapter1)\n",
    "    * [Section 1.1 - NER task for SoDa-RoBERTa](#section_1_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for SoDa-RoBERTa](#section_1_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for SoDa-RoBERTa](#section_1_3)\n",
    "    * [Section 1.4 - PANELIZATION task for SoDa-RoBERTa](#section_1_4)\n",
    "    * [Section 1.5 - BORING task for SoDa-RoBERTa](#section_1_5)\n",
    "* [Chapter 2 - RoBERTa](#chapter2)\n",
    "    * [Section 1.1 - NER task for RoBERTa](#section_2_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for RoBERTa](#section_2_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for RoBERTa](#section_2_3)\n",
    "    * [Section 1.4 - PANELIZATION task for RoBERTa](#section_2_4)\n",
    "    * [Section 1.5 - BORING task for RoBERTa](#section_2_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cfdb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from smtag.config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78b7cd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.15.0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ffa2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a571a",
   "metadata": {},
   "source": [
    "# Chapter 1 - SoDa RoBERTa <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "[SoDa-RoBERTa](https://github.com/source-data/soda-roberta) [(Liechti. et al. 2017)](https://doi.org/10.1038/nmeth.4471) is a package generated by the [SourceData](https://sourcedata.embo.org/) team. This package has been developed to improve the data curation of biomedical papers in the field of molecular and cell biology.\n",
    "\n",
    "This is the first model that we will use in our benchmarking. The data available in [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) has already been tokenized using the 🤗 `roberta-base` tokenizer. This tokenizer has been pre-trained with the `roberta-base` model, which is the base model on top of which SoDa-RoBERTa has been built. \n",
    "\n",
    "Since the model is already pre-trained, we just need to fine-tune it. The basic idea is that the pre-trained model with generate a series of outputs that will be token encoders. By fine-tuning a model, FFNN is added on the top of these embeddings and connected to a `softmax` layer to classify tokens.\n",
    "\n",
    "This process is mostly automated to us by the [🤗 `AutoModelForTokenClassification`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForTokenClassification) class.\n",
    "\n",
    "# Section 1.1 - NER task for SoDa-RoBERTa <a class=\"anchor\" id=\"section_1_1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7ccd929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dc426bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a57eec257a4eac8d8d7ee3d3daa0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0,\n",
       "   1640,\n",
       "   347,\n",
       "   43,\n",
       "   4052,\n",
       "   847,\n",
       "   33101,\n",
       "   43916,\n",
       "   14868,\n",
       "   303,\n",
       "   129,\n",
       "   15,\n",
       "   15145,\n",
       "   36,\n",
       "   571,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   8,\n",
       "   1368,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   43,\n",
       "   15,\n",
       "   36475,\n",
       "   36,\n",
       "   118,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   43,\n",
       "   9,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   4,\n",
       "   735,\n",
       "   2549,\n",
       "   58,\n",
       "   1455,\n",
       "   129,\n",
       "   15,\n",
       "   5,\n",
       "   5856,\n",
       "   9,\n",
       "   5,\n",
       "   155,\n",
       "   1484,\n",
       "   25,\n",
       "   22827,\n",
       "   13,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   36,\n",
       "   267,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   322,\n",
       "   1437,\n",
       "   2],\n",
       "  [0,\n",
       "   28588,\n",
       "   3693,\n",
       "   3041,\n",
       "   44193,\n",
       "   40899,\n",
       "   16007,\n",
       "   21258,\n",
       "   2018,\n",
       "   5,\n",
       "   127,\n",
       "   523,\n",
       "   12572,\n",
       "   3551,\n",
       "   5252,\n",
       "   11,\n",
       "   364,\n",
       "   771,\n",
       "   2571,\n",
       "   9,\n",
       "   545,\n",
       "   12,\n",
       "   3583,\n",
       "   12,\n",
       "   279,\n",
       "   15540,\n",
       "   9789,\n",
       "   15,\n",
       "   10,\n",
       "   239,\n",
       "   12,\n",
       "   19987,\n",
       "   5626,\n",
       "   36,\n",
       "   725,\n",
       "   24667,\n",
       "   43,\n",
       "   36,\n",
       "   4070,\n",
       "   43,\n",
       "   8,\n",
       "   5,\n",
       "   12337,\n",
       "   11257,\n",
       "   5656,\n",
       "   36,\n",
       "   6960,\n",
       "   322,\n",
       "   20,\n",
       "   2853,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   11579,\n",
       "   1978,\n",
       "   12,\n",
       "   38683,\n",
       "   401,\n",
       "   534,\n",
       "   12,\n",
       "   4590,\n",
       "   4,\n",
       "   20,\n",
       "   1692,\n",
       "   9217,\n",
       "   311,\n",
       "   5,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   438,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   3592,\n",
       "   13418,\n",
       "   21130,\n",
       "   3443,\n",
       "   8,\n",
       "   10888,\n",
       "   401,\n",
       "   347,\n",
       "   4411,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   2544,\n",
       "   4590,\n",
       "   6,\n",
       "   4067,\n",
       "   4,\n",
       "   20,\n",
       "   795,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   10888,\n",
       "   401,\n",
       "   534,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   746,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   40936,\n",
       "   36,\n",
       "   282,\n",
       "   5457,\n",
       "   290,\n",
       "   15540,\n",
       "   322,\n",
       "   1437,\n",
       "   2]],\n",
       " 'labels': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   9,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'tag_mask': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"drAbreu/sd-nlp-2\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "train_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d879d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c4ab1",
   "metadata": {},
   "source": [
    "Each of the different models we are using will use different configurations for training. We will generate them using the `config_dict` variable in the module `config.py`.\n",
    "\n",
    "This information contains topics as important as the model checkpoints to be used. \n",
    "\n",
    "SoDa-RoBERTa has generated a language model [`bio-lm`](https://huggingface.co/EMBO/bio-lm). This model has been initialized from the `roberta-base` checkpoint. It is for this reason that the tokenizer to be used is that of `roberta-base`.\n",
    "\n",
    "In the next line we will load the `bio-lm` checkpoint and the `roberta-base` tokenizer.\n",
    "\n",
    "The dataset for `sd-nlp` we have the data ready to be processed. The next step would be to organize the data into a way that it can be load into batches.\n",
    "\n",
    "This is done with data collators. There is a generic data collator known as `DataCollatorForTokenClassification` in  🤗 that will do what we need. However, we have a `DataCollatorForMaskedTokenClassification` generated that uses the `tag_mask` column to randomly mask the values. This was done by Thomas. I am assuming the reason behind was to improve the generalization of the task. But this needs to be checked  with him.\n",
    "\n",
    "After checking, it looks like for some reason only the `DataCollatorForMaskedTokenClassification` will work here, so let's keep it as it is and move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8957738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "# Check the case of the MaskedTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "config = Config(model_type = \"Autoencoder\", from_pretrained = \"EMBO/bio-lm\", tokenizer = 'roberta-base')\n",
    "data_collator_mask = DataCollatorForMaskedTokenClassification(tokenizer=AutoTokenizer.from_pretrained(config.tokenizer), \n",
    "                                                              padding=True,\n",
    "                                                              max_length=512,\n",
    "                                                              pad_to_multiple_of=None,\n",
    "                                                              return_tensors='pt',\n",
    "                                                              masking_probability=0.0,\n",
    "                                                              replacement_probability=0.0,\n",
    "                                                              select_labels=False)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                                   padding=True,\n",
    "                                                   return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f849770d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1640,   347,    43,  4052,   847, 33101, 43916, 14868,   303,\n",
       "            129,    15, 15145,    36,   571,    35,   361, 25610,     8,  1368,\n",
       "             35,   545, 25610,    43,    15, 36475,    36,   118,    35,   361,\n",
       "          25610,    43,     9,  3186,  3082,     4,   306,     4,   735,  2549,\n",
       "             58,  1455,   129,    15,     5,  5856,     9,     5,   155,  1484,\n",
       "             25, 22827,    13,  3186,  3082,     4,   306,    36,   267,    35,\n",
       "            545, 25610,   322,  1437,     2]]),\n",
       " 'labels': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,\n",
       "           0,  0, 10,  9,  0,  0,  0,  0,  0, 10,  0,  0,  0, 12,  0,  0,  0, 12,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 'tag_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1\n",
    "batch = data_collator([data[\"train\"][i] for i in range(batch_size)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b5171ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1640,   347,    43,  4052,   847, 33101, 43916, 14868,   303,\n",
       "            129,    15, 15145,    36,   571,    35,   361, 25610,     8,  1368,\n",
       "             35,   545, 25610,    43,    15, 36475,    36,   118,    35,   361,\n",
       "          25610,    43,     9,  3186,  3082,     4,   306,     4,   735,  2549,\n",
       "             58,  1455,   129,    15,     5,  5856,     9,     5,   155,  1484,\n",
       "             25, 22827,    13,  3186,  3082,     4,   306,    36,   267,    35,\n",
       "            545, 25610,   322,  1437,     2]]),\n",
       " 'labels': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,\n",
       "           0,  0, 10,  9,  0,  0,  0,  0,  0, 10,  0,  0,  0, 12,  0,  0,  0, 12,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1\n",
    "batch = data_collator_mask([data[\"train\"][i] for i in range(batch_size)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae8ba7",
   "metadata": {},
   "source": [
    "After the data collector is time to define the hyperparameters needed by the `Trainer` class of 🤗. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d278e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "LM_MODEL_PATH = os.getenv('LM_MODEL_PATH')\n",
    "TOKENIZER_PATH = os.getenv('TOKENIZER_PATH')\n",
    "TOKCL_MODEL_PATH = os.getenv('TOKCL_MODEL_PATH')\n",
    "CACHE = os.getenv('CACHE')\n",
    "RUNS_DIR = os.getenv('RUNS_DIR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdb5c4",
   "metadata": {},
   "source": [
    "We need to do also a series of important definitions at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "104b6da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 15 features:\n",
      "O, I-SMALL_MOLECULE, B-SMALL_MOLECULE, I-GENEPROD, B-GENEPROD, I-SUBCELLULAR, B-SUBCELLULAR, I-CELL, B-CELL, I-TISSUE, B-TISSUE, I-ORGANISM, B-ORGANISM, I-EXP_ASSAY, B-EXP_ASSAY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'I-SMALL_MOLECULE': 1,\n",
       " 'B-SMALL_MOLECULE': 2,\n",
       " 'I-GENEPROD': 3,\n",
       " 'B-GENEPROD': 4,\n",
       " 'I-SUBCELLULAR': 5,\n",
       " 'B-SUBCELLULAR': 6,\n",
       " 'I-CELL': 7,\n",
       " 'B-CELL': 8,\n",
       " 'I-TISSUE': 9,\n",
       " 'B-TISSUE': 10,\n",
       " 'I-ORGANISM': 11,\n",
       " 'B-ORGANISM': 12,\n",
       " 'I-EXP_ASSAY': 13,\n",
       " 'B-EXP_ASSAY': 14}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = train_dataset.info.features['labels'].feature.num_classes\n",
    "label_list = train_dataset.info.features['labels'].feature.names\n",
    "id2label, label2id = {}, {}\n",
    "for class_, label in zip(range(num_labels), label_list):\n",
    "    id2label[class_] = label \n",
    "    label2id[label] = class_ \n",
    "print(f\"\\nTraining on {num_labels} features:\")\n",
    "print(\", \".join(label_list))\n",
    "id2label\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d05f9",
   "metadata": {},
   "source": [
    "Let us define now the metrics that will be used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6377c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.metrics import MetricsTOKCL\n",
    "compute_metrics = MetricsTOKCL(label_list=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e22d6697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='/tokcl_models', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='/tokcl_models/runs/May20_08-45-53_5719849be9d3', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1000, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=1, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='/tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=True, resume_from_checkpoint=None, hub_model_id='EMBO/SourceData-NER', hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=False, gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=None, replacement_probability=None, select_labels=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL\n",
    "\n",
    "training_args = TrainingArgumentsTOKCL(\n",
    "    output_dir = TOKCL_MODEL_PATH,\n",
    "    overwrite_output_dir = True,\n",
    "    logging_steps = 1000,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    prediction_loss_only = True,  # crucial to avoid OOM at evaluation stage!\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 10,\n",
    "    masking_probability = None,\n",
    "    replacement_probability = None,\n",
    "    select_labels = False,\n",
    "    per_gpu_train_batch_size=None, \n",
    "    per_gpu_eval_batch_size=None, \n",
    "    gradient_accumulation_steps=1, \n",
    "    eval_accumulation_steps=None, \n",
    "    weight_decay=0.0, \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.999, \n",
    "    adam_epsilon=1e-08, \n",
    "    max_grad_norm=1.0, \n",
    "    max_steps=-1, \n",
    "    lr_scheduler_type='linear', \n",
    "    warmup_ratio=0.0, \n",
    "    warmup_steps=0, \n",
    "    save_strategy='epoch', \n",
    "    save_steps=1, \n",
    "    save_total_limit=5, \n",
    "    save_on_each_node=False, \n",
    "    no_cuda=False, \n",
    "    seed=42, \n",
    "    bf16=False, \n",
    "    fp16=False, \n",
    "    fp16_opt_level='O1', \n",
    "    half_precision_backend='auto', \n",
    "    bf16_full_eval=False, \n",
    "    fp16_full_eval=False, \n",
    "    tf32=None, \n",
    "    local_rank=-1, \n",
    "    xpu_backend=None, \n",
    "    tpu_num_cores=None, \n",
    "    tpu_metrics_debug=False, \n",
    "    debug=[], \n",
    "    dataloader_drop_last=False, \n",
    "    eval_steps=1000, \n",
    "    dataloader_num_workers=0, \n",
    "    past_index=-1, \n",
    "    run_name=TOKCL_MODEL_PATH, \n",
    "    disable_tqdm=False, \n",
    "    remove_unused_columns=True, \n",
    "    label_names=None, \n",
    "    load_best_model_at_end=False, \n",
    "    metric_for_best_model=None, \n",
    "    greater_is_better=None, \n",
    "    ignore_data_skip=False, \n",
    "    sharded_ddp=[], \n",
    "    deepspeed=None, \n",
    "    label_smoothing_factor=0.0, \n",
    "    adafactor=False, \n",
    "    group_by_length=False, \n",
    "    length_column_name='length', \n",
    "    report_to=['tensorboard'], \n",
    "    ddp_find_unused_parameters=None, \n",
    "    ddp_bucket_cap_mb=None, \n",
    "    dataloader_pin_memory=True, \n",
    "    skip_memory_metrics=True, \n",
    "    use_legacy_prediction_loop=False, \n",
    "    push_to_hub=True, \n",
    "    resume_from_checkpoint=None, \n",
    "    hub_model_id=\"EMBO/SourceData-NER\", \n",
    "    hub_strategy='every_save', \n",
    "    hub_token=False, \n",
    "    gradient_checkpointing=False, \n",
    "    fp16_backend='auto', \n",
    "    mp_parameters=''\n",
    "    )\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757d350",
   "metadata": {},
   "source": [
    "Up to now, we have pre-processed data and load a model. The loaded model has been cropped at the transformer network. For the model to be able to perform a task, we need to provide the model with a model head. \n",
    "\n",
    "The model heads are usually fully connected layers on the top of the transformer network. Although at this point we could perfectly use `torch` to build our own model from the output of the transformers, it has been shown that the performance of fully connected layers is at this point good enough to perform several NLP tasks, including NER.\n",
    "\n",
    "The reason is that the transformer models already encodes several context information on its resulting embeddings. We would therefore not benefit from generating a second RNN or conditional random fields on top, as it was usually done for NER. We will therefore keep it simple and use the fully connected network provided by 🤗.\n",
    "\n",
    "The way to do so is to load our model, but now using a different class: `AutoModelForTokenClassification`. In this case we use token classification since NER belongs to this task. \n",
    "\n",
    "We need to pass the number of labels. To avoid doing this for every single checkpoint we can do it programatically. The way of getting the number of classes from the training dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1a43353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "            config.from_pretrained,\n",
    "            num_labels=num_labels,\n",
    "            max_position_embeddings=config.max_length + 2,  # max_length + 2 for start/end token\n",
    "            id2label = id2label,\n",
    "            label2id = label2id\n",
    "        )\n",
    "model_config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "711022d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-SMALL_MOLECULE\",\n",
      "    \"2\": \"B-SMALL_MOLECULE\",\n",
      "    \"3\": \"I-GENEPROD\",\n",
      "    \"4\": \"B-GENEPROD\",\n",
      "    \"5\": \"I-SUBCELLULAR\",\n",
      "    \"6\": \"B-SUBCELLULAR\",\n",
      "    \"7\": \"I-CELL\",\n",
      "    \"8\": \"B-CELL\",\n",
      "    \"9\": \"I-TISSUE\",\n",
      "    \"10\": \"B-TISSUE\",\n",
      "    \"11\": \"I-ORGANISM\",\n",
      "    \"12\": \"B-ORGANISM\",\n",
      "    \"13\": \"I-EXP_ASSAY\",\n",
      "    \"14\": \"B-EXP_ASSAY\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CELL\": 8,\n",
      "    \"B-EXP_ASSAY\": 14,\n",
      "    \"B-GENEPROD\": 4,\n",
      "    \"B-ORGANISM\": 12,\n",
      "    \"B-SMALL_MOLECULE\": 2,\n",
      "    \"B-SUBCELLULAR\": 6,\n",
      "    \"B-TISSUE\": 10,\n",
      "    \"I-CELL\": 7,\n",
      "    \"I-EXP_ASSAY\": 13,\n",
      "    \"I-GENEPROD\": 3,\n",
      "    \"I-ORGANISM\": 11,\n",
      "    \"I-SMALL_MOLECULE\": 1,\n",
      "    \"I-SUBCELLULAR\": 5,\n",
      "    \"I-TISSUE\": 9,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=EMBO/SourceData-NER,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tokcl_models/runs/May20_08-45-53_5719849be9d3,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=None,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=/tokcl_models,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "replacement_probability=None,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=1,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining arguments for model type {config.model_type}:\")\n",
    "print(model_config)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda3105",
   "metadata": {},
   "source": [
    "#### Training Step 3: Define the `Trainer`\n",
    "\n",
    "We are ready now to define the [`Trainer` class](https://🤗.co/docs/transformers/main_classes/trainer). This class is a basic training loop supporting a series of features defined in the documentation. However, it can be further customized. We encourage you to take a look to the documentation and try it. \n",
    "\n",
    "As it is, `trainer.train` would already train our model. However, it would offer only information about the loss during the process. We know that we want the loss to get smaller with time, and ideally, that this is true for both, training and validation datasets. Otherwise we would be incurring in overfitting.\n",
    "\n",
    "What if we want to see other information during training like the accuracy or f1 score? `Trainer` provides an argument `compute_metrics` that will help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "403bed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/EMBO/SourceData-NER into local empty directory.\n",
      "WARNING:Cloning https://huggingface.co/EMBO/SourceData-NER into local empty directory.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '/tokcl_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     subprocess.run(\n\u001b[0m\u001b[1;32m    703\u001b[0m                         \u001b[0;34mf\"{'git clone' if self.skip_lfs_files else 'git lfs clone'} {repo_url} .\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'lfs', 'clone', 'https://huggingface.co/EMBO/SourceData-NER', '.']' returned non-zero exit status 2.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m             self.repo = Repository(\n\u001b[0m\u001b[1;32m   2599\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclone_from\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: WARNING: 'git lfs clone' is deprecated and will not be updated\n          with new flags from 'git clone'\n\n'git clone' has been updated in upstream Git to have comparable\nspeeds to 'git lfs clone'.\nCloning into '.'...\nremote: Repository not found\nfatal: repository 'https://huggingface.co/EMBO/SourceData-NER/' not found\nError(s) during clone:\ngit clone failed: exit status 128\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/2379092524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoardCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# Create clone of distant repo and output directory if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;31m# In case of pull, we need to make sure every process has the latest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m                 \u001b[0;31m# Try again after wiping output_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2606\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2607\u001b[0m                 self.repo = Repository(\n\u001b[1;32m   2608\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    720\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '/tokcl_models'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ShowExampleTOKCL(AutoTokenizer.from_pretrained(config.tokenizer))]\n",
    ")\n",
    "\n",
    "# switch the Tensorboard callback to plot losses on same plot\n",
    "trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abb978",
   "metadata": {},
   "source": [
    "## Train the models using the general tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328a4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from smtag.metrics import MetricsTOKCL\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from smtag.config import Config\n",
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32a97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_label(label):\n",
    "    # If the label is B-XXX we change it to I-XX\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    Expands the NER tags once the sub-word tokenization is added.\n",
    "    Arguments\n",
    "    ---------\n",
    "    labels list[int]:\n",
    "    word_ids list[int]\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            # As far as word_id matches the index of the current word\n",
    "            # We append the same label\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            new_labels.append(shift_label(labels[word_id]))\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['words'], \n",
    "                       truncation=True,\n",
    "                       is_split_into_words=True)\n",
    "    \n",
    "    all_labels = examples['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e73ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\"bert-cased\": \"bert-base-cased\", # working\n",
    "               \"bert-uncased\": \"bert-base-uncased\", # working\n",
    "              \"biobert\": \"dmis-lab/biobert-base-cased-v1.1\", # working\n",
    "              \"pubmedbert\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"} # Working on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93407d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "WARNING:Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8add818ecd4c5ba0d577be1b0d4def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Loading cached processed dataset at /root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581/cache-179a23aafb0e5b9b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea480391d76c4bddb8fbf368750ab60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "WARNING:Loading cached processed dataset at /root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581/cache-c4f3852bd5830ff7.arrow\n"
     ]
    }
   ],
   "source": [
    "checkpoint = checkpoints[\"biobert\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "data = load_dataset(\"EMBO/sd-nlp-non-tokenized\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "train_dataset[0:2]\n",
    "\n",
    "tokenized_data = data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=data['train'].column_names)#,\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                        return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953c3b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d03dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=IntervalStrategy.NO,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=dmis-lab/biobert-base-cased-v1.1/runs/May20_12-04-59_5719849be9d3,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "output_dir=dmis-lab/biobert-base-cased-v1.1,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=dmis-lab/biobert-base-cased-v1.1,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c024eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ['O', 'I-SMALL_MOLECULE', 'B-SMALL_MOLECULE', 'I-GENEPROD', 'B-GENEPROD', 'I-SUBCELLULAR', 'B-SUBCELLULAR', 'I-CELL', 'B-CELL', 'I-TISSUE', 'B-TISSUE', 'I-ORGANISM', 'B-ORGANISM', 'I-EXP_ASSAY', 'B-EXP_ASSAY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'I-SMALL_MOLECULE',\n",
       " 2: 'B-SMALL_MOLECULE',\n",
       " 3: 'I-GENEPROD',\n",
       " 4: 'B-GENEPROD',\n",
       " 5: 'I-SUBCELLULAR',\n",
       " 6: 'B-SUBCELLULAR',\n",
       " 7: 'I-CELL',\n",
       " 8: 'B-CELL',\n",
       " 9: 'I-TISSUE',\n",
       " 10: 'B-TISSUE',\n",
       " 11: 'I-ORGANISM',\n",
       " 12: 'B-ORGANISM',\n",
       " 13: 'I-EXP_ASSAY',\n",
       " 14: 'B-EXP_ASSAY'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def define_labels(dataset):\n",
    "    num_labels = dataset.info.features['labels'].feature.num_classes\n",
    "    label_list = dataset.info.features['labels'].feature.names\n",
    "    id2label, label2id = {}, {}\n",
    "    print(num_labels, label_list)\n",
    "    for class_, label in zip(range(num_labels), label_list):\n",
    "        id2label[class_] = label \n",
    "        label2id[label] = class_ \n",
    "    return id2label, label2id\n",
    "id2label, label2id = define_labels(tokenized_data['train'])\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c653e09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-SMALL_MOLECULE\",\n",
      "    \"2\": \"B-SMALL_MOLECULE\",\n",
      "    \"3\": \"I-GENEPROD\",\n",
      "    \"4\": \"B-GENEPROD\",\n",
      "    \"5\": \"I-SUBCELLULAR\",\n",
      "    \"6\": \"B-SUBCELLULAR\",\n",
      "    \"7\": \"I-CELL\",\n",
      "    \"8\": \"B-CELL\",\n",
      "    \"9\": \"I-TISSUE\",\n",
      "    \"10\": \"B-TISSUE\",\n",
      "    \"11\": \"I-ORGANISM\",\n",
      "    \"12\": \"B-ORGANISM\",\n",
      "    \"13\": \"I-EXP_ASSAY\",\n",
      "    \"14\": \"B-EXP_ASSAY\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CELL\": 8,\n",
      "    \"B-EXP_ASSAY\": 14,\n",
      "    \"B-GENEPROD\": 4,\n",
      "    \"B-ORGANISM\": 12,\n",
      "    \"B-SMALL_MOLECULE\": 2,\n",
      "    \"B-SUBCELLULAR\": 6,\n",
      "    \"B-TISSUE\": 10,\n",
      "    \"I-CELL\": 7,\n",
      "    \"I-EXP_ASSAY\": 13,\n",
      "    \"I-GENEPROD\": 3,\n",
      "    \"I-ORGANISM\": 11,\n",
      "    \"I-SMALL_MOLECULE\": 1,\n",
      "    \"I-SUBCELLULAR\": 5,\n",
      "    \"I-TISSUE\": 9,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphwdj30zi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f665cd924e451aad6929f48297ea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n",
      "creating metadata file for /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n",
      "loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(list(id2label.keys())),\n",
    "    max_position_embeddings=max_length,  \n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")\n",
    "model_config = model.config\n",
    "compute_metrics = MetricsTOKCL(label_list=list(label2id.keys()))\n",
    "       \n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# switch the Tensorboard callback to plot losses on same plot\n",
    "trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "#trainer.train()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ab3f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smtag.metrics.MetricsTOKCL"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(MetricsTOKCL())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ddbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "Trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5987ad",
   "metadata": {},
   "source": [
    "# Loading BioMegatron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfe5d157",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory data/models/biomegatron/ or `from_tf` and `from_flax` set to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_377/126808532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"data/models/biomegatron/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         raise ValueError(\n\u001b[1;32m    443\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0marchive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                     raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1299\u001b[0m                         \u001b[0;34mf\"Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                         \u001b[0;34mf\"directory {pretrained_model_name_or_path} or `from_tf` and `from_flax` set to False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory data/models/biomegatron/ or `from_tf` and `from_flax` set to False."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"data/models/biomegatron/\",\n",
    "    from_tf=False,\n",
    "    from_flax=False\n",
    "#     num_labels=len(list(id2label.keys())),\n",
    "#     max_position_embeddings=max_length,  \n",
    "#     id2label = id2label,\n",
    "#     label2id = label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e24117f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'str'> for this kind of AutoModel: AutoModel.\nModel type should be one of ImageGPTConfig, QDQBertConfig, FNetConfig, SegformerConfig, VisionTextDualEncoderConfig, PerceiverConfig, GPTJConfig, LayoutLMv2Config, BeitConfig, RemBertConfig, VisualBertConfig, CanineConfig, RoFormerConfig, CLIPConfig, BigBirdPegasusConfig, DeiTConfig, LukeConfig, DetrConfig, GPTNeoConfig, BigBirdConfig, Speech2TextConfig, ViTConfig, Wav2Vec2Config, M2M100Config, ConvBertConfig, LEDConfig, BlenderbotSmallConfig, RetriBertConfig, IBertConfig, MT5Config, T5Config, MobileBertConfig, DistilBertConfig, AlbertConfig, BertGenerationConfig, CamembertConfig, XLMRobertaConfig, PegasusConfig, MarianConfig, MBartConfig, MegatronBertConfig, MPNetConfig, BartConfig, BlenderbotConfig, ReformerConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, FSMTConfig, SqueezeBertConfig, HubertConfig, BertConfig, OpenAIGPTConfig, GPT2Config, TransfoXLConfig, XLNetConfig, XLMProphetNetConfig, ProphetNetConfig, XLMConfig, CTRLConfig, ElectraConfig, FunnelConfig, LxmertConfig, DPRConfig, LayoutLMConfig, TapasConfig, SplinterConfig, SEWDConfig, SEWConfig, UniSpeechSatConfig, UniSpeechConfig, WavLMConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/3779020004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMegatronBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/models/biomegatron/config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m weights = torch.load('data/models/biomegatron/MegatronBERT.pt', \n\u001b[1;32m      5\u001b[0m                                  map_location=torch.device('cpu'))\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'str'> for this kind of AutoModel: AutoModel.\nModel type should be one of ImageGPTConfig, QDQBertConfig, FNetConfig, SegformerConfig, VisionTextDualEncoderConfig, PerceiverConfig, GPTJConfig, LayoutLMv2Config, BeitConfig, RemBertConfig, VisualBertConfig, CanineConfig, RoFormerConfig, CLIPConfig, BigBirdPegasusConfig, DeiTConfig, LukeConfig, DetrConfig, GPTNeoConfig, BigBirdConfig, Speech2TextConfig, ViTConfig, Wav2Vec2Config, M2M100Config, ConvBertConfig, LEDConfig, BlenderbotSmallConfig, RetriBertConfig, IBertConfig, MT5Config, T5Config, MobileBertConfig, DistilBertConfig, AlbertConfig, BertGenerationConfig, CamembertConfig, XLMRobertaConfig, PegasusConfig, MarianConfig, MBartConfig, MegatronBertConfig, MPNetConfig, BartConfig, BlenderbotConfig, ReformerConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, FSMTConfig, SqueezeBertConfig, HubertConfig, BertConfig, OpenAIGPTConfig, GPT2Config, TransfoXLConfig, XLNetConfig, XLMProphetNetConfig, ProphetNetConfig, XLMConfig, CTRLConfig, ElectraConfig, FunnelConfig, LxmertConfig, DPRConfig, LayoutLMConfig, TapasConfig, SplinterConfig, SEWDConfig, SEWConfig, UniSpeechSatConfig, UniSpeechConfig, WavLMConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, MegatronBertConfig\n",
    "import torch\n",
    "model = AutoModel.from_config('data/models/biomegatron/config.json')\n",
    "weights = torch.load('data/models/biomegatron/MegatronBERT.pt', \n",
    "                                 map_location=torch.device('cpu'))\n",
    "model.load_state_dict(weights['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc4a1a85",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MegatronBertModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"encoder.layer.0.attention.ln.weight\", \"encoder.layer.0.attention.ln.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.ln.weight\", \"encoder.layer.0.ln.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.1.attention.ln.weight\", \"encoder.layer.1.attention.ln.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.ln.weight\", \"encoder.layer.1.ln.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.2.attention.ln.weight\", \"encoder.layer.2.attention.ln.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.ln.weight\", \"encoder.layer.2.ln.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.3.attention.ln.weight\", \"encoder.layer.3.attention.ln.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.ln.weight\", \"encoder.layer.3.ln.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.4.attention.ln.weight\", \"encoder.layer.4.attention.ln.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.ln.weight\", \"encoder.layer.4.ln.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.5.attention.ln.weight\", \"encoder.layer.5.attention.ln.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.ln.weight\", \"encoder.layer.5.ln.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.6.attention.ln.weight\", \"encoder.layer.6.attention.ln.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.ln.weight\", \"encoder.layer.6.ln.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.7.attention.ln.weight\", \"encoder.layer.7.attention.ln.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.ln.weight\", \"encoder.layer.7.ln.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.8.attention.ln.weight\", \"encoder.layer.8.attention.ln.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.ln.weight\", \"encoder.layer.8.ln.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.9.attention.ln.weight\", \"encoder.layer.9.attention.ln.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.ln.weight\", \"encoder.layer.9.ln.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.10.attention.ln.weight\", \"encoder.layer.10.attention.ln.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.ln.weight\", \"encoder.layer.10.ln.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.11.attention.ln.weight\", \"encoder.layer.11.attention.ln.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.ln.weight\", \"encoder.layer.11.ln.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.12.attention.ln.weight\", \"encoder.layer.12.attention.ln.bias\", \"encoder.layer.12.attention.self.query.weight\", \"encoder.layer.12.attention.self.query.bias\", \"encoder.layer.12.attention.self.key.weight\", \"encoder.layer.12.attention.self.key.bias\", \"encoder.layer.12.attention.self.value.weight\", \"encoder.layer.12.attention.self.value.bias\", \"encoder.layer.12.attention.output.dense.weight\", \"encoder.layer.12.attention.output.dense.bias\", \"encoder.layer.12.ln.weight\", \"encoder.layer.12.ln.bias\", \"encoder.layer.12.intermediate.dense.weight\", \"encoder.layer.12.intermediate.dense.bias\", \"encoder.layer.12.output.dense.weight\", \"encoder.layer.12.output.dense.bias\", \"encoder.layer.13.attention.ln.weight\", \"encoder.layer.13.attention.ln.bias\", \"encoder.layer.13.attention.self.query.weight\", \"encoder.layer.13.attention.self.query.bias\", \"encoder.layer.13.attention.self.key.weight\", \"encoder.layer.13.attention.self.key.bias\", \"encoder.layer.13.attention.self.value.weight\", \"encoder.layer.13.attention.self.value.bias\", \"encoder.layer.13.attention.output.dense.weight\", \"encoder.layer.13.attention.output.dense.bias\", \"encoder.layer.13.ln.weight\", \"encoder.layer.13.ln.bias\", \"encoder.layer.13.intermediate.dense.weight\", \"encoder.layer.13.intermediate.dense.bias\", \"encoder.layer.13.output.dense.weight\", \"encoder.layer.13.output.dense.bias\", \"encoder.layer.14.attention.ln.weight\", \"encoder.layer.14.attention.ln.bias\", \"encoder.layer.14.attention.self.query.weight\", \"encoder.layer.14.attention.self.query.bias\", \"encoder.layer.14.attention.self.key.weight\", \"encoder.layer.14.attention.self.key.bias\", \"encoder.layer.14.attention.self.value.weight\", \"encoder.layer.14.attention.self.value.bias\", \"encoder.layer.14.attention.output.dense.weight\", \"encoder.layer.14.attention.output.dense.bias\", \"encoder.layer.14.ln.weight\", \"encoder.layer.14.ln.bias\", \"encoder.layer.14.intermediate.dense.weight\", \"encoder.layer.14.intermediate.dense.bias\", \"encoder.layer.14.output.dense.weight\", \"encoder.layer.14.output.dense.bias\", \"encoder.layer.15.attention.ln.weight\", \"encoder.layer.15.attention.ln.bias\", \"encoder.layer.15.attention.self.query.weight\", \"encoder.layer.15.attention.self.query.bias\", \"encoder.layer.15.attention.self.key.weight\", \"encoder.layer.15.attention.self.key.bias\", \"encoder.layer.15.attention.self.value.weight\", \"encoder.layer.15.attention.self.value.bias\", \"encoder.layer.15.attention.output.dense.weight\", \"encoder.layer.15.attention.output.dense.bias\", \"encoder.layer.15.ln.weight\", \"encoder.layer.15.ln.bias\", \"encoder.layer.15.intermediate.dense.weight\", \"encoder.layer.15.intermediate.dense.bias\", \"encoder.layer.15.output.dense.weight\", \"encoder.layer.15.output.dense.bias\", \"encoder.layer.16.attention.ln.weight\", \"encoder.layer.16.attention.ln.bias\", \"encoder.layer.16.attention.self.query.weight\", \"encoder.layer.16.attention.self.query.bias\", \"encoder.layer.16.attention.self.key.weight\", \"encoder.layer.16.attention.self.key.bias\", \"encoder.layer.16.attention.self.value.weight\", \"encoder.layer.16.attention.self.value.bias\", \"encoder.layer.16.attention.output.dense.weight\", \"encoder.layer.16.attention.output.dense.bias\", \"encoder.layer.16.ln.weight\", \"encoder.layer.16.ln.bias\", \"encoder.layer.16.intermediate.dense.weight\", \"encoder.layer.16.intermediate.dense.bias\", \"encoder.layer.16.output.dense.weight\", \"encoder.layer.16.output.dense.bias\", \"encoder.layer.17.attention.ln.weight\", \"encoder.layer.17.attention.ln.bias\", \"encoder.layer.17.attention.self.query.weight\", \"encoder.layer.17.attention.self.query.bias\", \"encoder.layer.17.attention.self.key.weight\", \"encoder.layer.17.attention.self.key.bias\", \"encoder.layer.17.attention.self.value.weight\", \"encoder.layer.17.attention.self.value.bias\", \"encoder.layer.17.attention.output.dense.weight\", \"encoder.layer.17.attention.output.dense.bias\", \"encoder.layer.17.ln.weight\", \"encoder.layer.17.ln.bias\", \"encoder.layer.17.intermediate.dense.weight\", \"encoder.layer.17.intermediate.dense.bias\", \"encoder.layer.17.output.dense.weight\", \"encoder.layer.17.output.dense.bias\", \"encoder.layer.18.attention.ln.weight\", \"encoder.layer.18.attention.ln.bias\", \"encoder.layer.18.attention.self.query.weight\", \"encoder.layer.18.attention.self.query.bias\", \"encoder.layer.18.attention.self.key.weight\", \"encoder.layer.18.attention.self.key.bias\", \"encoder.layer.18.attention.self.value.weight\", \"encoder.layer.18.attention.self.value.bias\", \"encoder.layer.18.attention.output.dense.weight\", \"encoder.layer.18.attention.output.dense.bias\", \"encoder.layer.18.ln.weight\", \"encoder.layer.18.ln.bias\", \"encoder.layer.18.intermediate.dense.weight\", \"encoder.layer.18.intermediate.dense.bias\", \"encoder.layer.18.output.dense.weight\", \"encoder.layer.18.output.dense.bias\", \"encoder.layer.19.attention.ln.weight\", \"encoder.layer.19.attention.ln.bias\", \"encoder.layer.19.attention.self.query.weight\", \"encoder.layer.19.attention.self.query.bias\", \"encoder.layer.19.attention.self.key.weight\", \"encoder.layer.19.attention.self.key.bias\", \"encoder.layer.19.attention.self.value.weight\", \"encoder.layer.19.attention.self.value.bias\", \"encoder.layer.19.attention.output.dense.weight\", \"encoder.layer.19.attention.output.dense.bias\", \"encoder.layer.19.ln.weight\", \"encoder.layer.19.ln.bias\", \"encoder.layer.19.intermediate.dense.weight\", \"encoder.layer.19.intermediate.dense.bias\", \"encoder.layer.19.output.dense.weight\", \"encoder.layer.19.output.dense.bias\", \"encoder.layer.20.attention.ln.weight\", \"encoder.layer.20.attention.ln.bias\", \"encoder.layer.20.attention.self.query.weight\", \"encoder.layer.20.attention.self.query.bias\", \"encoder.layer.20.attention.self.key.weight\", \"encoder.layer.20.attention.self.key.bias\", \"encoder.layer.20.attention.self.value.weight\", \"encoder.layer.20.attention.self.value.bias\", \"encoder.layer.20.attention.output.dense.weight\", \"encoder.layer.20.attention.output.dense.bias\", \"encoder.layer.20.ln.weight\", \"encoder.layer.20.ln.bias\", \"encoder.layer.20.intermediate.dense.weight\", \"encoder.layer.20.intermediate.dense.bias\", \"encoder.layer.20.output.dense.weight\", \"encoder.layer.20.output.dense.bias\", \"encoder.layer.21.attention.ln.weight\", \"encoder.layer.21.attention.ln.bias\", \"encoder.layer.21.attention.self.query.weight\", \"encoder.layer.21.attention.self.query.bias\", \"encoder.layer.21.attention.self.key.weight\", \"encoder.layer.21.attention.self.key.bias\", \"encoder.layer.21.attention.self.value.weight\", \"encoder.layer.21.attention.self.value.bias\", \"encoder.layer.21.attention.output.dense.weight\", \"encoder.layer.21.attention.output.dense.bias\", \"encoder.layer.21.ln.weight\", \"encoder.layer.21.ln.bias\", \"encoder.layer.21.intermediate.dense.weight\", \"encoder.layer.21.intermediate.dense.bias\", \"encoder.layer.21.output.dense.weight\", \"encoder.layer.21.output.dense.bias\", \"encoder.layer.22.attention.ln.weight\", \"encoder.layer.22.attention.ln.bias\", \"encoder.layer.22.attention.self.query.weight\", \"encoder.layer.22.attention.self.query.bias\", \"encoder.layer.22.attention.self.key.weight\", \"encoder.layer.22.attention.self.key.bias\", \"encoder.layer.22.attention.self.value.weight\", \"encoder.layer.22.attention.self.value.bias\", \"encoder.layer.22.attention.output.dense.weight\", \"encoder.layer.22.attention.output.dense.bias\", \"encoder.layer.22.ln.weight\", \"encoder.layer.22.ln.bias\", \"encoder.layer.22.intermediate.dense.weight\", \"encoder.layer.22.intermediate.dense.bias\", \"encoder.layer.22.output.dense.weight\", \"encoder.layer.22.output.dense.bias\", \"encoder.layer.23.attention.ln.weight\", \"encoder.layer.23.attention.ln.bias\", \"encoder.layer.23.attention.self.query.weight\", \"encoder.layer.23.attention.self.query.bias\", \"encoder.layer.23.attention.self.key.weight\", \"encoder.layer.23.attention.self.key.bias\", \"encoder.layer.23.attention.self.value.weight\", \"encoder.layer.23.attention.self.value.bias\", \"encoder.layer.23.attention.output.dense.weight\", \"encoder.layer.23.attention.output.dense.bias\", \"encoder.layer.23.ln.weight\", \"encoder.layer.23.ln.bias\", \"encoder.layer.23.intermediate.dense.weight\", \"encoder.layer.23.intermediate.dense.bias\", \"encoder.layer.23.output.dense.weight\", \"encoder.layer.23.output.dense.bias\", \"encoder.ln.weight\", \"encoder.ln.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"iteration\", \"model\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/2898531425.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMegatronBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMegatronBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMegatronBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMegatronBertConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.load_state_dict(torch.load('data/models/biomegatron/MegatronBERT.pt', \n\u001b[0m\u001b[1;32m      4\u001b[0m                                  map_location=torch.device('cpu')))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MegatronBertModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"encoder.layer.0.attention.ln.weight\", \"encoder.layer.0.attention.ln.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.ln.weight\", \"encoder.layer.0.ln.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.1.attention.ln.weight\", \"encoder.layer.1.attention.ln.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.ln.weight\", \"encoder.layer.1.ln.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.2.attention.ln.weight\", \"encoder.layer.2.attention.ln.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.ln.weight\", \"encoder.layer.2.ln.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.3.attention.ln.weight\", \"encoder.layer.3.attention.ln.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.ln.weight\", \"encoder.layer.3.ln.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.4.attention.ln.weight\", \"encoder.layer.4.attention.ln.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.ln.weight\", \"encoder.layer.4.ln.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.5.attention.ln.weight\", \"encoder.layer.5.attention.ln.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.ln.weight\", \"encoder.layer.5.ln.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.6.attention.ln.weight\", \"encoder.layer.6.attention.ln.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.ln.weight\", \"encoder.layer.6.ln.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.7.attention.ln.weight\", \"encoder.layer.7.attention.ln.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.ln.weight\", \"encoder.layer.7.ln.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.8.attention.ln.weight\", \"encoder.layer.8.attention.ln.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.ln.weight\", \"encoder.layer.8.ln.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.9.attention.ln.weight\", \"encoder.layer.9.attention.ln.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.ln.weight\", \"encoder.layer.9.ln.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.10.attention.ln.weight\", \"encoder.layer.10.attention.ln.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.ln.weight\", \"encoder.layer.10.ln.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.11.attention.ln.weight\", \"encoder.layer.11.attention.ln.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.ln.weight\", \"encoder.layer.11.ln.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.12.attention.ln.weight\", \"encoder.layer.12.attention.ln.bias\", \"encoder.layer.12.attention.self.query.weight\", \"encoder.layer.12.attention.self.query.bias\", \"encoder.layer.12.attention.self.key.weight\", \"encoder.layer.12.attention.self.key.bias\", \"encoder.layer.12.attention.self.value.weight\", \"encoder.layer.12.attention.self.value.bias\", \"encoder.layer.12.attention.output.dense.weight\", \"encoder.layer.12.attention.output.dense.bias\", \"encoder.layer.12.ln.weight\", \"encoder.layer.12.ln.bias\", \"encoder.layer.12.intermediate.dense.weight\", \"encoder.layer.12.intermediate.dense.bias\", \"encoder.layer.12.output.dense.weight\", \"encoder.layer.12.output.dense.bias\", \"encoder.layer.13.attention.ln.weight\", \"encoder.layer.13.attention.ln.bias\", \"encoder.layer.13.attention.self.query.weight\", \"encoder.layer.13.attention.self.query.bias\", \"encoder.layer.13.attention.self.key.weight\", \"encoder.layer.13.attention.self.key.bias\", \"encoder.layer.13.attention.self.value.weight\", \"encoder.layer.13.attention.self.value.bias\", \"encoder.layer.13.attention.output.dense.weight\", \"encoder.layer.13.attention.output.dense.bias\", \"encoder.layer.13.ln.weight\", \"encoder.layer.13.ln.bias\", \"encoder.layer.13.intermediate.dense.weight\", \"encoder.layer.13.intermediate.dense.bias\", \"encoder.layer.13.output.dense.weight\", \"encoder.layer.13.output.dense.bias\", \"encoder.layer.14.attention.ln.weight\", \"encoder.layer.14.attention.ln.bias\", \"encoder.layer.14.attention.self.query.weight\", \"encoder.layer.14.attention.self.query.bias\", \"encoder.layer.14.attention.self.key.weight\", \"encoder.layer.14.attention.self.key.bias\", \"encoder.layer.14.attention.self.value.weight\", \"encoder.layer.14.attention.self.value.bias\", \"encoder.layer.14.attention.output.dense.weight\", \"encoder.layer.14.attention.output.dense.bias\", \"encoder.layer.14.ln.weight\", \"encoder.layer.14.ln.bias\", \"encoder.layer.14.intermediate.dense.weight\", \"encoder.layer.14.intermediate.dense.bias\", \"encoder.layer.14.output.dense.weight\", \"encoder.layer.14.output.dense.bias\", \"encoder.layer.15.attention.ln.weight\", \"encoder.layer.15.attention.ln.bias\", \"encoder.layer.15.attention.self.query.weight\", \"encoder.layer.15.attention.self.query.bias\", \"encoder.layer.15.attention.self.key.weight\", \"encoder.layer.15.attention.self.key.bias\", \"encoder.layer.15.attention.self.value.weight\", \"encoder.layer.15.attention.self.value.bias\", \"encoder.layer.15.attention.output.dense.weight\", \"encoder.layer.15.attention.output.dense.bias\", \"encoder.layer.15.ln.weight\", \"encoder.layer.15.ln.bias\", \"encoder.layer.15.intermediate.dense.weight\", \"encoder.layer.15.intermediate.dense.bias\", \"encoder.layer.15.output.dense.weight\", \"encoder.layer.15.output.dense.bias\", \"encoder.layer.16.attention.ln.weight\", \"encoder.layer.16.attention.ln.bias\", \"encoder.layer.16.attention.self.query.weight\", \"encoder.layer.16.attention.self.query.bias\", \"encoder.layer.16.attention.self.key.weight\", \"encoder.layer.16.attention.self.key.bias\", \"encoder.layer.16.attention.self.value.weight\", \"encoder.layer.16.attention.self.value.bias\", \"encoder.layer.16.attention.output.dense.weight\", \"encoder.layer.16.attention.output.dense.bias\", \"encoder.layer.16.ln.weight\", \"encoder.layer.16.ln.bias\", \"encoder.layer.16.intermediate.dense.weight\", \"encoder.layer.16.intermediate.dense.bias\", \"encoder.layer.16.output.dense.weight\", \"encoder.layer.16.output.dense.bias\", \"encoder.layer.17.attention.ln.weight\", \"encoder.layer.17.attention.ln.bias\", \"encoder.layer.17.attention.self.query.weight\", \"encoder.layer.17.attention.self.query.bias\", \"encoder.layer.17.attention.self.key.weight\", \"encoder.layer.17.attention.self.key.bias\", \"encoder.layer.17.attention.self.value.weight\", \"encoder.layer.17.attention.self.value.bias\", \"encoder.layer.17.attention.output.dense.weight\", \"encoder.layer.17.attention.output.dense.bias\", \"encoder.layer.17.ln.weight\", \"encoder.layer.17.ln.bias\", \"encoder.layer.17.intermediate.dense.weight\", \"encoder.layer.17.intermediate.dense.bias\", \"encoder.layer.17.output.dense.weight\", \"encoder.layer.17.output.dense.bias\", \"encoder.layer.18.attention.ln.weight\", \"encoder.layer.18.attention.ln.bias\", \"encoder.layer.18.attention.self.query.weight\", \"encoder.layer.18.attention.self.query.bias\", \"encoder.layer.18.attention.self.key.weight\", \"encoder.layer.18.attention.self.key.bias\", \"encoder.layer.18.attention.self.value.weight\", \"encoder.layer.18.attention.self.value.bias\", \"encoder.layer.18.attention.output.dense.weight\", \"encoder.layer.18.attention.output.dense.bias\", \"encoder.layer.18.ln.weight\", \"encoder.layer.18.ln.bias\", \"encoder.layer.18.intermediate.dense.weight\", \"encoder.layer.18.intermediate.dense.bias\", \"encoder.layer.18.output.dense.weight\", \"encoder.layer.18.output.dense.bias\", \"encoder.layer.19.attention.ln.weight\", \"encoder.layer.19.attention.ln.bias\", \"encoder.layer.19.attention.self.query.weight\", \"encoder.layer.19.attention.self.query.bias\", \"encoder.layer.19.attention.self.key.weight\", \"encoder.layer.19.attention.self.key.bias\", \"encoder.layer.19.attention.self.value.weight\", \"encoder.layer.19.attention.self.value.bias\", \"encoder.layer.19.attention.output.dense.weight\", \"encoder.layer.19.attention.output.dense.bias\", \"encoder.layer.19.ln.weight\", \"encoder.layer.19.ln.bias\", \"encoder.layer.19.intermediate.dense.weight\", \"encoder.layer.19.intermediate.dense.bias\", \"encoder.layer.19.output.dense.weight\", \"encoder.layer.19.output.dense.bias\", \"encoder.layer.20.attention.ln.weight\", \"encoder.layer.20.attention.ln.bias\", \"encoder.layer.20.attention.self.query.weight\", \"encoder.layer.20.attention.self.query.bias\", \"encoder.layer.20.attention.self.key.weight\", \"encoder.layer.20.attention.self.key.bias\", \"encoder.layer.20.attention.self.value.weight\", \"encoder.layer.20.attention.self.value.bias\", \"encoder.layer.20.attention.output.dense.weight\", \"encoder.layer.20.attention.output.dense.bias\", \"encoder.layer.20.ln.weight\", \"encoder.layer.20.ln.bias\", \"encoder.layer.20.intermediate.dense.weight\", \"encoder.layer.20.intermediate.dense.bias\", \"encoder.layer.20.output.dense.weight\", \"encoder.layer.20.output.dense.bias\", \"encoder.layer.21.attention.ln.weight\", \"encoder.layer.21.attention.ln.bias\", \"encoder.layer.21.attention.self.query.weight\", \"encoder.layer.21.attention.self.query.bias\", \"encoder.layer.21.attention.self.key.weight\", \"encoder.layer.21.attention.self.key.bias\", \"encoder.layer.21.attention.self.value.weight\", \"encoder.layer.21.attention.self.value.bias\", \"encoder.layer.21.attention.output.dense.weight\", \"encoder.layer.21.attention.output.dense.bias\", \"encoder.layer.21.ln.weight\", \"encoder.layer.21.ln.bias\", \"encoder.layer.21.intermediate.dense.weight\", \"encoder.layer.21.intermediate.dense.bias\", \"encoder.layer.21.output.dense.weight\", \"encoder.layer.21.output.dense.bias\", \"encoder.layer.22.attention.ln.weight\", \"encoder.layer.22.attention.ln.bias\", \"encoder.layer.22.attention.self.query.weight\", \"encoder.layer.22.attention.self.query.bias\", \"encoder.layer.22.attention.self.key.weight\", \"encoder.layer.22.attention.self.key.bias\", \"encoder.layer.22.attention.self.value.weight\", \"encoder.layer.22.attention.self.value.bias\", \"encoder.layer.22.attention.output.dense.weight\", \"encoder.layer.22.attention.output.dense.bias\", \"encoder.layer.22.ln.weight\", \"encoder.layer.22.ln.bias\", \"encoder.layer.22.intermediate.dense.weight\", \"encoder.layer.22.intermediate.dense.bias\", \"encoder.layer.22.output.dense.weight\", \"encoder.layer.22.output.dense.bias\", \"encoder.layer.23.attention.ln.weight\", \"encoder.layer.23.attention.ln.bias\", \"encoder.layer.23.attention.self.query.weight\", \"encoder.layer.23.attention.self.query.bias\", \"encoder.layer.23.attention.self.key.weight\", \"encoder.layer.23.attention.self.key.bias\", \"encoder.layer.23.attention.self.value.weight\", \"encoder.layer.23.attention.self.value.bias\", \"encoder.layer.23.attention.output.dense.weight\", \"encoder.layer.23.attention.output.dense.bias\", \"encoder.layer.23.ln.weight\", \"encoder.layer.23.ln.bias\", \"encoder.layer.23.intermediate.dense.weight\", \"encoder.layer.23.intermediate.dense.bias\", \"encoder.layer.23.output.dense.weight\", \"encoder.layer.23.output.dense.bias\", \"encoder.ln.weight\", \"encoder.ln.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"iteration\", \"model\". "
     ]
    }
   ],
   "source": [
    "from transformers import MegatronBertModel, MegatronBertConfig\n",
    "model = MegatronBertModel(MegatronBertConfig()) \n",
    "model.load_state_dict(torch.load('data/models/biomegatron/MegatronBERT.pt', \n",
    "                                 map_location=torch.device('cpu')))\n",
    "\n",
    "# (torch.load('data/models/biomegatron/MegatronBERT.pt', \n",
    "#                                  map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b0dad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/nvidia/megatron-bert-cased-345m/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'nvidia/megatron-bert-cased-345m'. Make sure that:\n\n- 'nvidia/megatron-bert-cased-345m' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'nvidia/megatron-bert-cased-345m' is not a path to a local directory with something else, in that case)\n\n- or 'nvidia/megatron-bert-cased-345m' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             resolved_config_file = cached_path(\n\u001b[0m\u001b[1;32m    569\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1777\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1947\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1948\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1949\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/nvidia/megatron-bert-cased-345m/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/141831498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nvidia/megatron-bert-cased-345m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'nvidia/megatron-bert-cased-345m'. Make sure that:\n\n- 'nvidia/megatron-bert-cased-345m' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'nvidia/megatron-bert-cased-345m' is not a path to a local directory with something else, in that case)\n\n- or 'nvidia/megatron-bert-cased-345m' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/megatron-bert-cased-345m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a48bf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['language_model', 'qa_head'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c61e3416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.5,\n",
       "  \"hidden_size\": 240,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "from transformers import BertConfig\n",
    "# BertConfig(  \"hidden_dropout_prob\": 0.1,\n",
    "#               \"hidden_size\": 768) \n",
    "\n",
    "model = BertForTokenClassification(BertConfig(**{\"hidden_dropout_prob\": 0.5,\"hidden_size\": 12*20}) )\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a539a3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 240, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 240)\n",
       "      (token_type_embeddings): Embedding(2, 240)\n",
       "      (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (key): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (value): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=240, out_features=240, bias=True)\n",
       "              (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=240, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=240, bias=True)\n",
       "            (LayerNorm): LayerNorm((240,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=240, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, RobertaConfig, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_config(BertConfig(**{\"hidden_dropout_prob\": 0.5,\n",
    "                                                                  \"hidden_size\": 12*20,\n",
    "                                                                 \"num_labels\": 5}))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bceb2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_config(\n",
    "                            RobertaConfig(**{\"hidden_dropout_prob\": 0.5,\n",
    "                            \"hidden_size\": 12 * RobertaConfig().num_attention_heads,\n",
    "                            \"num_labels\":  5,\n",
    "                            \"max_position_embeddings\": 512}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71f4a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"employees\": [{\"name\": \"John Doe\", \"department\": \"Marketing\", \"place\": \"Remote\"}, {\"name\": \"Jane Doe\", \"department\": \"Software Engineering\", \"place\": \"Remote\"}, {\"name\": \"Don Joe\", \"department\": \"Software Engineering\", \"place\": \"Office\"}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    'employees' : [\n",
    "        {\n",
    "            'name' : 'John Doe',\n",
    "            'department' : 'Marketing',\n",
    "            'place' : 'Remote'\n",
    "        },\n",
    "        {\n",
    "            'name' : 'Jane Doe',\n",
    "            'department' : 'Software Engineering',\n",
    "            'place' : 'Remote'\n",
    "        },\n",
    "        {\n",
    "            'name' : 'Don Joe',\n",
    "            'department' : 'Software Engineering',\n",
    "            'place' : 'Office'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "json_string = json.dumps(data)\n",
    "print(json_string)\n",
    "\n",
    "with open('json_data.json', 'w') as outfile:\n",
    "    outfile.write(json_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a90cb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "file_ = 'json_data.json'\n",
    "\n",
    "if exists(file_):\n",
    "    with open('json_data.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        print((type(data)))\n",
    "        data[\"employees\"].append({\n",
    "            'name' : 'John Doe',\n",
    "            'department' : 'Marketing',\n",
    "            'place' : 'Remote'\n",
    "        })\n",
    "    json_string = json.dumps(data)\n",
    "    with open('json_data.json', 'w') as outfile:\n",
    "        outfile.write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba17789e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'employees': [{'name': 'John Doe',\n",
       "   'department': 'Marketing',\n",
       "   'place': 'Remote'},\n",
       "  {'name': 'Jane Doe',\n",
       "   'department': 'Software Engineering',\n",
       "   'place': 'Remote'},\n",
       "  {'name': 'Don Joe', 'department': 'Software Engineering', 'place': 'Office'},\n",
       "  {'name': 'John Doe', 'department': 'Marketing', 'place': 'Remote'},\n",
       "  {'name': 'John Doe', 'department': 'Marketing', 'place': 'Remote'}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('json_data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96513e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de494c",
   "metadata": {},
   "source": [
    "## Testing a loop for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f63680c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from smtag.metrics import MetricsTOKCL\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from smtag.config import Config\n",
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f656bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\"NER\", \"GENEPROD_ROLES\", \"SMALL_MOL_ROLES\", \"BORING\", \"PANELIZATION\"]\n",
    "MODELS = [\"EMBO/bio-lm\", \"roberta-base\"]\n",
    "\n",
    "ROBERTA_DATASET = \"drAbreu/sd-nlp-2\"\n",
    "GENERAL_DATASET = \"EMBO/sd-nlp-non-tokenized\"\n",
    "\n",
    "HUB_TOKEN = \"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\"\n",
    "\n",
    "HUB_USER = \"EMBO\"\n",
    "\n",
    "load_dotenv()\n",
    "LM_MODEL_PATH = os.getenv('LM_MODEL_PATH')\n",
    "TOKENIZER_PATH = os.getenv('TOKENIZER_PATH')\n",
    "TOKCL_MODEL_PATH = os.getenv('TOKCL_MODEL_PATH')\n",
    "CACHE = os.getenv('CACHE')\n",
    "RUNS_DIR = os.getenv('RUNS_DIR')\n",
    "\n",
    "TRAINING_ARGS_DICT = {\"output_dir\": TOKCL_MODEL_PATH,\n",
    "                     \"overwrite_output_dir\": True,\n",
    "                    \"logging_steps\":1000,\n",
    "                    \"evaluation_strategy\":'epoch',\n",
    "                    \"lr_scheduler_type\":'linear', \n",
    "                    \"save_strategy\":'epoch', \n",
    "                    \"save_steps\":1, \n",
    "#                     \"eval_strategy\":'epoch', \n",
    "                    \"save_total_limit\":5, \n",
    "                    \"seed\":42, \n",
    "                    \"eval_steps\":1, \n",
    "                    \"past_index\":-1, \n",
    "                    \"run_name\":TOKCL_MODEL_PATH, \n",
    "                    \"disable_tqdm\":False, \n",
    "                    \"metric_for_best_model\":'overall_f1', \n",
    "                    \"load_best_model_at_end\":True, \n",
    "                    \"greater_is_better\":True, \n",
    "                    \"length_column_name\":'length', \n",
    "                    \"report_to\":['tensorboard'], \n",
    "                    \"push_to_hub\":False, \n",
    "                    \"resume_from_checkpoint\":None,  \n",
    "                    \"hub_strategy\":'every_save', \n",
    "                    \"hub_token\":HUB_TOKEN, \n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b995d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7980f743c3c41d3a4117420c6c8b568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"drAbreu/sd-nlp-2\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3e343d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48771"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9baa1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_labels(dataset):\n",
    "    num_labels = dataset.info.features['labels'].feature.num_classes\n",
    "    label_list = dataset.info.features['labels'].feature.names\n",
    "    id2label, label2id = {}, {}\n",
    "    for class_, label in zip(range(num_labels), label_list):\n",
    "        id2label[class_] = label \n",
    "        label2id[label] = class_ \n",
    "        return id2label, label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec9016ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_label(label):\n",
    "    # If the label is B-XXX we change it to I-XX\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    Expands the NER tags once the sub-word tokenization is added.\n",
    "    Arguments\n",
    "    ---------\n",
    "    labels list[int]:\n",
    "    word_ids list[int]\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            # As far as word_id matches the index of the current word\n",
    "            # We append the same label\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            new_labels.append(shift_label(labels[word_id]))\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['words'], \n",
    "                       truncation=True,\n",
    "                       is_split_into_words=True)\n",
    "    \n",
    "    all_labels = examples['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e5b15dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab6b802e05a4661aa3d6e5acbdb117b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_586/1982323694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mid2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n\u001b[1;32m     17\u001b[0m                                                                \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1748\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0;31m# Second attempt. If we have not yet found tokenizer_class, let's try to use the config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m                 config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1806\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m                     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mconfig_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             configuration_file = get_configuration_file(\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_configuration_file\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \"\"\"\n\u001b[1;32m    842\u001b[0m     \u001b[0;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m     all_files = get_list_of_files(\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist_repo_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             args_msg = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mlist_repo_files\u001b[0;34m(self, repo_id, revision, repo_type, token, timeout)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \"\"\"\n\u001b[1;32m   1161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrepo_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrepo_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m             info = self.model_info(\n\u001b[0m\u001b[1;32m   1163\u001b[0m                 \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             args_msg = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout, securityStatus)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Bearer {token}\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mstatus_query_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"securityStatus\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msecurityStatus\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         r = requests.get(\n\u001b[0m\u001b[1;32m   1119\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_query_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_repo_names = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    for task in TASKS:\n",
    "        TRAINING_ARGS_DICT[\"hub_model_id\"] = f\"{HUB_USER}/{model_name.replace('/','_')}_{task}\"\n",
    "        if model_name in [\"EMBO/bio-lm\", \"roberta-base\"]:\n",
    "            \n",
    "            config = Config(model_type = \"Autoencoder\", \n",
    "                            from_pretrained = model_name, \n",
    "                            tokenizer = \"roberta-base\")\n",
    "            \n",
    "            data = load_dataset(ROBERTA_DATASET, task)\n",
    "            train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "            id2label, label2id = define_labels(train_dataset)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)\n",
    "            data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                                               padding=True,\n",
    "                                                               return_tensors='pt')\n",
    "            \n",
    "            compute_metrics = MetricsTOKCL(label_list=list(label2id.keys()))\n",
    "            \n",
    "            \n",
    "            # Get the training arguments\n",
    "            training_args = TrainingArgumentsTOKCL(**TRAINING_ARGS_DICT)\n",
    "\n",
    "            # Select the model (This is for Token Classification)\n",
    "            \n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                config.from_pretrained,\n",
    "                num_labels=len(list(id2label.keys())),\n",
    "                max_position_embeddings=config.max_length + 2,  \n",
    "                id2label = id2label,\n",
    "                label2id = label2id\n",
    "            )\n",
    "            model_config = model.config\n",
    "            \n",
    "            # Define the trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[ShowExampleTOKCL(tokenizer)]\n",
    "            )\n",
    "\n",
    "            # switch the Tensorboard callback to plot losses on same plot\n",
    "            trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "            trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "#             train()\n",
    "            list_repo_names.append(TRAINING_ARGS_DICT[\"hub_model_id\"])\n",
    "    \n",
    "        elif model_name in []:\n",
    "            data = load_dataset(GENERAL_DATASET, task)\n",
    "            list_repo_names.append(TRAINING_ARGS_DICT[\"hub_model_id\"])\n",
    "        else:\n",
    "            raise ValueError(f\"The selected model ({model_name}) is not contained in our Benchmark list. Please add it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be43dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": 0.5,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", \n",
    "                                                num_labels=15,\n",
    "                                               classifier_dropout=0.5).config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e612370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_config(\n",
    "                BertConfig(**{\n",
    "                    \"_name_or_path\": \"bert-base-uncased\",\n",
    "                    \"classifier_dropout\": 0.1,\n",
    "                    \"hidden_size\": 768,\n",
    "                    \"num_labels\": 15,\n",
    "                    \"max_position_embeddings\": 512}\n",
    "                              )\n",
    "            )\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7fee54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, MegatronBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53d203c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "megatron = AutoModelForTokenClassification.from_config(MegatronBertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17a8cecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MegatronBertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"megatron-bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 29056\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "megatron.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "feb22e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nemo_toolkit[all]\n",
      "  Downloading nemo_toolkit-1.8.2-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 KB\u001b[0m \u001b[31m733.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ruamel.yaml\n",
      "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (2.8.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (1.11.0a0+bfe5ad2)\n",
      "Requirement already satisfied: onnx>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (1.10.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (4.62.3)\n",
      "Requirement already satisfied: wget in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (3.2)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (0.53.1)\n",
      "Collecting frozendict\n",
      "  Downloading frozendict-2.3.2-py3-none-any.whl (11 kB)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (0.24.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (7.7.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='developer.download.nvidia.com', port=443): Read timed out. (read timeout=15)\")': /compute/redist/pydub/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: sphinx in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (4.4.0)\n",
      "Collecting marshmallow\n",
      "  Downloading marshmallow-3.15.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting youtokentome>=1.0.5\n",
      "  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (0.12.0a0)\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (0.10.3.post1)\n",
      "Collecting pytest-runner\n",
      "  Downloading pytest_runner-6.0.0-py3-none-any.whl (7.2 kB)\n",
      "Collecting attrdict\n",
      "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (1.6.3)\n",
      "Collecting nltk>=3.6.5\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m852.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-stft\n",
      "  Downloading torch_stft-0.1.4-py3-none-any.whl (6.2 kB)\n",
      "Collecting pyannote.core\n",
      "  Downloading pyannote.core-4.4-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-lightning>=1.6.0\n",
      "  Downloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.0/584.0 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting isort[requirements]<5\n",
      "  Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 KB\u001b[0m \u001b[31m141.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting editdistance\n",
      "  Downloading editdistance-0.6.0-cp38-cp38-manylinux2014_aarch64.whl (263 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m835.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flask-restful\n",
      "  Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
      "Collecting sphinxcontrib-bibtex\n",
      "  Downloading sphinxcontrib_bibtex-2.4.2-py3-none-any.whl (39 kB)\n",
      "Collecting webdataset<=0.1.62,>=0.1.48\n",
      "  Downloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (0.8.1)\n",
      "Collecting black==19.10b0\n",
      "  Downloading black-19.10b0-py36-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 KB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (21.3)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (2022.1.18)\n",
      "Requirement already satisfied: sacremoses>=0.0.43 in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (0.0.47)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-2.0.11-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kaldi-io\n",
      "  Downloading kaldi_io-0.9.4-py3-none-any.whl (14 kB)\n",
      "Collecting inflect\n",
      "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torchmetrics>=0.4.1rc0\n",
      "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sacrebleu[ja]\n",
      "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (1.4.2)\n",
      "Collecting hydra-core>=1.1.0\n",
      "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (3.5.1)\n",
      "Collecting pypinyin\n",
      "  Downloading pypinyin-0.46.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opencc\n",
      "  Downloading OpenCC-0.2.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (4.15.0)\n",
      "Collecting sentencepiece<1.0.0\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting g2p-en\n",
      "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyannote.metrics\n",
      "  Downloading pyannote.metrics-3.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 KB\u001b[0m \u001b[31m660.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytest in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (6.2.5)\n",
      "Collecting pynini==2.1.4\n",
      "  Downloading pynini-2.1.4.tar.gz (621 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m621.9/621.9 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pangu\n",
      "  Downloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
      "Collecting pesq\n",
      "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pystoi\n",
      "  Downloading pystoi-0.3.3.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kaldiio\n",
      "  Downloading kaldiio-2.17.2.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting omegaconf>=2.1.0\n",
      "  Downloading omegaconf-2.2.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kaldi-python-io\n",
      "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sox\n",
      "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from nemo_toolkit[all]) (8.2.0)\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml<6\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux2014_aarch64.whl (818 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.23.5-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.6.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting braceexpand\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Collecting parameterized\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting pathspec<1,>=0.6\n",
      "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from black==19.10b0->nemo_toolkit[all]) (1.4.4)\n",
      "Requirement already satisfied: toml>=0.9.4 in /opt/conda/lib/python3.8/site-packages (from black==19.10b0->nemo_toolkit[all]) (0.10.2)\n",
      "Requirement already satisfied: click>=6.5 in /opt/conda/lib/python3.8/site-packages (from black==19.10b0->nemo_toolkit[all]) (7.1.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.8/site-packages (from black==19.10b0->nemo_toolkit[all]) (21.4.0)\n",
      "Collecting typed-ast>=1.4.0\n",
      "  Downloading typed_ast-1.5.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (860 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m860.9/860.9 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Cython>=0.29 in /opt/conda/lib/python3.8/site-packages (from pynini==2.1.4->nemo_toolkit[all]) (0.29.26)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.8/site-packages (from hydra-core>=1.1.0->nemo_toolkit[all]) (5.4.0)\n",
      "Collecting pipreqs\n",
      "  Downloading pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
      "Collecting pip-api\n",
      "  Downloading pip_api-0.0.29-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.4/111.4 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->nemo_toolkit[all]) (4.29.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->nemo_toolkit[all]) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->nemo_toolkit[all]) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->nemo_toolkit[all]) (0.11.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk>=3.6.5->nemo_toolkit[all]) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from onnx>=1.7.0->nemo_toolkit[all]) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.8/site-packages (from onnx>=1.7.0->nemo_toolkit[all]) (4.0.1)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.8/site-packages (from onnx>=1.7.0->nemo_toolkit[all]) (3.19.3)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning>=1.6.0->nemo_toolkit[all]) (2.8.0)\n",
      "Collecting pyDeprecate<0.4.0,>=0.3.1\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning>=1.6.0->nemo_toolkit[all]) (2022.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers>=4.0.1->nemo_toolkit[all]) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers>=4.0.1->nemo_toolkit[all]) (3.4.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.0.1->nemo_toolkit[all]) (0.10.3)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m713.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.27.0,>=1.26.5\n",
      "  Downloading botocore-1.26.5-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.8/site-packages (from fasttext->nemo_toolkit[all]) (2.9.0)\n",
      "Requirement already satisfied: Flask>=0.8 in /opt/conda/lib/python3.8/site-packages (from flask-restful->nemo_toolkit[all]) (2.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aniso8601>=0.82\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 KB\u001b[0m \u001b[31m701.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from flask-restful->nemo_toolkit[all]) (2021.3)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.8/site-packages (from ftfy->nemo_toolkit[all]) (0.2.5)\n",
      "Collecting distance>=0.1.3\n",
      "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from gdown->nemo_toolkit[all]) (4.10.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->nemo_toolkit[all]) (3.6.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->nemo_toolkit[all]) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->nemo_toolkit[all]) (1.1.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->nemo_toolkit[all]) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->nemo_toolkit[all]) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->nemo_toolkit[all]) (6.7.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets->nemo_toolkit[all]) (7.31.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa->nemo_toolkit[all]) (0.2.2)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa->nemo_toolkit[all]) (1.6.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa->nemo_toolkit[all]) (5.1.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa->nemo_toolkit[all]) (2.1.9)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba->nemo_toolkit[all]) (0.36.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->nemo_toolkit[all]) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile->nemo_toolkit[all]) (1.15.0)\n",
      "Collecting simplejson>=3.8.1\n",
      "  Downloading simplejson-3.17.6-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sortedcontainers>=2.0.4\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.8/site-packages (from pyannote.metrics->nemo_toolkit[all]) (0.8.9)\n",
      "Collecting pyannote.database>=4.0.1\n",
      "  Downloading pyannote.database-4.1.3-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.1\n",
      "  Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from pytest->nemo_toolkit[all]) (1.11.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest->nemo_toolkit[all]) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.8/site-packages (from pytest->nemo_toolkit[all]) (1.1.1)\n",
      "Collecting jarowinkler<1.1.0,>=1.0.2\n",
      "  Downloading jarowinkler-1.0.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
      "  Downloading ruamel.yaml.clib-0.2.6.tar.gz (180 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from sacrebleu[ja]->nemo_toolkit[all]) (0.4.4)\n",
      "Collecting ipadic<2.0,>=1.0\n",
      "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mecab-python3==1.0.5\n",
      "  Downloading mecab_python3-1.0.5-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (554 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m555.0/555.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=2.3 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (3.0.3)\n",
      "Requirement already satisfied: docutils<0.18,>=0.14 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (0.17.1)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (1.0.3)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (1.1.5)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (2.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
      "Requirement already satisfied: babel>=1.3 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (2.9.1)\n",
      "Requirement already satisfied: imagesize in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (4.10.1)\n",
      "Requirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (2.11.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.8/site-packages (from sphinx->nemo_toolkit[all]) (0.7.12)\n",
      "Collecting pybtex>=0.24\n",
      "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pybtex-docutils>=1.0.0\n",
      "  Downloading pybtex_docutils-1.0.1-py3-none-any.whl (4.8 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb->nemo_toolkit[all]) (5.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.27.0,>=1.26.5->boto3->nemo_toolkit[all]) (1.26.7)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile->nemo_toolkit[all]) (2.21)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.8/site-packages (from Flask>=0.8->flask-restful->nemo_toolkit[all]) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /opt/conda/lib/python3.8/site-packages (from Flask>=0.8->flask-restful->nemo_toolkit[all]) (2.0.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (3.8.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m545.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->sphinx->nemo_toolkit[all]) (3.7.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (7.1.2)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (0.1.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (1.5.4)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (3.0.24)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.7.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2>=2.3->sphinx->nemo_toolkit[all]) (2.0.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets->nemo_toolkit[all]) (4.9.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets->nemo_toolkit[all]) (4.4.0)\n",
      "Requirement already satisfied: typer[all]>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[all]) (0.4.0)\n",
      "Collecting latexcodec>=1.0.4\n",
      "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=4.0.1->nemo_toolkit[all]) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=4.0.1->nemo_toolkit[all]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=4.0.1->nemo_toolkit[all]) (3.1)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (2.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (1.43.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (0.37.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (1.0.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (6.4.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->gdown->nemo_toolkit[all]) (2.3.1)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (from pip-api->isort[requirements]<5->nemo_toolkit[all]) (22.0.4)\n",
      "Collecting yarg\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=4.0.1->nemo_toolkit[all]) (1.7.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->nemo_toolkit[all]) (0.18.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (22.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.8.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (6.4.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (21.3.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.7.0)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[all]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (1.7.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.0->nemo_toolkit[all]) (3.1.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (21.2.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.5.10)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.5.1)\n",
      "Building wheels for collected packages: pynini, antlr4-python3-runtime, youtokentome, fasttext, gdown, jieba, kaldi-python-io, kaldiio, opencc, pesq, pystoi, distance, docopt, ipadic, promise, ruamel.yaml.clib, pathtools\n",
      "  Building wheel for pynini (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[56 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-38\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-38/pynini\n",
      "  \u001b[31m   \u001b[0m copying pynini/__init__.py -> build/lib.linux-aarch64-cpython-38/pynini\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-38/pywrapfst\n",
      "  \u001b[31m   \u001b[0m copying pywrapfst/__init__.py -> build/lib.linux-aarch64-cpython-38/pywrapfst\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/multi_grm_example.py -> build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/grm.py -> build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/export.py -> build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/grm_example.py -> build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/multi_grm.py -> build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/__init__.py -> build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/numbers.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/chatspeak.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/chatspeak_model.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/weather.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/dates.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/g2p.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/t9.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/plurals.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/__init__.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/case.py -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/rewrite.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/utf8.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/rule_cascade.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/features.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/byte.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/pynutil.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/paradigms.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/__init__.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/tagger.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/edit_transducer.py -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/__init__.pyi -> build/lib.linux-aarch64-cpython-38/pynini\n",
      "  \u001b[31m   \u001b[0m copying pynini/py.typed -> build/lib.linux-aarch64-cpython-38/pynini\n",
      "  \u001b[31m   \u001b[0m copying pywrapfst/__init__.pyi -> build/lib.linux-aarch64-cpython-38/pywrapfst\n",
      "  \u001b[31m   \u001b[0m copying pywrapfst/py.typed -> build/lib.linux-aarch64-cpython-38/pywrapfst\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/py.typed -> build/lib.linux-aarch64-cpython-38/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/py.typed -> build/lib.linux-aarch64-cpython-38/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/py.typed -> build/lib.linux-aarch64-cpython-38/pynini/lib\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building '_pywrapfst' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-38\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-cpython-38/extensions\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.8 -c extensions/_pywrapfst.cpp -o build/temp.linux-aarch64-cpython-38/extensions/_pywrapfst.o -std=c++17 -Wno-register -Wno-deprecated-declarations -Wno-unused-function -Wno-unused-local-typedefs -funsigned-char\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m extensions/_pywrapfst.cpp:745:10: fatal error: fst/types.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m   745 | #include <fst/types.h>\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pynini\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pynini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=a17d7388ab5973e413114a183b39fbed2081e1307483e7075dbed9d58a095761\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "  Building wheel for youtokentome (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp38-cp38-linux_aarch64.whl size=1862562 sha256=fc72a2d1f5aaae792ba3a87f0a7f316e248cd1f617d968851d08d2326d1f0112\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/d6/b1/d4/6080102e6213d8b915d003eed3361c25f593b078127ac6db5c\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_aarch64.whl size=4519708 sha256=15ac95a858d9f9b3e8250196880c59f890c1cc68dd7dacadfd05847d13eb1f5f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=c975ddf96a5aeee89f67820c3903c55db1b20aa2fd82c4f56ae16de0762f4af3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/7b/7b/5d/656f46cd6889e4c93977be9586901d0adc1271b2d876c84c96\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314476 sha256=ed5271a64250092292a6b86aafe2e0dee4d2f073df81708c34f20dfa64f2321b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/ca/38/d8/dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8968 sha256=c144760a24d15f522b26fe6e716419c0625e2527e3392d34956c38d23203fc73\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/8c/da/46/4afb7e26376c33af41c3ec388d5b63d34d186f6df1545cac30\n",
      "  Building wheel for kaldiio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaldiio: filename=kaldiio-2.17.2-py3-none-any.whl size=24472 sha256=7bbf5de7640f057187b2fa85e2de915c648d2cfe58adda97592db09c201c7d4c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/49/b3/00/af2103b510836161326bb51e27795407b07fda6969d0ae5967\n",
      "  Building wheel for opencc (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opencc: filename=OpenCC-0.2-py3-none-any.whl size=6562 sha256=6ad30a06915e2c6ec47260b7902587c181d5b324c801f4f3b8a3b33726714acd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/48/46/4f/2bf3b639fd8a795e864457b571c35b70bc7bc9a7ad7a8da342\n",
      "  Building wheel for pesq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pesq: filename=pesq-0.0.4-cp38-cp38-linux_aarch64.whl size=283643 sha256=e3418be6ed72c37c496806a9cf52cd7045ae52899258490b5c068319994f139f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/be/50/3f/1e20ce10f2e57d3481b488245b0f05f013602dd6dca0a67d18\n",
      "  Building wheel for pystoi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7793 sha256=4a82ac11c09256c10032b39bcace1d42c72407b08530f2364653a01cdf8b0622\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/62/35/75/c07f0861a60fb8aacf44fdd5c8c214a224a6c9edb4a4e1402f\n",
      "  Building wheel for distance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16275 sha256=00b56c3700da488fa17fa77b7b39c449b0590ae03e210fe99ca07c87ed95cae8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/fb/a8/64/6edcab63ec51512a87cacf9b3563c711ad6b7b05d61b704493\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=66a9ac719395267750075c8c0a48dbaaeafebd10040f7bfcbffcc26e25fba232\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "  Building wheel for ipadic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=335be22651a1e54ec5412db83a5b7ad9c9d0367164f5da5ca6fc105ecf12afa4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/45/b7/f5/a21e68db846eedcd00d69e37d60bab3f68eb20b1d99cdff652\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=f45fbe3c9f14796db358a9cebec66075ef17149e8acc646e43cab2b9e051f791\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for ruamel.yaml.clib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ruamel.yaml.clib: filename=ruamel.yaml.clib-0.2.6-cp38-cp38-linux_aarch64.whl size=747258 sha256=2befa1bc1a022afceea9ff10b400d523cb336c9b0bea21013b7f2c0a64ee5b88\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/17/0a/cb/8df6ed5547555605ad0f025f670be83ff322b3cfb5982e9427\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=72e5fca8082ca34fb8ab0d4805b9190bde30a09cf036f74d8139f1860ac18ccf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mkn9aa1/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built antlr4-python3-runtime youtokentome fasttext gdown jieba kaldi-python-io kaldiio opencc pesq pystoi distance docopt ipadic promise ruamel.yaml.clib pathtools\n",
      "Failed to build pynini\n",
      "Installing collected packages: sortedcontainers, sentencepiece, pydub, pesq, pathtools, parameterized, pangu, opencc, mpmath, mecab-python3, jieba, ipadic, docopt, distance, braceexpand, antlr4-python3-runtime, aniso8601, youtokentome, wrapt, webdataset, unidecode, typed-ast, sympy, sox, smmap, simplejson, shortuuid, setuptools, setproctitle, sentry-sdk, ruamel.yaml.clib, pyyaml, pytest-runner, pypinyin, pynini, pyDeprecate, promise, portalocker, pip-api, pathspec, nltk, latexcodec, kaldiio, kaldi-python-io, kaldi-io, jmespath, jarowinkler, isort, inflect, h5py, ftfy, frozendict, editdistance, docker-pycreds, attrdict, yarg, torchmetrics, sacrebleu, ruamel.yaml, rapidfuzz, pystoi, pybtex, pyannote.core, omegaconf, marshmallow, gitdb, g2p-en, fasttext, botocore, black, s3transfer, pybtex-docutils, pyannote.database, pipreqs, nemo_toolkit, hydra-core, GitPython, gdown, flask-restful, wandb, sphinxcontrib-bibtex, pyannote.metrics, boto3, torch-stft, pytorch-lightning\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 62.1.0\n",
      "    Uninstalling setuptools-62.1.0:\n",
      "      Successfully uninstalled setuptools-62.1.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Running setup.py install for pynini ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for pynini\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[58 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m /opt/conda/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-3.8/pynini\n",
      "  \u001b[31m   \u001b[0m copying pynini/__init__.py -> build/lib.linux-aarch64-3.8/pynini\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-3.8/pywrapfst\n",
      "  \u001b[31m   \u001b[0m copying pywrapfst/__init__.py -> build/lib.linux-aarch64-3.8/pywrapfst\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/multi_grm_example.py -> build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/grm.py -> build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/export.py -> build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/grm_example.py -> build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/multi_grm.py -> build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/__init__.py -> build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/numbers.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/chatspeak.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/chatspeak_model.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/weather.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/dates.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/g2p.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/t9.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/plurals.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/__init__.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/case.py -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/rewrite.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/utf8.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/rule_cascade.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/features.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/byte.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/pynutil.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/paradigms.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/__init__.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/tagger.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/edit_transducer.py -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m copying pynini/__init__.pyi -> build/lib.linux-aarch64-3.8/pynini\n",
      "  \u001b[31m   \u001b[0m copying pynini/py.typed -> build/lib.linux-aarch64-3.8/pynini\n",
      "  \u001b[31m   \u001b[0m copying pywrapfst/__init__.pyi -> build/lib.linux-aarch64-3.8/pywrapfst\n",
      "  \u001b[31m   \u001b[0m copying pywrapfst/py.typed -> build/lib.linux-aarch64-3.8/pywrapfst\n",
      "  \u001b[31m   \u001b[0m copying pynini/export/py.typed -> build/lib.linux-aarch64-3.8/pynini/export\n",
      "  \u001b[31m   \u001b[0m copying pynini/examples/py.typed -> build/lib.linux-aarch64-3.8/pynini/examples\n",
      "  \u001b[31m   \u001b[0m copying pynini/lib/py.typed -> build/lib.linux-aarch64-3.8/pynini/lib\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building '_pywrapfst' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-3.8\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-aarch64-3.8/extensions\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.8 -c extensions/_pywrapfst.cpp -o build/temp.linux-aarch64-3.8/extensions/_pywrapfst.o -std=c++17 -Wno-register -Wno-deprecated-declarations -Wno-unused-function -Wno-unused-local-typedefs -funsigned-char\n",
      "  \u001b[31m   \u001b[0m cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  \u001b[31m   \u001b[0m extensions/_pywrapfst.cpp:745:10: fatal error: fst/types.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m   745 | #include <fst/types.h>\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command 'gcc' failed with exit status 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m pynini\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install nemo_toolkit['all']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
