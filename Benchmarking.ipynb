{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8b44d2",
   "metadata": {},
   "source": [
    "# Benchmarking models against SoDa-RoBERTa\n",
    "\n",
    "In this document we want to benchmark the performance of different models in comparisson to [SoDa-RoBERTa](https://github.com/source-data/soda-roberta) in different `token classification` tasks. \n",
    "\n",
    "The goal behind this experiment is not to find the best overall models, but those that are more accurate in classifying text entities of interest in the [SourceData](https://sourcedata.embo.org/) context. This means in the field of molecular cell biology. With this goal in mind, we will not use typical benchmarking datasets, but the [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) and [`sd-nlp-non-tokenized`](https://huggingface.co/datasets/EMBO/sd-nlp-non-tokenized) datasets. These datasets contain annotated image captions extracted from papers on molecular biology. The data has been curated and annotated by profesionals in the field. \n",
    "\n",
    "This notebook is intended to be used with the [ðŸ¤— Datasets](https://huggingface.co/) library. \n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [Chapter 1 - SoDa-RoBERTa](#chapter1)\n",
    "    * [Section 1.1 - NER task for SoDa-RoBERTa](#section_1_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for SoDa-RoBERTa](#section_1_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for SoDa-RoBERTa](#section_1_3)\n",
    "    * [Section 1.4 - PANELIZATION task for SoDa-RoBERTa](#section_1_4)\n",
    "    * [Section 1.5 - BORING task for SoDa-RoBERTa](#section_1_5)\n",
    "* [Chapter 2 - RoBERTa](#chapter2)\n",
    "    * [Section 1.1 - NER task for RoBERTa](#section_2_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for RoBERTa](#section_2_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for RoBERTa](#section_2_3)\n",
    "    * [Section 1.4 - PANELIZATION task for RoBERTa](#section_2_4)\n",
    "    * [Section 1.5 - BORING task for RoBERTa](#section_2_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f683acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from smtag.config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c6f0469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.15.0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9296f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bac618",
   "metadata": {},
   "source": [
    "# Chapter 1 - SoDa RoBERTa <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "[SoDa-RoBERTa](https://github.com/source-data/soda-roberta) [(Liechti. et al. 2017)](https://doi.org/10.1038/nmeth.4471) is a package generated by the [SourceData](https://sourcedata.embo.org/) team. This package has been developed to improve the data curation of biomedical papers in the field of molecular and cell biology.\n",
    "\n",
    "This is the first model that we will use in our benchmarking. The data available in [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) has already been tokenized using the ðŸ¤— `roberta-base` tokenizer. This tokenizer has been pre-trained with the `roberta-base` model, which is the base model on top of which SoDa-RoBERTa has been built. \n",
    "\n",
    "Since the model is already pre-trained, we just need to fine-tune it. The basic idea is that the pre-trained model with generate a series of outputs that will be token encoders. By fine-tuning a model, FFNN is added on the top of these embeddings and connected to a `softmax` layer to classify tokens.\n",
    "\n",
    "This process is mostly automated to us by the [ðŸ¤— `AutoModelForTokenClassification`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForTokenClassification) class.\n",
    "\n",
    "# Section 1.1 - NER task for SoDa-RoBERTa <a class=\"anchor\" id=\"section_1_1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e625ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "141efac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a57eec257a4eac8d8d7ee3d3daa0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0,\n",
       "   1640,\n",
       "   347,\n",
       "   43,\n",
       "   4052,\n",
       "   847,\n",
       "   33101,\n",
       "   43916,\n",
       "   14868,\n",
       "   303,\n",
       "   129,\n",
       "   15,\n",
       "   15145,\n",
       "   36,\n",
       "   571,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   8,\n",
       "   1368,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   43,\n",
       "   15,\n",
       "   36475,\n",
       "   36,\n",
       "   118,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   43,\n",
       "   9,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   4,\n",
       "   735,\n",
       "   2549,\n",
       "   58,\n",
       "   1455,\n",
       "   129,\n",
       "   15,\n",
       "   5,\n",
       "   5856,\n",
       "   9,\n",
       "   5,\n",
       "   155,\n",
       "   1484,\n",
       "   25,\n",
       "   22827,\n",
       "   13,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   36,\n",
       "   267,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   322,\n",
       "   1437,\n",
       "   2],\n",
       "  [0,\n",
       "   28588,\n",
       "   3693,\n",
       "   3041,\n",
       "   44193,\n",
       "   40899,\n",
       "   16007,\n",
       "   21258,\n",
       "   2018,\n",
       "   5,\n",
       "   127,\n",
       "   523,\n",
       "   12572,\n",
       "   3551,\n",
       "   5252,\n",
       "   11,\n",
       "   364,\n",
       "   771,\n",
       "   2571,\n",
       "   9,\n",
       "   545,\n",
       "   12,\n",
       "   3583,\n",
       "   12,\n",
       "   279,\n",
       "   15540,\n",
       "   9789,\n",
       "   15,\n",
       "   10,\n",
       "   239,\n",
       "   12,\n",
       "   19987,\n",
       "   5626,\n",
       "   36,\n",
       "   725,\n",
       "   24667,\n",
       "   43,\n",
       "   36,\n",
       "   4070,\n",
       "   43,\n",
       "   8,\n",
       "   5,\n",
       "   12337,\n",
       "   11257,\n",
       "   5656,\n",
       "   36,\n",
       "   6960,\n",
       "   322,\n",
       "   20,\n",
       "   2853,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   11579,\n",
       "   1978,\n",
       "   12,\n",
       "   38683,\n",
       "   401,\n",
       "   534,\n",
       "   12,\n",
       "   4590,\n",
       "   4,\n",
       "   20,\n",
       "   1692,\n",
       "   9217,\n",
       "   311,\n",
       "   5,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   438,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   3592,\n",
       "   13418,\n",
       "   21130,\n",
       "   3443,\n",
       "   8,\n",
       "   10888,\n",
       "   401,\n",
       "   347,\n",
       "   4411,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   2544,\n",
       "   4590,\n",
       "   6,\n",
       "   4067,\n",
       "   4,\n",
       "   20,\n",
       "   795,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   10888,\n",
       "   401,\n",
       "   534,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   746,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   40936,\n",
       "   36,\n",
       "   282,\n",
       "   5457,\n",
       "   290,\n",
       "   15540,\n",
       "   322,\n",
       "   1437,\n",
       "   2]],\n",
       " 'labels': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   9,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'tag_mask': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"drAbreu/sd-nlp-2\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "train_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19fb9def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c985ea",
   "metadata": {},
   "source": [
    "Each of the different models we are using will use different configurations for training. We will generate them using the `config_dict` variable in the module `config.py`.\n",
    "\n",
    "This information contains topics as important as the model checkpoints to be used. \n",
    "\n",
    "SoDa-RoBERTa has generated a language model [`bio-lm`](https://huggingface.co/EMBO/bio-lm). This model has been initialized from the `roberta-base` checkpoint. It is for this reason that the tokenizer to be used is that of `roberta-base`.\n",
    "\n",
    "In the next line we will load the `bio-lm` checkpoint and the `roberta-base` tokenizer.\n",
    "\n",
    "The dataset for `sd-nlp` we have the data ready to be processed. The next step would be to organize the data into a way that it can be load into batches.\n",
    "\n",
    "This is done with data collators. There is a generic data collator known as `DataCollatorForTokenClassification` in  ðŸ¤— that will do what we need. However, we have a `DataCollatorForMaskedTokenClassification` generated that uses the `tag_mask` column to randomly mask the values. This was done by Thomas. I am assuming the reason behind was to improve the generalization of the task. But this needs to be checked  with him.\n",
    "\n",
    "After checking, it looks like for some reason only the `DataCollatorForMaskedTokenClassification` will work here, so let's keep it as it is and move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92aa22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "# Check the case of the MaskedTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "config = Config(model_type = \"Autoencoder\", from_pretrained = \"EMBO/bio-lm\", tokenizer = 'roberta-base')\n",
    "data_collator_mask = DataCollatorForMaskedTokenClassification(tokenizer=AutoTokenizer.from_pretrained(config.tokenizer), \n",
    "                                                              padding=True,\n",
    "                                                              max_length=512,\n",
    "                                                              pad_to_multiple_of=None,\n",
    "                                                              return_tensors='pt',\n",
    "                                                              masking_probability=0.0,\n",
    "                                                              replacement_probability=0.0,\n",
    "                                                              select_labels=False)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                                   padding=True,\n",
    "                                                   return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "404cc395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1640,   347,    43,  4052,   847, 33101, 43916, 14868,   303,\n",
       "            129,    15, 15145,    36,   571,    35,   361, 25610,     8,  1368,\n",
       "             35,   545, 25610,    43,    15, 36475,    36,   118,    35,   361,\n",
       "          25610,    43,     9,  3186,  3082,     4,   306,     4,   735,  2549,\n",
       "             58,  1455,   129,    15,     5,  5856,     9,     5,   155,  1484,\n",
       "             25, 22827,    13,  3186,  3082,     4,   306,    36,   267,    35,\n",
       "            545, 25610,   322,  1437,     2]]),\n",
       " 'labels': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,\n",
       "           0,  0, 10,  9,  0,  0,  0,  0,  0, 10,  0,  0,  0, 12,  0,  0,  0, 12,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 'tag_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1\n",
    "batch = data_collator([data[\"train\"][i] for i in range(batch_size)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "653f70fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1640,   347,    43,  4052,   847, 33101, 43916, 14868,   303,\n",
       "            129,    15, 15145,    36,   571,    35,   361, 25610,     8,  1368,\n",
       "             35,   545, 25610,    43,    15, 36475,    36,   118,    35,   361,\n",
       "          25610,    43,     9,  3186,  3082,     4,   306,     4,   735,  2549,\n",
       "             58,  1455,   129,    15,     5,  5856,     9,     5,   155,  1484,\n",
       "             25, 22827,    13,  3186,  3082,     4,   306,    36,   267,    35,\n",
       "            545, 25610,   322,  1437,     2]]),\n",
       " 'labels': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,\n",
       "           0,  0, 10,  9,  0,  0,  0,  0,  0, 10,  0,  0,  0, 12,  0,  0,  0, 12,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1\n",
    "batch = data_collator_mask([data[\"train\"][i] for i in range(batch_size)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b986b",
   "metadata": {},
   "source": [
    "After the data collector is time to define the hyperparameters needed by the `Trainer` class of ðŸ¤—. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14764513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "LM_MODEL_PATH = os.getenv('LM_MODEL_PATH')\n",
    "TOKENIZER_PATH = os.getenv('TOKENIZER_PATH')\n",
    "TOKCL_MODEL_PATH = os.getenv('TOKCL_MODEL_PATH')\n",
    "CACHE = os.getenv('CACHE')\n",
    "RUNS_DIR = os.getenv('RUNS_DIR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677371f0",
   "metadata": {},
   "source": [
    "We need to do also a series of important definitions at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3fa39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 15 features:\n",
      "O, I-SMALL_MOLECULE, B-SMALL_MOLECULE, I-GENEPROD, B-GENEPROD, I-SUBCELLULAR, B-SUBCELLULAR, I-CELL, B-CELL, I-TISSUE, B-TISSUE, I-ORGANISM, B-ORGANISM, I-EXP_ASSAY, B-EXP_ASSAY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'I-SMALL_MOLECULE': 1,\n",
       " 'B-SMALL_MOLECULE': 2,\n",
       " 'I-GENEPROD': 3,\n",
       " 'B-GENEPROD': 4,\n",
       " 'I-SUBCELLULAR': 5,\n",
       " 'B-SUBCELLULAR': 6,\n",
       " 'I-CELL': 7,\n",
       " 'B-CELL': 8,\n",
       " 'I-TISSUE': 9,\n",
       " 'B-TISSUE': 10,\n",
       " 'I-ORGANISM': 11,\n",
       " 'B-ORGANISM': 12,\n",
       " 'I-EXP_ASSAY': 13,\n",
       " 'B-EXP_ASSAY': 14}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = train_dataset.info.features['labels'].feature.num_classes\n",
    "label_list = train_dataset.info.features['labels'].feature.names\n",
    "id2label, label2id = {}, {}\n",
    "for class_, label in zip(range(num_labels), label_list):\n",
    "    id2label[class_] = label \n",
    "    label2id[label] = class_ \n",
    "print(f\"\\nTraining on {num_labels} features:\")\n",
    "print(\", \".join(label_list))\n",
    "id2label\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286585db",
   "metadata": {},
   "source": [
    "Let us define now the metrics that will be used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d5d1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.metrics import MetricsTOKCL\n",
    "compute_metrics = MetricsTOKCL(label_list=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d55300f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='/tokcl_models', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='/tokcl_models/runs/May20_08-45-53_5719849be9d3', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1000, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=1, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='/tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=True, resume_from_checkpoint=None, hub_model_id='EMBO/SourceData-NER', hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=False, gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=None, replacement_probability=None, select_labels=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL\n",
    "\n",
    "training_args = TrainingArgumentsTOKCL(\n",
    "    output_dir = TOKCL_MODEL_PATH,\n",
    "    overwrite_output_dir = True,\n",
    "    logging_steps = 1000,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    prediction_loss_only = True,  # crucial to avoid OOM at evaluation stage!\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 10,\n",
    "    masking_probability = None,\n",
    "    replacement_probability = None,\n",
    "    select_labels = False,\n",
    "    per_gpu_train_batch_size=None, \n",
    "    per_gpu_eval_batch_size=None, \n",
    "    gradient_accumulation_steps=1, \n",
    "    eval_accumulation_steps=None, \n",
    "    weight_decay=0.0, \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.999, \n",
    "    adam_epsilon=1e-08, \n",
    "    max_grad_norm=1.0, \n",
    "    max_steps=-1, \n",
    "    lr_scheduler_type='linear', \n",
    "    warmup_ratio=0.0, \n",
    "    warmup_steps=0, \n",
    "    save_strategy='epoch', \n",
    "    save_steps=1, \n",
    "    save_total_limit=5, \n",
    "    save_on_each_node=False, \n",
    "    no_cuda=False, \n",
    "    seed=42, \n",
    "    bf16=False, \n",
    "    fp16=False, \n",
    "    fp16_opt_level='O1', \n",
    "    half_precision_backend='auto', \n",
    "    bf16_full_eval=False, \n",
    "    fp16_full_eval=False, \n",
    "    tf32=None, \n",
    "    local_rank=-1, \n",
    "    xpu_backend=None, \n",
    "    tpu_num_cores=None, \n",
    "    tpu_metrics_debug=False, \n",
    "    debug=[], \n",
    "    dataloader_drop_last=False, \n",
    "    eval_steps=1000, \n",
    "    dataloader_num_workers=0, \n",
    "    past_index=-1, \n",
    "    run_name=TOKCL_MODEL_PATH, \n",
    "    disable_tqdm=False, \n",
    "    remove_unused_columns=True, \n",
    "    label_names=None, \n",
    "    load_best_model_at_end=False, \n",
    "    metric_for_best_model=None, \n",
    "    greater_is_better=None, \n",
    "    ignore_data_skip=False, \n",
    "    sharded_ddp=[], \n",
    "    deepspeed=None, \n",
    "    label_smoothing_factor=0.0, \n",
    "    adafactor=False, \n",
    "    group_by_length=False, \n",
    "    length_column_name='length', \n",
    "    report_to=['tensorboard'], \n",
    "    ddp_find_unused_parameters=None, \n",
    "    ddp_bucket_cap_mb=None, \n",
    "    dataloader_pin_memory=True, \n",
    "    skip_memory_metrics=True, \n",
    "    use_legacy_prediction_loop=False, \n",
    "    push_to_hub=True, \n",
    "    resume_from_checkpoint=None, \n",
    "    hub_model_id=\"EMBO/SourceData-NER\", \n",
    "    hub_strategy='every_save', \n",
    "    hub_token=False, \n",
    "    gradient_checkpointing=False, \n",
    "    fp16_backend='auto', \n",
    "    mp_parameters=''\n",
    "    )\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0914d2",
   "metadata": {},
   "source": [
    "Up to now, we have pre-processed data and load a model. The loaded model has been cropped at the transformer network. For the model to be able to perform a task, we need to provide the model with a model head. \n",
    "\n",
    "The model heads are usually fully connected layers on the top of the transformer network. Although at this point we could perfectly use `torch` to build our own model from the output of the transformers, it has been shown that the performance of fully connected layers is at this point good enough to perform several NLP tasks, including NER.\n",
    "\n",
    "The reason is that the transformer models already encodes several context information on its resulting embeddings. We would therefore not benefit from generating a second RNN or conditional random fields on top, as it was usually done for NER. We will therefore keep it simple and use the fully connected network provided by ðŸ¤—.\n",
    "\n",
    "The way to do so is to load our model, but now using a different class: `AutoModelForTokenClassification`. In this case we use token classification since NER belongs to this task. \n",
    "\n",
    "We need to pass the number of labels. To avoid doing this for every single checkpoint we can do it programatically. The way of getting the number of classes from the training dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04ff68c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "            config.from_pretrained,\n",
    "            num_labels=num_labels,\n",
    "            max_position_embeddings=config.max_length + 2,  # max_length + 2 for start/end token\n",
    "            id2label = id2label,\n",
    "            label2id = label2id\n",
    "        )\n",
    "model_config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd7370b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-SMALL_MOLECULE\",\n",
      "    \"2\": \"B-SMALL_MOLECULE\",\n",
      "    \"3\": \"I-GENEPROD\",\n",
      "    \"4\": \"B-GENEPROD\",\n",
      "    \"5\": \"I-SUBCELLULAR\",\n",
      "    \"6\": \"B-SUBCELLULAR\",\n",
      "    \"7\": \"I-CELL\",\n",
      "    \"8\": \"B-CELL\",\n",
      "    \"9\": \"I-TISSUE\",\n",
      "    \"10\": \"B-TISSUE\",\n",
      "    \"11\": \"I-ORGANISM\",\n",
      "    \"12\": \"B-ORGANISM\",\n",
      "    \"13\": \"I-EXP_ASSAY\",\n",
      "    \"14\": \"B-EXP_ASSAY\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CELL\": 8,\n",
      "    \"B-EXP_ASSAY\": 14,\n",
      "    \"B-GENEPROD\": 4,\n",
      "    \"B-ORGANISM\": 12,\n",
      "    \"B-SMALL_MOLECULE\": 2,\n",
      "    \"B-SUBCELLULAR\": 6,\n",
      "    \"B-TISSUE\": 10,\n",
      "    \"I-CELL\": 7,\n",
      "    \"I-EXP_ASSAY\": 13,\n",
      "    \"I-GENEPROD\": 3,\n",
      "    \"I-ORGANISM\": 11,\n",
      "    \"I-SMALL_MOLECULE\": 1,\n",
      "    \"I-SUBCELLULAR\": 5,\n",
      "    \"I-TISSUE\": 9,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=EMBO/SourceData-NER,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tokcl_models/runs/May20_08-45-53_5719849be9d3,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=None,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=/tokcl_models,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "replacement_probability=None,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=1,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining arguments for model type {config.model_type}:\")\n",
    "print(model_config)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c63edc",
   "metadata": {},
   "source": [
    "#### Training Step 3: Define the `Trainer`\n",
    "\n",
    "We are ready now to define the [`Trainer` class](https://ðŸ¤—.co/docs/transformers/main_classes/trainer). This class is a basic training loop supporting a series of features defined in the documentation. However, it can be further customized. We encourage you to take a look to the documentation and try it. \n",
    "\n",
    "As it is, `trainer.train` would already train our model. However, it would offer only information about the loss during the process. We know that we want the loss to get smaller with time, and ideally, that this is true for both, training and validation datasets. Otherwise we would be incurring in overfitting.\n",
    "\n",
    "What if we want to see other information during training like the accuracy or f1 score? `Trainer` provides an argument `compute_metrics` that will help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14d2554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/EMBO/SourceData-NER into local empty directory.\n",
      "WARNING:Cloning https://huggingface.co/EMBO/SourceData-NER into local empty directory.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '/tokcl_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     subprocess.run(\n\u001b[0m\u001b[1;32m    703\u001b[0m                         \u001b[0;34mf\"{'git clone' if self.skip_lfs_files else 'git lfs clone'} {repo_url} .\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'lfs', 'clone', 'https://huggingface.co/EMBO/SourceData-NER', '.']' returned non-zero exit status 2.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m             self.repo = Repository(\n\u001b[0m\u001b[1;32m   2599\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclone_from\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: WARNING: 'git lfs clone' is deprecated and will not be updated\n          with new flags from 'git clone'\n\n'git clone' has been updated in upstream Git to have comparable\nspeeds to 'git lfs clone'.\nCloning into '.'...\nremote: Repository not found\nfatal: repository 'https://huggingface.co/EMBO/SourceData-NER/' not found\nError(s) during clone:\ngit clone failed: exit status 128\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/2379092524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoardCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# Create clone of distant repo and output directory if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;31m# In case of pull, we need to make sure every process has the latest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m                 \u001b[0;31m# Try again after wiping output_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2606\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2607\u001b[0m                 self.repo = Repository(\n\u001b[1;32m   2608\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    720\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '/tokcl_models'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ShowExampleTOKCL(AutoTokenizer.from_pretrained(config.tokenizer))]\n",
    ")\n",
    "\n",
    "# switch the Tensorboard callback to plot losses on same plot\n",
    "trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33adf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69455720",
   "metadata": {},
   "source": [
    "## Train the models using the general tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25d932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from smtag.metrics import MetricsTOKCL\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from smtag.config import Config\n",
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434c84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_label(label):\n",
    "    # If the label is B-XXX we change it to I-XX\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    Expands the NER tags once the sub-word tokenization is added.\n",
    "    Arguments\n",
    "    ---------\n",
    "    labels list[int]:\n",
    "    word_ids list[int]\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            # As far as word_id matches the index of the current word\n",
    "            # We append the same label\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            new_labels.append(shift_label(labels[word_id]))\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['words'], \n",
    "                       truncation=True,\n",
    "                       is_split_into_words=True)\n",
    "    \n",
    "    all_labels = examples['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66389239",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\"bert-cased\": \"bert-base-cased\", # working\n",
    "               \"bert-uncased\": \"bert-base-uncased\", # working\n",
    "              \"biobert\": \"dmis-lab/biobert-base-cased-v1.1\", # working\n",
    "              \"pubmedbert\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"} # Working on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a90be3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/cda9c238619f77c4dbf65f23ab841b5239f0289800d673957c843b9dda52ad44.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "WARNING:Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8add818ecd4c5ba0d577be1b0d4def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Loading cached processed dataset at /root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581/cache-179a23aafb0e5b9b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea480391d76c4bddb8fbf368750ab60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "WARNING:Loading cached processed dataset at /root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581/cache-c4f3852bd5830ff7.arrow\n"
     ]
    }
   ],
   "source": [
    "checkpoint = checkpoints[\"biobert\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "data = load_dataset(\"EMBO/sd-nlp-non-tokenized\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "train_dataset[0:2]\n",
    "\n",
    "tokenized_data = data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=data['train'].column_names)#,\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                        return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "634c90cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec42f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=IntervalStrategy.NO,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=dmis-lab/biobert-base-cased-v1.1/runs/May20_12-04-59_5719849be9d3,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "output_dir=dmis-lab/biobert-base-cased-v1.1,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=dmis-lab/biobert-base-cased-v1.1,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8934555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ['O', 'I-SMALL_MOLECULE', 'B-SMALL_MOLECULE', 'I-GENEPROD', 'B-GENEPROD', 'I-SUBCELLULAR', 'B-SUBCELLULAR', 'I-CELL', 'B-CELL', 'I-TISSUE', 'B-TISSUE', 'I-ORGANISM', 'B-ORGANISM', 'I-EXP_ASSAY', 'B-EXP_ASSAY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'I-SMALL_MOLECULE',\n",
       " 2: 'B-SMALL_MOLECULE',\n",
       " 3: 'I-GENEPROD',\n",
       " 4: 'B-GENEPROD',\n",
       " 5: 'I-SUBCELLULAR',\n",
       " 6: 'B-SUBCELLULAR',\n",
       " 7: 'I-CELL',\n",
       " 8: 'B-CELL',\n",
       " 9: 'I-TISSUE',\n",
       " 10: 'B-TISSUE',\n",
       " 11: 'I-ORGANISM',\n",
       " 12: 'B-ORGANISM',\n",
       " 13: 'I-EXP_ASSAY',\n",
       " 14: 'B-EXP_ASSAY'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def define_labels(dataset):\n",
    "    num_labels = dataset.info.features['labels'].feature.num_classes\n",
    "    label_list = dataset.info.features['labels'].feature.names\n",
    "    id2label, label2id = {}, {}\n",
    "    print(num_labels, label_list)\n",
    "    for class_, label in zip(range(num_labels), label_list):\n",
    "        id2label[class_] = label \n",
    "        label2id[label] = class_ \n",
    "    return id2label, label2id\n",
    "id2label, label2id = define_labels(tokenized_data['train'])\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceff6245",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/78e6e8ece5b58501028ce314273009ad7707ef4c5ba44251914fd6bca8a05eff.e4a2e693122d98b8b56b7dc1f0d89b644226aacef228afb5030ee3621b2829d3\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-SMALL_MOLECULE\",\n",
      "    \"2\": \"B-SMALL_MOLECULE\",\n",
      "    \"3\": \"I-GENEPROD\",\n",
      "    \"4\": \"B-GENEPROD\",\n",
      "    \"5\": \"I-SUBCELLULAR\",\n",
      "    \"6\": \"B-SUBCELLULAR\",\n",
      "    \"7\": \"I-CELL\",\n",
      "    \"8\": \"B-CELL\",\n",
      "    \"9\": \"I-TISSUE\",\n",
      "    \"10\": \"B-TISSUE\",\n",
      "    \"11\": \"I-ORGANISM\",\n",
      "    \"12\": \"B-ORGANISM\",\n",
      "    \"13\": \"I-EXP_ASSAY\",\n",
      "    \"14\": \"B-EXP_ASSAY\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CELL\": 8,\n",
      "    \"B-EXP_ASSAY\": 14,\n",
      "    \"B-GENEPROD\": 4,\n",
      "    \"B-ORGANISM\": 12,\n",
      "    \"B-SMALL_MOLECULE\": 2,\n",
      "    \"B-SUBCELLULAR\": 6,\n",
      "    \"B-TISSUE\": 10,\n",
      "    \"I-CELL\": 7,\n",
      "    \"I-EXP_ASSAY\": 13,\n",
      "    \"I-GENEPROD\": 3,\n",
      "    \"I-ORGANISM\": 11,\n",
      "    \"I-SMALL_MOLECULE\": 1,\n",
      "    \"I-SUBCELLULAR\": 5,\n",
      "    \"I-TISSUE\": 9,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphwdj30zi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f665cd924e451aad6929f48297ea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n",
      "creating metadata file for /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n",
      "loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f22d3e86464eb4aa2e5f76e0560e9d595617f55e014c78f176b834f9245a62f0.2cf22e1fb1f2db88b971c8da5593b1e94a0ee79d8c892a0f6d4f607d2b77fa23\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(list(id2label.keys())),\n",
    "    max_position_embeddings=max_length,  \n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")\n",
    "model_config = model.config\n",
    "compute_metrics = MetricsTOKCL(label_list=list(label2id.keys()))\n",
    "       \n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# switch the Tensorboard callback to plot losses on same plot\n",
    "trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "#trainer.train()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c4b6ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smtag.metrics.MetricsTOKCL"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403cd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589f856",
   "metadata": {},
   "source": [
    "# Loading BioMegatron\n",
    "\n",
    "* [BioMegatron345mCased](https://catalog.ngc.nvidia.com/orgs/nvidia/models/biomegatron345mcased/files) - Model downloaded\n",
    "* [BioMegatron345mUncased](https://catalog.ngc.nvidia.com/orgs/nvidia/models/biomegatron345muncased/files)\n",
    "* [BioMegatron345m-biovocab-50k-cased](https://catalog.ngc.nvidia.com/orgs/nvidia/models/biomegatron345m_biovocab_50k_cased/files) - Model downloaded\n",
    "* [BioMegatron345m-biovocab-50k-uncased](https://catalog.ngc.nvidia.com/orgs/nvidia/models/biomegatron345m_biovocab_50k_uncased/files)\n",
    "* [BioMegatron345m-biovocab-30k-cased](https://catalog.ngc.nvidia.com/orgs/nvidia/models/biomegatron345m_biovocab_30k_cased/files)\n",
    "* [BioMegatron345m-biovocab-30k-uncased](https://catalog.ngc.nvidia.com/orgs/nvidia/models/biomegatron345m_biovocab_30k_uncased/files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "968eadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76147b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language_model': {'embedding': {'word_embeddings': OrderedDict([('weight',\n",
       "                 tensor([[ 0.0185, -0.0429, -0.0260,  ...,  0.0311, -0.0312, -0.0686],\n",
       "                         [ 0.0243, -0.0040, -0.0945,  ...,  0.0467, -0.0547, -0.0226],\n",
       "                         [ 0.0252, -0.0084, -0.0521,  ...,  0.0382, -0.0627, -0.0886],\n",
       "                         ...,\n",
       "                         [-0.0075,  0.0019, -0.0333,  ..., -0.0016, -0.0105, -0.0500],\n",
       "                         [-0.0059,  0.0071, -0.0423,  ..., -0.0025, -0.0124, -0.0368],\n",
       "                         [ 0.0005, -0.0040, -0.0260,  ..., -0.0439,  0.0103, -0.0366]],\n",
       "                        dtype=torch.float16))]),\n",
       "   'position_embeddings': OrderedDict([('weight',\n",
       "                 tensor([[ 0.0553,  0.0024,  0.0171,  ...,  0.0003, -0.0299,  0.0214],\n",
       "                         [-0.0094, -0.0430,  0.0197,  ...,  0.0025, -0.0072,  0.0046],\n",
       "                         [ 0.0090, -0.0186, -0.0066,  ..., -0.0168, -0.0044, -0.0069],\n",
       "                         ...,\n",
       "                         [-0.0448, -0.0428,  0.0126,  ..., -0.0168,  0.0224,  0.0032],\n",
       "                         [-0.0467, -0.0109,  0.0104,  ...,  0.0051,  0.0394,  0.0011],\n",
       "                         [-0.0515,  0.0513,  0.0240,  ...,  0.0204, -0.0456,  0.1006]],\n",
       "                        dtype=torch.float16))]),\n",
       "   'tokentype_embeddings': OrderedDict([('weight',\n",
       "                 tensor([[ 0.0047,  0.0026, -0.0037,  ..., -0.0019, -0.0014,  0.0034],\n",
       "                         [ 0.0021,  0.0037, -0.0037,  ..., -0.0023, -0.0043,  0.0038]],\n",
       "                        dtype=torch.float16))])},\n",
       "  'transformer': OrderedDict([('layers.0.input_layernorm.weight',\n",
       "                tensor([0.2949, 0.2754, 0.2812,  ..., 0.2991, 0.2778, 0.3103],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.input_layernorm.bias',\n",
       "                tensor([-0.0223,  0.0016, -0.0022,  ..., -0.0080,  0.0286, -0.0186],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.attention.query_key_value.weight',\n",
       "                tensor([[-0.0336, -0.0028, -0.0144,  ..., -0.0451, -0.0117,  0.0859],\n",
       "                        [ 0.0258, -0.0656, -0.0133,  ..., -0.0376,  0.0131,  0.0209],\n",
       "                        [-0.0048,  0.0249, -0.0284,  ..., -0.0016, -0.0750, -0.0110],\n",
       "                        ...,\n",
       "                        [-0.0285,  0.0092, -0.0029,  ...,  0.0234, -0.0325,  0.0079],\n",
       "                        [ 0.0359, -0.0001, -0.0358,  ..., -0.0400,  0.0255,  0.0310],\n",
       "                        [ 0.0333,  0.0137, -0.0386,  ..., -0.0751,  0.0259, -0.0021]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.attention.query_key_value.bias',\n",
       "                tensor([ 0.0319, -0.0452, -0.4368,  ...,  0.0078, -0.0072,  0.0366],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.attention.dense.weight',\n",
       "                tensor([[ 0.0184,  0.0120,  0.0051,  ...,  0.0073,  0.0156, -0.0104],\n",
       "                        [ 0.0126, -0.0079, -0.0025,  ..., -0.0229,  0.0168,  0.0013],\n",
       "                        [ 0.0055,  0.0008, -0.0044,  ..., -0.0024, -0.0254, -0.0453],\n",
       "                        ...,\n",
       "                        [-0.0110,  0.0014,  0.0006,  ...,  0.0061,  0.0030,  0.0010],\n",
       "                        [ 0.0062,  0.0117, -0.0075,  ...,  0.0281,  0.0098,  0.0274],\n",
       "                        [ 0.0058,  0.0070,  0.0200,  ..., -0.0126,  0.0163, -0.0095]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.attention.dense.bias',\n",
       "                tensor([ 0.0038,  0.0027, -0.0012,  ..., -0.0012, -0.0059, -0.0006],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.post_attention_layernorm.weight',\n",
       "                tensor([0.7603, 0.7339, 0.7627,  ..., 0.7642, 0.7373, 0.7510],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.post_attention_layernorm.bias',\n",
       "                tensor([-0.0135, -0.0158, -0.0370,  ..., -0.0650, -0.0301, -0.0235],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0495,  0.0037,  0.0090,  ..., -0.0506, -0.0246,  0.0033],\n",
       "                        [ 0.0482, -0.0737, -0.0040,  ..., -0.0069,  0.0129,  0.0374],\n",
       "                        [-0.0111,  0.0037, -0.0220,  ..., -0.0127,  0.0061,  0.0216],\n",
       "                        ...,\n",
       "                        [ 0.0561,  0.0448,  0.0237,  ...,  0.1069,  0.0152, -0.0206],\n",
       "                        [-0.0968,  0.0598, -0.0430,  ...,  0.0789, -0.0950,  0.0477],\n",
       "                        [-0.0163, -0.0126,  0.0095,  ..., -0.0567, -0.0372, -0.0439]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0402, -0.0434, -0.0682,  ..., -0.0467, -0.0756, -0.0364],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0119, -0.0379, -0.0137,  ...,  0.0081, -0.0640,  0.0205],\n",
       "                        [-0.0144, -0.0294,  0.0424,  ..., -0.0481,  0.0406, -0.0029],\n",
       "                        [ 0.0619,  0.0563, -0.0119,  ...,  0.0186,  0.0751, -0.0227],\n",
       "                        ...,\n",
       "                        [ 0.0082,  0.0240, -0.0342,  ...,  0.0355,  0.0581,  0.0334],\n",
       "                        [ 0.0087,  0.0225,  0.0470,  ..., -0.0178,  0.0649,  0.0254],\n",
       "                        [-0.0094,  0.0521,  0.0078,  ...,  0.0132, -0.0226, -0.0724]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.0.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0147,  0.0052, -0.0775,  ...,  0.0433,  0.0048, -0.0319],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.input_layernorm.weight',\n",
       "                tensor([0.5918, 0.6724, 0.5938,  ..., 0.6348, 0.6733, 0.5918],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.input_layernorm.bias',\n",
       "                tensor([-0.0044, -0.0941, -0.0003,  ..., -0.0532, -0.0516, -0.0674],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.attention.query_key_value.weight',\n",
       "                tensor([[-2.7328e-02, -1.6449e-02, -1.0521e-02,  ...,  3.9581e-02,\n",
       "                          1.2146e-02,  4.6021e-02],\n",
       "                        [-4.1046e-02, -2.4757e-03, -4.0092e-03,  ...,  1.4626e-02,\n",
       "                         -1.8356e-02,  2.4109e-02],\n",
       "                        [ 5.9485e-05,  3.1137e-04, -4.0100e-02,  ..., -6.2561e-02,\n",
       "                         -7.1960e-02, -8.8730e-03],\n",
       "                        ...,\n",
       "                        [-5.3978e-03,  8.6212e-03, -5.1971e-02,  ...,  1.3969e-02,\n",
       "                          2.6184e-02,  1.7487e-02],\n",
       "                        [ 1.4511e-02, -2.4281e-03,  3.3203e-02,  ..., -5.6496e-03,\n",
       "                          1.9169e-03, -5.7449e-03],\n",
       "                        [-3.4393e-02,  1.5160e-02, -7.3547e-03,  ..., -3.9673e-03,\n",
       "                         -3.5339e-02,  3.0991e-02]], dtype=torch.float16)),\n",
       "               ('layers.1.attention.query_key_value.bias',\n",
       "                tensor([ 0.0272, -0.0283,  0.0477,  ..., -0.0792,  0.0085, -0.0567],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.attention.dense.weight',\n",
       "                tensor([[ 0.0140, -0.0141, -0.0608,  ..., -0.0094,  0.0043,  0.0119],\n",
       "                        [ 0.0082,  0.0254,  0.0004,  ...,  0.0181, -0.0148, -0.0388],\n",
       "                        [ 0.0051,  0.0209,  0.0085,  ...,  0.0063, -0.0254,  0.0036],\n",
       "                        ...,\n",
       "                        [ 0.0044, -0.0030,  0.0061,  ..., -0.0035,  0.0106, -0.0095],\n",
       "                        [-0.0122,  0.0056,  0.0011,  ..., -0.0061, -0.0276,  0.0206],\n",
       "                        [-0.0359, -0.0189,  0.0094,  ..., -0.0017,  0.0720, -0.0350]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.attention.dense.bias',\n",
       "                tensor([-0.0238,  0.0038, -0.0209,  ..., -0.0471,  0.0430,  0.0253],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.post_attention_layernorm.weight',\n",
       "                tensor([0.8022, 0.8506, 0.8672,  ..., 0.7808, 0.8428, 0.7578],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.post_attention_layernorm.bias',\n",
       "                tensor([-0.0432,  0.0341, -0.1295,  ..., -0.0238, -0.0223, -0.0159],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0239,  0.0005, -0.0106,  ..., -0.0392,  0.0659, -0.0257],\n",
       "                        [ 0.0400,  0.0168,  0.0168,  ...,  0.0592, -0.0245, -0.0164],\n",
       "                        [ 0.0453, -0.0137,  0.0315,  ...,  0.0329, -0.0490, -0.0055],\n",
       "                        ...,\n",
       "                        [-0.0010, -0.0301,  0.0208,  ...,  0.0584,  0.0031, -0.0679],\n",
       "                        [ 0.0038, -0.0373,  0.1431,  ...,  0.0331,  0.0140, -0.0779],\n",
       "                        [ 0.0686,  0.0618, -0.1023,  ..., -0.0150,  0.0242, -0.0110]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0458,  0.0010, -0.0386,  ...,  0.0617, -0.0404, -0.0489],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0361,  0.0862,  0.0312,  ...,  0.0282,  0.0180,  0.0497],\n",
       "                        [ 0.0350, -0.0553, -0.0059,  ..., -0.0866,  0.0618,  0.0475],\n",
       "                        [ 0.0414,  0.0327,  0.0323,  ...,  0.0333,  0.0476, -0.0527],\n",
       "                        ...,\n",
       "                        [-0.0093, -0.0289, -0.0104,  ...,  0.0643, -0.0383,  0.0169],\n",
       "                        [ 0.0021,  0.0057, -0.0281,  ..., -0.0595,  0.0071,  0.0309],\n",
       "                        [-0.0556, -0.0383, -0.0640,  ..., -0.0060,  0.0409, -0.0301]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.1.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0583,  0.0467, -0.0525,  ...,  0.0347,  0.0207, -0.0341],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.input_layernorm.weight',\n",
       "                tensor([0.7686, 0.8130, 0.7646,  ..., 0.8022, 0.7954, 0.7529],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.input_layernorm.bias',\n",
       "                tensor([ 0.0121, -0.0918,  0.0210,  ..., -0.0349, -0.0784, -0.0728],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0769,  0.0405, -0.0395,  ..., -0.0812, -0.0032,  0.0880],\n",
       "                        [ 0.0459, -0.0007,  0.0233,  ..., -0.0454, -0.0284,  0.0549],\n",
       "                        [-0.0012, -0.0892, -0.0346,  ..., -0.0182, -0.0936, -0.0262],\n",
       "                        ...,\n",
       "                        [ 0.0228, -0.0105, -0.0279,  ...,  0.0142, -0.0041,  0.0305],\n",
       "                        [-0.0308,  0.0079, -0.0074,  ..., -0.0218,  0.0111,  0.0035],\n",
       "                        [ 0.0070, -0.0496, -0.0143,  ..., -0.0361,  0.0282, -0.0147]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.attention.query_key_value.bias',\n",
       "                tensor([-0.0186, -0.0002, -0.0127,  ..., -0.0263, -0.0298, -0.0068],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.attention.dense.weight',\n",
       "                tensor([[-0.0122,  0.0044,  0.0131,  ..., -0.0103,  0.0205, -0.0162],\n",
       "                        [-0.0363,  0.0469,  0.0485,  ..., -0.0199,  0.0176,  0.0281],\n",
       "                        [ 0.0169,  0.0175,  0.0195,  ...,  0.0190,  0.0106,  0.0010],\n",
       "                        ...,\n",
       "                        [-0.0144,  0.0532,  0.0013,  ..., -0.0410,  0.0003,  0.0087],\n",
       "                        [-0.0257,  0.0371, -0.0528,  ..., -0.0041, -0.0137, -0.0296],\n",
       "                        [-0.0098, -0.0478,  0.0053,  ...,  0.0088, -0.0140,  0.0249]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.attention.dense.bias',\n",
       "                tensor([ 0.0422, -0.0140, -0.0446,  ...,  0.0384, -0.0426,  0.0222],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.post_attention_layernorm.weight',\n",
       "                tensor([0.9751, 0.9570, 0.9512,  ..., 0.9092, 0.9009, 0.8682],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.post_attention_layernorm.bias',\n",
       "                tensor([-0.0723,  0.0651, -0.0934,  ...,  0.0303, -0.0099, -0.0372],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0493, -0.0742,  0.0891,  ..., -0.0354, -0.0074,  0.0053],\n",
       "                        [-0.0515, -0.0341, -0.0184,  ..., -0.0737, -0.0357,  0.0792],\n",
       "                        [-0.0041,  0.0162, -0.0598,  ..., -0.0059,  0.0033, -0.0250],\n",
       "                        ...,\n",
       "                        [ 0.0294,  0.0347,  0.0229,  ..., -0.0257,  0.1092, -0.0376],\n",
       "                        [ 0.0010,  0.0044,  0.0588,  ..., -0.0416,  0.0236, -0.0595],\n",
       "                        [-0.0088,  0.0248,  0.0337,  ...,  0.0659, -0.0503,  0.0123]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0244, -0.0363, -0.0138,  ..., -0.0230, -0.0728, -0.0464],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0343, -0.0421,  0.0141,  ...,  0.0094, -0.0005, -0.0154],\n",
       "                        [ 0.0748, -0.0194,  0.0041,  ..., -0.0320, -0.0319, -0.0629],\n",
       "                        [-0.0363,  0.0488,  0.0366,  ...,  0.0294, -0.0488,  0.0500],\n",
       "                        ...,\n",
       "                        [ 0.0275, -0.0483,  0.0181,  ...,  0.0300,  0.0365,  0.0332],\n",
       "                        [-0.0044, -0.0092,  0.0127,  ...,  0.0626, -0.0342, -0.0495],\n",
       "                        [-0.0389,  0.0890,  0.0207,  ..., -0.0502,  0.0275, -0.0630]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.2.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0468,  0.0186, -0.0246,  ...,  0.0345,  0.0329, -0.0192],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.input_layernorm.weight',\n",
       "                tensor([0.8926, 0.8789, 0.7910,  ..., 0.9121, 0.8325, 0.7842],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.input_layernorm.bias',\n",
       "                tensor([ 0.0152, -0.0764,  0.0085,  ..., -0.0165, -0.1031, -0.0822],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.attention.query_key_value.weight',\n",
       "                tensor([[-0.0291,  0.0559, -0.0334,  ...,  0.0146,  0.0238, -0.0157],\n",
       "                        [-0.0168, -0.0578, -0.0100,  ..., -0.0522, -0.0231,  0.0651],\n",
       "                        [ 0.0544, -0.0246, -0.0325,  ..., -0.0298,  0.0338,  0.0298],\n",
       "                        ...,\n",
       "                        [-0.0032, -0.0247,  0.0141,  ...,  0.0110,  0.0410, -0.0756],\n",
       "                        [-0.0833, -0.0266, -0.0519,  ..., -0.0281,  0.0385, -0.0108],\n",
       "                        [ 0.0025,  0.0225,  0.0250,  ...,  0.0208, -0.0381,  0.0326]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.attention.query_key_value.bias',\n",
       "                tensor([ 0.0256,  0.0533,  0.0429,  ...,  0.0030, -0.0170,  0.0115],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.attention.dense.weight',\n",
       "                tensor([[ 0.0080, -0.0146, -0.0429,  ..., -0.0292, -0.0189, -0.0054],\n",
       "                        [-0.0011, -0.0080,  0.0192,  ...,  0.0180, -0.0132,  0.0153],\n",
       "                        [-0.0308, -0.0525, -0.0424,  ...,  0.0541, -0.0343, -0.0026],\n",
       "                        ...,\n",
       "                        [-0.0056, -0.0220,  0.0049,  ..., -0.0179,  0.0287, -0.0059],\n",
       "                        [-0.0548, -0.0159,  0.0034,  ..., -0.0054, -0.0546,  0.0291],\n",
       "                        [ 0.0025, -0.0728,  0.0212,  ...,  0.0247,  0.0299, -0.0230]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.attention.dense.bias',\n",
       "                tensor([ 0.0365,  0.0228, -0.0105,  ...,  0.0235, -0.0413, -0.0300],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.post_attention_layernorm.weight',\n",
       "                tensor([0.9380, 1.0693, 0.9595,  ..., 0.9956, 0.9912, 0.8496],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.post_attention_layernorm.bias',\n",
       "                tensor([-0.0598,  0.0341, -0.0610,  ...,  0.0333,  0.0033, -0.0313],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0039, -0.0546,  0.0282,  ..., -0.0187,  0.0125, -0.0748],\n",
       "                        [ 0.0487,  0.0798, -0.0044,  ...,  0.0084, -0.0515, -0.0323],\n",
       "                        [-0.0252, -0.0032,  0.0481,  ...,  0.0418, -0.0114,  0.0293],\n",
       "                        ...,\n",
       "                        [ 0.0665, -0.0630,  0.0443,  ..., -0.0055, -0.0627, -0.0428],\n",
       "                        [ 0.0065, -0.0221, -0.0103,  ..., -0.0847, -0.0189,  0.1135],\n",
       "                        [ 0.0419,  0.0456, -0.0277,  ...,  0.0261, -0.0044, -0.0026]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0564, -0.0554, -0.0415,  ..., -0.0505, -0.0425, -0.0364],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0606, -0.0449,  0.0014,  ...,  0.0461, -0.1027, -0.0542],\n",
       "                        [ 0.0561,  0.0037, -0.0095,  ..., -0.0567, -0.0583, -0.0450],\n",
       "                        [ 0.1011,  0.0361, -0.0177,  ...,  0.0744, -0.0089,  0.0462],\n",
       "                        ...,\n",
       "                        [-0.0228,  0.0956, -0.0136,  ..., -0.0017,  0.0011, -0.0304],\n",
       "                        [-0.0878,  0.0404, -0.0075,  ..., -0.0391, -0.0490,  0.0027],\n",
       "                        [-0.0138, -0.0024, -0.0260,  ...,  0.0081, -0.0084, -0.0580]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.3.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0424,  0.0119, -0.0150,  ..., -0.0255,  0.0296,  0.0102],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.input_layernorm.weight',\n",
       "                tensor([0.8569, 0.9111, 0.7358,  ..., 0.8350, 0.8086, 0.7754],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.input_layernorm.bias',\n",
       "                tensor([ 0.0090, -0.0444,  0.0008,  ..., -0.0138, -0.0933, -0.0901],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0416,  0.0812,  0.0282,  ..., -0.0260, -0.0822, -0.0587],\n",
       "                        [ 0.0042,  0.0267, -0.0276,  ..., -0.0452, -0.0144,  0.0111],\n",
       "                        [-0.0284, -0.0482, -0.0082,  ...,  0.0023,  0.0210, -0.0623],\n",
       "                        ...,\n",
       "                        [-0.0026,  0.0134,  0.0195,  ..., -0.0934, -0.0318,  0.0671],\n",
       "                        [-0.0051, -0.0525,  0.0294,  ..., -0.0182, -0.0452,  0.0208],\n",
       "                        [-0.0454, -0.0401, -0.0184,  ...,  0.0254,  0.0011,  0.1036]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.attention.query_key_value.bias',\n",
       "                tensor([-0.0167, -0.0929, -0.0619,  ...,  0.0008,  0.0210,  0.0204],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.attention.dense.weight',\n",
       "                tensor([[ 0.0226, -0.0290, -0.0154,  ...,  0.0317, -0.0093,  0.0331],\n",
       "                        [ 0.0244, -0.0359,  0.0342,  ..., -0.0190,  0.0358, -0.0146],\n",
       "                        [ 0.0501, -0.0033,  0.0089,  ..., -0.0021,  0.0177, -0.0258],\n",
       "                        ...,\n",
       "                        [-0.0213, -0.0108,  0.0124,  ...,  0.0143,  0.0259, -0.0638],\n",
       "                        [-0.0357,  0.0327,  0.0486,  ...,  0.0429,  0.0426, -0.0762],\n",
       "                        [-0.0364, -0.0029, -0.0171,  ..., -0.0553,  0.0066, -0.0462]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.attention.dense.bias',\n",
       "                tensor([ 0.0077, -0.0966,  0.0212,  ..., -0.0130, -0.0275,  0.0116],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.post_attention_layernorm.weight',\n",
       "                tensor([1.0127, 1.0488, 0.9434,  ..., 0.9473, 1.0117, 0.8940],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.post_attention_layernorm.bias',\n",
       "                tensor([-0.0772,  0.0441, -0.0640,  ..., -0.0147,  0.0076,  0.0133],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0356, -0.0392, -0.0299,  ...,  0.0809,  0.0163, -0.0230],\n",
       "                        [-0.0264, -0.0028,  0.0583,  ...,  0.0174,  0.0492,  0.0169],\n",
       "                        [-0.0563,  0.0431,  0.0248,  ...,  0.0008, -0.0315,  0.0090],\n",
       "                        ...,\n",
       "                        [ 0.0186, -0.0424, -0.0318,  ...,  0.0527,  0.0550, -0.0067],\n",
       "                        [-0.0246,  0.0707,  0.0349,  ..., -0.0281, -0.0358, -0.0021],\n",
       "                        [ 0.0290,  0.0358, -0.0552,  ...,  0.0253, -0.0161,  0.0242]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0368, -0.0603, -0.0379,  ..., -0.0479, -0.0453, -0.0304],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0168, -0.0489,  0.0845,  ..., -0.0175, -0.0136, -0.0321],\n",
       "                        [-0.0515, -0.0135, -0.0230,  ..., -0.0380, -0.0221, -0.0122],\n",
       "                        [ 0.0050,  0.1126, -0.0282,  ..., -0.0055,  0.0171,  0.0269],\n",
       "                        ...,\n",
       "                        [ 0.0378,  0.0516,  0.0006,  ...,  0.0105, -0.0040, -0.0344],\n",
       "                        [-0.0045,  0.0034,  0.0192,  ..., -0.0456, -0.0125,  0.0138],\n",
       "                        [-0.0959, -0.0340, -0.0070,  ...,  0.0410,  0.0263, -0.0293]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.4.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0310,  0.0405, -0.0009,  ..., -0.0177,  0.0237,  0.0131],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.input_layernorm.weight',\n",
       "                tensor([0.8838, 0.9771, 0.8315,  ..., 0.9077, 0.8877, 0.8364],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.input_layernorm.bias',\n",
       "                tensor([-0.0091, -0.0332,  0.0035,  ..., -0.0162, -0.0939, -0.0779],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.attention.query_key_value.weight',\n",
       "                tensor([[-0.0194,  0.0346, -0.0015,  ...,  0.0592,  0.0052, -0.0587],\n",
       "                        [-0.0497, -0.0798, -0.0194,  ..., -0.0540,  0.0837,  0.0279],\n",
       "                        [-0.0144, -0.0095,  0.0752,  ...,  0.0405, -0.0005, -0.0750],\n",
       "                        ...,\n",
       "                        [ 0.0246, -0.0767,  0.0490,  ..., -0.0234,  0.0014, -0.0037],\n",
       "                        [-0.0215,  0.0119, -0.0299,  ...,  0.0063, -0.0109,  0.0012],\n",
       "                        [ 0.0737, -0.0257, -0.0094,  ...,  0.0112, -0.0035, -0.0135]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.attention.query_key_value.bias',\n",
       "                tensor([ 0.0038,  0.0105,  0.0022,  ...,  0.0016,  0.0261, -0.0170],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.attention.dense.weight',\n",
       "                tensor([[-0.0183, -0.0261,  0.0093,  ..., -0.0909,  0.0346, -0.0255],\n",
       "                        [-0.0923, -0.0249, -0.0359,  ..., -0.0074,  0.0370, -0.0232],\n",
       "                        [ 0.0277,  0.0463, -0.0129,  ...,  0.0172,  0.0284,  0.0313],\n",
       "                        ...,\n",
       "                        [-0.0531, -0.0248,  0.0087,  ..., -0.0421,  0.0204, -0.0231],\n",
       "                        [-0.0712, -0.0061, -0.0258,  ...,  0.0016,  0.0234, -0.0140],\n",
       "                        [-0.0077, -0.0140,  0.0682,  ...,  0.0138,  0.0218,  0.0228]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.attention.dense.bias',\n",
       "                tensor([ 0.0534,  0.0402, -0.0157,  ...,  0.0240,  0.0156, -0.0243],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.post_attention_layernorm.weight',\n",
       "                tensor([1.0459, 1.0488, 0.9648,  ..., 1.0518, 1.0615, 0.8857],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.post_attention_layernorm.bias',\n",
       "                tensor([-0.0247,  0.0556, -0.0755,  ..., -0.0033,  0.0294,  0.0156],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0478, -0.0281,  0.0283,  ..., -0.0137,  0.0275,  0.1224],\n",
       "                        [ 0.0244, -0.1212,  0.0109,  ...,  0.0448,  0.0082, -0.0587],\n",
       "                        [ 0.0342,  0.0170, -0.0593,  ..., -0.0377,  0.0374, -0.0343],\n",
       "                        ...,\n",
       "                        [ 0.0185,  0.0649,  0.0072,  ..., -0.0273,  0.0432, -0.0171],\n",
       "                        [-0.0042, -0.0144,  0.0173,  ..., -0.0273, -0.0568, -0.0264],\n",
       "                        [ 0.0018, -0.0276,  0.1035,  ..., -0.0048, -0.0461,  0.0286]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0289, -0.0739, -0.0671,  ...,  0.0399, -0.0380, -0.0273],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0352,  0.0388,  0.0120,  ...,  0.0192,  0.0341,  0.0056],\n",
       "                        [-0.0565,  0.0189,  0.0002,  ...,  0.0589,  0.0101,  0.0515],\n",
       "                        [ 0.0410, -0.0151, -0.0401,  ..., -0.0243,  0.0092, -0.0259],\n",
       "                        ...,\n",
       "                        [ 0.0490,  0.0028,  0.0003,  ...,  0.0320,  0.0518,  0.0276],\n",
       "                        [ 0.0309,  0.0932,  0.0355,  ..., -0.0327,  0.0624,  0.0410],\n",
       "                        [-0.0297,  0.0353,  0.0012,  ..., -0.0318,  0.0095, -0.0375]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.5.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0211,  0.0221, -0.0062,  ..., -0.0442,  0.0399,  0.0313],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.input_layernorm.weight',\n",
       "                tensor([0.8916, 0.8740, 0.7881,  ..., 0.9150, 0.8799, 0.7930],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.input_layernorm.bias',\n",
       "                tensor([ 2.3232e-03, -2.5574e-02,  9.4271e-04,  ..., -4.3511e-06,\n",
       "                        -7.4951e-02, -6.8481e-02], dtype=torch.float16)),\n",
       "               ('layers.6.attention.query_key_value.weight',\n",
       "                tensor([[-0.0260, -0.0665, -0.0627,  ...,  0.0504,  0.0532,  0.0065],\n",
       "                        [ 0.1223, -0.0353, -0.0987,  ...,  0.0366, -0.0520,  0.0360],\n",
       "                        [-0.0682,  0.0461,  0.0891,  ...,  0.0058, -0.0020,  0.0167],\n",
       "                        ...,\n",
       "                        [-0.0667,  0.0200,  0.0504,  ...,  0.0216,  0.0146, -0.0398],\n",
       "                        [ 0.0228, -0.0277,  0.0334,  ...,  0.0017, -0.0939, -0.0402],\n",
       "                        [ 0.0068, -0.0142, -0.0409,  ...,  0.0468,  0.0472, -0.0372]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.attention.query_key_value.bias',\n",
       "                tensor([ 0.0685,  0.0072, -0.0065,  ...,  0.0242,  0.0154, -0.0156],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.attention.dense.weight',\n",
       "                tensor([[-0.0439,  0.0122,  0.0710,  ..., -0.0012,  0.0252,  0.0403],\n",
       "                        [ 0.0039, -0.0224,  0.0149,  ..., -0.0519, -0.0351,  0.0287],\n",
       "                        [-0.0050, -0.0335,  0.0030,  ...,  0.0221, -0.0522,  0.0156],\n",
       "                        ...,\n",
       "                        [-0.0729,  0.0102,  0.0250,  ...,  0.0443,  0.0733,  0.0700],\n",
       "                        [-0.0111,  0.0024,  0.0308,  ..., -0.0061,  0.0562, -0.0013],\n",
       "                        [ 0.0183,  0.0383, -0.0532,  ..., -0.0014, -0.0489, -0.0183]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.attention.dense.bias',\n",
       "                tensor([ 0.0346,  0.0079, -0.0150,  ..., -0.0148,  0.0224,  0.0507],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.post_attention_layernorm.weight',\n",
       "                tensor([0.9980, 0.9785, 0.9854,  ..., 1.0439, 1.0215, 0.9272],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.post_attention_layernorm.bias',\n",
       "                tensor([-0.0122,  0.0459, -0.0643,  ..., -0.0288,  0.0764,  0.0246],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 2.2476e-02,  1.7487e-02,  2.3537e-03,  ...,  3.5553e-02,\n",
       "                          5.1910e-02, -9.2407e-02],\n",
       "                        [-1.9623e-02, -3.2715e-02, -7.0143e-04,  ..., -3.6407e-02,\n",
       "                         -1.1688e-02, -2.9236e-02],\n",
       "                        [ 7.7784e-05,  2.0309e-02,  3.2349e-02,  ..., -6.5613e-02,\n",
       "                          1.0979e-02, -3.8513e-02],\n",
       "                        ...,\n",
       "                        [-5.9509e-03, -5.7983e-02,  3.4454e-02,  ...,  6.2370e-03,\n",
       "                         -2.5864e-02, -6.3660e-02],\n",
       "                        [-1.1292e-02, -5.3650e-02, -1.5610e-02,  ...,  1.3245e-01,\n",
       "                         -4.7272e-02,  5.3635e-03],\n",
       "                        [ 5.0995e-02, -4.2450e-02,  1.2878e-01,  ...,  3.7567e-02,\n",
       "                         -2.9175e-02,  3.6865e-02]], dtype=torch.float16)),\n",
       "               ('layers.6.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0120, -0.0865, -0.0363,  ..., -0.0311, -0.0075, -0.0604],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0359,  0.0330, -0.0999,  ...,  0.0027,  0.0264, -0.0359],\n",
       "                        [ 0.0064,  0.0506,  0.0421,  ..., -0.0400, -0.0017, -0.0254],\n",
       "                        [-0.0497, -0.0307, -0.0591,  ...,  0.0737,  0.0476,  0.1432],\n",
       "                        ...,\n",
       "                        [ 0.0117,  0.0396,  0.0373,  ...,  0.0423, -0.0640,  0.0583],\n",
       "                        [-0.0217, -0.0125, -0.0392,  ..., -0.0333, -0.0191, -0.0177],\n",
       "                        [ 0.0382,  0.0093,  0.0171,  ..., -0.0318, -0.0215, -0.0467]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.6.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0239,  0.0212, -0.0213,  ..., -0.0137,  0.0750,  0.0509],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.input_layernorm.weight',\n",
       "                tensor([0.9302, 0.9277, 0.8525,  ..., 0.9756, 0.9067, 0.8804],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.input_layernorm.bias',\n",
       "                tensor([-0.0028, -0.0242,  0.0105,  ...,  0.0101, -0.0634, -0.0598],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.attention.query_key_value.weight',\n",
       "                tensor([[-0.0352, -0.0465, -0.0181,  ..., -0.0264, -0.0100, -0.0873],\n",
       "                        [ 0.1215,  0.0409,  0.0007,  ...,  0.0353, -0.0219, -0.0464],\n",
       "                        [ 0.0447,  0.0905, -0.0276,  ..., -0.0289, -0.0035, -0.0938],\n",
       "                        ...,\n",
       "                        [ 0.0124,  0.0367, -0.0446,  ...,  0.0435, -0.0145,  0.0042],\n",
       "                        [ 0.0385,  0.0140, -0.0077,  ..., -0.0895,  0.0282, -0.0395],\n",
       "                        [ 0.0952,  0.0109,  0.0021,  ...,  0.0424,  0.0009,  0.0418]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.attention.query_key_value.bias',\n",
       "                tensor([-0.0975, -0.0954, -0.0648,  ...,  0.0069, -0.0162, -0.0018],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.attention.dense.weight',\n",
       "                tensor([[-0.0413,  0.0202, -0.0623,  ...,  0.0004,  0.0394, -0.0179],\n",
       "                        [ 0.0242,  0.0757,  0.0416,  ..., -0.0218, -0.0063,  0.0009],\n",
       "                        [ 0.0826,  0.0444,  0.0078,  ..., -0.0182,  0.0375, -0.0655],\n",
       "                        ...,\n",
       "                        [-0.0167, -0.0601, -0.0419,  ..., -0.0169,  0.0268, -0.0173],\n",
       "                        [ 0.0352,  0.0126, -0.0122,  ..., -0.0275,  0.0213,  0.0080],\n",
       "                        [-0.0216,  0.0582, -0.0274,  ..., -0.0632, -0.0385,  0.0469]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.attention.dense.bias',\n",
       "                tensor([-0.0209,  0.0270, -0.0734,  ..., -0.0699,  0.0287, -0.0046],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.post_attention_layernorm.weight',\n",
       "                tensor([1.0146, 0.9727, 0.9551,  ..., 1.0596, 1.0127, 0.9224],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.post_attention_layernorm.bias',\n",
       "                tensor([-0.0452,  0.0221, -0.0710,  ..., -0.0182,  0.0726,  0.0841],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0984,  0.0456, -0.0461,  ...,  0.0165,  0.0241, -0.0159],\n",
       "                        [ 0.0431, -0.0446, -0.0128,  ..., -0.0255,  0.0469, -0.0299],\n",
       "                        [ 0.0233,  0.0123, -0.0120,  ...,  0.0252, -0.0069, -0.0848],\n",
       "                        ...,\n",
       "                        [ 0.0602, -0.0713,  0.0930,  ...,  0.0678,  0.0035, -0.0207],\n",
       "                        [-0.0110, -0.0883,  0.0188,  ...,  0.0281,  0.0144, -0.0009],\n",
       "                        [-0.0311, -0.0712,  0.0013,  ..., -0.0374, -0.0784, -0.0157]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0719, -0.0577, -0.0599,  ..., -0.0631, -0.0157, -0.0508],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0598,  0.0408,  0.0047,  ...,  0.0319, -0.0119,  0.0016],\n",
       "                        [ 0.0419, -0.0056,  0.0067,  ..., -0.0012,  0.0508, -0.0256],\n",
       "                        [-0.0105,  0.0072,  0.0972,  ...,  0.0384, -0.0184,  0.0138],\n",
       "                        ...,\n",
       "                        [-0.0171, -0.0880, -0.0246,  ...,  0.0494, -0.0352, -0.0088],\n",
       "                        [ 0.0359,  0.0212,  0.0253,  ..., -0.0105, -0.0029, -0.0528],\n",
       "                        [-0.0182, -0.0009,  0.0311,  ...,  0.0038, -0.0094,  0.0063]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.7.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0540, -0.0210, -0.0122,  ..., -0.0334,  0.0751,  0.0569],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.input_layernorm.weight',\n",
       "                tensor([0.9487, 0.9727, 0.9185,  ..., 1.0566, 0.9785, 0.9023],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.input_layernorm.bias',\n",
       "                tensor([-0.0072, -0.0208, -0.0092,  ..., -0.0047, -0.0635, -0.0439],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.attention.query_key_value.weight',\n",
       "                tensor([[-0.0163,  0.0374,  0.0751,  ..., -0.0071,  0.0519, -0.0638],\n",
       "                        [ 0.0179, -0.0707,  0.0226,  ..., -0.0114,  0.0289,  0.0133],\n",
       "                        [-0.0928,  0.0171,  0.0056,  ...,  0.0472,  0.0399, -0.0022],\n",
       "                        ...,\n",
       "                        [ 0.0007,  0.0240, -0.0433,  ...,  0.0254, -0.0914,  0.0120],\n",
       "                        [-0.0957, -0.0338,  0.0113,  ..., -0.1196, -0.0509,  0.0465],\n",
       "                        [ 0.0213, -0.0311, -0.0241,  ..., -0.0060, -0.0469,  0.0303]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.attention.query_key_value.bias',\n",
       "                tensor([ 0.0141,  0.0544, -0.0131,  ..., -0.0418, -0.0283, -0.0325],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.attention.dense.weight',\n",
       "                tensor([[-0.0283, -0.0635,  0.0355,  ...,  0.0177, -0.0308,  0.0078],\n",
       "                        [ 0.0167,  0.0685, -0.0743,  ...,  0.0035,  0.0062, -0.0180],\n",
       "                        [-0.0005, -0.0281, -0.0146,  ...,  0.0632,  0.0191, -0.0387],\n",
       "                        ...,\n",
       "                        [ 0.0606, -0.0113, -0.0107,  ...,  0.0535,  0.0171,  0.0343],\n",
       "                        [ 0.0073, -0.0273, -0.0067,  ..., -0.1057,  0.0583,  0.0452],\n",
       "                        [-0.0235,  0.0473,  0.0236,  ..., -0.0757, -0.0270,  0.0588]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.attention.dense.bias',\n",
       "                tensor([-3.5405e-05,  2.5909e-02, -3.8391e-02,  ...,  3.2673e-03,\n",
       "                        -3.0365e-02,  4.0680e-02], dtype=torch.float16)),\n",
       "               ('layers.8.post_attention_layernorm.weight',\n",
       "                tensor([0.9722, 0.9492, 0.9487,  ..., 1.0215, 1.0000, 0.9263],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.post_attention_layernorm.bias',\n",
       "                tensor([-0.0006, -0.0200, -0.0891,  ..., -0.0449,  0.0571,  0.0875],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0626, -0.0319,  0.0977,  ...,  0.0394, -0.0211, -0.0172],\n",
       "                        [ 0.0228,  0.0500, -0.0030,  ...,  0.0054, -0.0055, -0.0008],\n",
       "                        [ 0.0336,  0.1449, -0.0166,  ..., -0.0883, -0.0632,  0.0306],\n",
       "                        ...,\n",
       "                        [-0.0206, -0.0117, -0.0496,  ...,  0.0345,  0.0002, -0.0671],\n",
       "                        [-0.0686,  0.0365,  0.0449,  ...,  0.0614, -0.0201, -0.0485],\n",
       "                        [ 0.0064, -0.0296,  0.0667,  ...,  0.0895, -0.0619,  0.0077]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0500, -0.0209, -0.0488,  ..., -0.0600, -0.0296, -0.0509],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0454, -0.1176, -0.0043,  ...,  0.0145,  0.0753,  0.0146],\n",
       "                        [-0.0501, -0.0025,  0.0477,  ..., -0.0310,  0.0292,  0.0028],\n",
       "                        [-0.0067, -0.0665,  0.0144,  ..., -0.0555,  0.0008, -0.0194],\n",
       "                        ...,\n",
       "                        [-0.0032,  0.0045, -0.0385,  ..., -0.0341, -0.0470,  0.0703],\n",
       "                        [-0.0256,  0.0466, -0.0345,  ...,  0.0455, -0.0070,  0.0372],\n",
       "                        [-0.0094,  0.0073, -0.0206,  ...,  0.0170,  0.0640,  0.0432]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.8.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0516,  0.0022,  0.0153,  ..., -0.0105,  0.0803,  0.0320],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.input_layernorm.weight',\n",
       "                tensor([0.8965, 0.9653, 0.9189,  ..., 1.0156, 1.0205, 0.8647],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.input_layernorm.bias',\n",
       "                tensor([-0.0006, -0.0044,  0.0005,  ...,  0.0010, -0.0508, -0.0484],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.attention.query_key_value.weight',\n",
       "                tensor([[-0.0847, -0.0066,  0.0520,  ..., -0.0776, -0.1005, -0.0469],\n",
       "                        [ 0.0354,  0.0341,  0.0162,  ..., -0.0163, -0.0556,  0.0226],\n",
       "                        [-0.0066, -0.0536,  0.0115,  ..., -0.0269,  0.0194,  0.0297],\n",
       "                        ...,\n",
       "                        [-0.0030, -0.0213,  0.0648,  ..., -0.0457, -0.0259,  0.0468],\n",
       "                        [-0.0259,  0.0285, -0.0629,  ..., -0.0398, -0.0347,  0.0276],\n",
       "                        [-0.0084,  0.0314, -0.0503,  ...,  0.0287, -0.0124,  0.0455]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.attention.query_key_value.bias',\n",
       "                tensor([-0.0275, -0.0135, -0.0321,  ..., -0.0078, -0.0011,  0.0234],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.attention.dense.weight',\n",
       "                tensor([[ 0.0910,  0.0115, -0.0526,  ...,  0.0322,  0.0513, -0.0607],\n",
       "                        [-0.0374, -0.0345, -0.0041,  ...,  0.0436, -0.0729, -0.0463],\n",
       "                        [ 0.0260, -0.0526,  0.0594,  ...,  0.0349,  0.0470, -0.0361],\n",
       "                        ...,\n",
       "                        [-0.0369, -0.0486, -0.0399,  ...,  0.0106,  0.0271,  0.0572],\n",
       "                        [ 0.0048, -0.0160,  0.0202,  ...,  0.1256, -0.0265, -0.0361],\n",
       "                        [-0.0325, -0.0637,  0.0271,  ..., -0.0179, -0.0102, -0.0546]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.attention.dense.bias',\n",
       "                tensor([-0.0311, -0.0430,  0.0127,  ..., -0.0267,  0.0314,  0.0560],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.post_attention_layernorm.weight',\n",
       "                tensor([0.9839, 0.9360, 0.9487,  ..., 0.9678, 0.9814, 0.8926],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.post_attention_layernorm.bias',\n",
       "                tensor([-0.0255, -0.0082, -0.0486,  ..., -0.0760,  0.0454,  0.0906],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 3.9764e-02, -4.3335e-03, -1.0048e-02,  ..., -2.6062e-02,\n",
       "                          1.5656e-02, -4.1840e-02],\n",
       "                        [-6.9046e-03,  3.6713e-02,  3.4332e-02,  ...,  1.4160e-02,\n",
       "                          5.8716e-02, -7.0152e-03],\n",
       "                        [ 2.8168e-02,  9.1736e-02,  3.2379e-02,  ...,  1.4626e-02,\n",
       "                         -2.3651e-02,  2.8515e-03],\n",
       "                        ...,\n",
       "                        [ 3.6713e-02, -1.4793e-02,  6.0852e-02,  ..., -2.7039e-02,\n",
       "                         -5.3772e-02,  1.1627e-02],\n",
       "                        [-3.2349e-02, -4.4373e-02, -4.7363e-02,  ...,  7.9956e-03,\n",
       "                          2.7863e-02,  4.2175e-02],\n",
       "                        [ 3.7018e-02, -2.0432e-02, -9.7513e-05,  ...,  2.0782e-02,\n",
       "                          1.5564e-03,  1.3626e-02]], dtype=torch.float16)),\n",
       "               ('layers.9.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0287, -0.0443, -0.0225,  ..., -0.0080, -0.0222, -0.0352],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0057, -0.0221, -0.0443,  ..., -0.0769,  0.0294, -0.0131],\n",
       "                        [ 0.0281, -0.0500, -0.0202,  ..., -0.0099,  0.0213, -0.0094],\n",
       "                        [ 0.0435,  0.0295, -0.0259,  ..., -0.0590,  0.0697,  0.0453],\n",
       "                        ...,\n",
       "                        [-0.0035,  0.0012, -0.0326,  ...,  0.0095, -0.0205,  0.0330],\n",
       "                        [-0.0422,  0.0044,  0.0459,  ...,  0.0723, -0.0717, -0.0172],\n",
       "                        [ 0.0267,  0.0126,  0.0041,  ..., -0.0006, -0.0278, -0.0159]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.9.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0909, -0.0309, -0.0119,  ..., -0.0321,  0.0485,  0.0631],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.input_layernorm.weight',\n",
       "                tensor([0.9937, 1.0166, 0.9624,  ..., 1.0537, 1.0547, 0.9321],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.input_layernorm.bias',\n",
       "                tensor([-0.0230, -0.0206,  0.0072,  ..., -0.0017, -0.0325, -0.0388],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0242,  0.0175, -0.0169,  ...,  0.0580, -0.0142,  0.0105],\n",
       "                        [ 0.0664,  0.0025, -0.0562,  ..., -0.0132, -0.0154,  0.1149],\n",
       "                        [-0.0300,  0.0021,  0.0349,  ..., -0.0131,  0.0656,  0.0734],\n",
       "                        ...,\n",
       "                        [ 0.0043, -0.0445, -0.0361,  ..., -0.0573, -0.0263,  0.0108],\n",
       "                        [ 0.0136, -0.0497, -0.0385,  ..., -0.0056, -0.0378,  0.0743],\n",
       "                        [-0.0258,  0.0502,  0.0540,  ..., -0.0003,  0.0625, -0.0005]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.attention.query_key_value.bias',\n",
       "                tensor([ 0.0351, -0.0403, -0.0908,  ...,  0.0063,  0.0062, -0.0420],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.attention.dense.weight',\n",
       "                tensor([[ 0.0260,  0.0430, -0.0026,  ..., -0.0503,  0.0268,  0.0709],\n",
       "                        [ 0.0437, -0.0535, -0.0114,  ...,  0.0071, -0.0492,  0.0868],\n",
       "                        [ 0.0498, -0.0468,  0.0200,  ..., -0.0235, -0.0244, -0.0258],\n",
       "                        ...,\n",
       "                        [-0.0217, -0.0030,  0.0421,  ..., -0.0661, -0.0329, -0.0379],\n",
       "                        [ 0.0059, -0.0676,  0.0321,  ..., -0.0224,  0.0533, -0.0621],\n",
       "                        [ 0.0855,  0.0111,  0.0174,  ...,  0.0046, -0.0368,  0.0277]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.attention.dense.bias',\n",
       "                tensor([-0.0186, -0.0118, -0.0187,  ..., -0.0526,  0.0474, -0.0264],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.post_attention_layernorm.weight',\n",
       "                tensor([0.9771, 0.9434, 0.9531,  ..., 0.9692, 0.9590, 0.8848],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.post_attention_layernorm.bias',\n",
       "                tensor([-0.0375, -0.0416, -0.0550,  ..., -0.0626,  0.0581,  0.0666],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0053,  0.0178,  0.0406,  ...,  0.0244, -0.0297, -0.0017],\n",
       "                        [-0.0605, -0.0144,  0.0164,  ...,  0.0168, -0.0740,  0.1011],\n",
       "                        [-0.0342,  0.0223, -0.0249,  ..., -0.0035, -0.0529, -0.0117],\n",
       "                        ...,\n",
       "                        [-0.0862,  0.0570,  0.0321,  ...,  0.0632,  0.0213, -0.0156],\n",
       "                        [-0.0061,  0.0818, -0.0119,  ..., -0.0203, -0.0199,  0.0207],\n",
       "                        [-0.0410, -0.0340,  0.1245,  ..., -0.0337, -0.0562, -0.0339]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0067, -0.0430, -0.0167,  ..., -0.0354, -0.0374, -0.0546],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0420, -0.0930,  0.0152,  ..., -0.0941,  0.0035, -0.0168],\n",
       "                        [-0.0129,  0.0649, -0.0378,  ...,  0.0652,  0.0720,  0.0844],\n",
       "                        [-0.0059,  0.0169,  0.0130,  ...,  0.0015,  0.0357, -0.0120],\n",
       "                        ...,\n",
       "                        [ 0.0219,  0.0732, -0.0241,  ...,  0.0534, -0.0297, -0.0393],\n",
       "                        [ 0.0375, -0.0367,  0.0135,  ...,  0.0136,  0.0052, -0.0066],\n",
       "                        [ 0.0178, -0.0418, -0.0028,  ...,  0.0682, -0.0011, -0.0333]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.10.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.1199, -0.0322, -0.0272,  ..., -0.0288,  0.0065,  0.0260],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.input_layernorm.weight',\n",
       "                tensor([0.9438, 1.0068, 0.9448,  ..., 1.0020, 1.0479, 0.8955],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.input_layernorm.bias',\n",
       "                tensor([ 0.0002, -0.0020,  0.0061,  ...,  0.0026, -0.0378, -0.0594],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0544, -0.0416, -0.0002,  ...,  0.0100, -0.0196,  0.0054],\n",
       "                        [ 0.0862, -0.0046, -0.0239,  ...,  0.0100,  0.0260,  0.0570],\n",
       "                        [-0.0085,  0.0715, -0.0063,  ..., -0.0049, -0.0092, -0.0112],\n",
       "                        ...,\n",
       "                        [-0.0258, -0.0439,  0.0320,  ..., -0.0635, -0.0311, -0.0054],\n",
       "                        [-0.0468,  0.0039,  0.0142,  ...,  0.0585, -0.0493, -0.0867],\n",
       "                        [-0.0688, -0.0276,  0.0352,  ...,  0.0594,  0.0365, -0.0969]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.attention.query_key_value.bias',\n",
       "                tensor([ 0.0591,  0.0915,  0.2551,  ..., -0.0048, -0.0153,  0.0040],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.attention.dense.weight',\n",
       "                tensor([[ 0.0685,  0.0308, -0.0513,  ...,  0.0307, -0.0074,  0.0394],\n",
       "                        [-0.0734,  0.0152, -0.0720,  ..., -0.0423, -0.0721,  0.0233],\n",
       "                        [-0.0122,  0.0302,  0.0157,  ..., -0.0112, -0.0398, -0.0394],\n",
       "                        ...,\n",
       "                        [ 0.0778,  0.0209,  0.0327,  ..., -0.0009, -0.0676,  0.0467],\n",
       "                        [-0.0469, -0.0847,  0.0073,  ...,  0.0266, -0.0858, -0.0820],\n",
       "                        [ 0.0732, -0.0137, -0.0220,  ..., -0.0047,  0.0095, -0.0447]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.attention.dense.bias',\n",
       "                tensor([-0.0890,  0.0220, -0.0073,  ..., -0.0622,  0.0471,  0.0070],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.post_attention_layernorm.weight',\n",
       "                tensor([0.9683, 0.9736, 0.9521,  ..., 0.9458, 0.9551, 0.8931],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.post_attention_layernorm.bias',\n",
       "                tensor([-0.0726, -0.0353, -0.0815,  ..., -0.0621,  0.0162,  0.0583],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0797,  0.0365,  0.0722,  ...,  0.0200, -0.0388, -0.0056],\n",
       "                        [ 0.0323,  0.0087,  0.0313,  ..., -0.0053, -0.0022, -0.0444],\n",
       "                        [-0.0084,  0.0180, -0.0661,  ...,  0.0223, -0.0450, -0.0704],\n",
       "                        ...,\n",
       "                        [ 0.1033, -0.0583, -0.0527,  ..., -0.0844,  0.0098, -0.0306],\n",
       "                        [ 0.0631, -0.0410,  0.0704,  ...,  0.0305,  0.0177,  0.0149],\n",
       "                        [ 0.0057, -0.1016, -0.0447,  ..., -0.0056, -0.0305, -0.0112]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0456, -0.0684, -0.0847,  ..., -0.0734, -0.0307, -0.0526],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0533, -0.0302,  0.0219,  ..., -0.0305, -0.0407,  0.0407],\n",
       "                        [-0.0562, -0.0170,  0.0681,  ...,  0.0472, -0.0068, -0.0481],\n",
       "                        [ 0.1246,  0.0505, -0.0802,  ...,  0.0464, -0.0316, -0.0570],\n",
       "                        ...,\n",
       "                        [-0.0184,  0.0235,  0.0224,  ...,  0.0626, -0.0044,  0.0032],\n",
       "                        [-0.0299, -0.0230,  0.0512,  ...,  0.0233,  0.0427, -0.0041],\n",
       "                        [-0.0455,  0.0505, -0.0504,  ...,  0.0040,  0.0255, -0.0051]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.11.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0613, -0.0269, -0.0410,  ..., -0.0391,  0.0250,  0.0125],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.input_layernorm.weight',\n",
       "                tensor([0.9912, 1.0312, 1.0049,  ..., 1.0332, 1.0625, 0.9443],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.input_layernorm.bias',\n",
       "                tensor([-0.0171, -0.0023, -0.0088,  ..., -0.0052, -0.0248, -0.0520],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0471, -0.0666, -0.0111,  ..., -0.0193,  0.1056, -0.0476],\n",
       "                        [ 0.0444, -0.0016,  0.0755,  ...,  0.0665,  0.0378,  0.0609],\n",
       "                        [ 0.0083,  0.0183, -0.0205,  ...,  0.0439,  0.0662, -0.0091],\n",
       "                        ...,\n",
       "                        [-0.0523, -0.0603, -0.1021,  ..., -0.1033,  0.0297,  0.0375],\n",
       "                        [-0.0085,  0.0300, -0.0292,  ...,  0.0018, -0.0117,  0.0258],\n",
       "                        [ 0.0177, -0.0435,  0.0605,  ...,  0.0766,  0.0020,  0.0400]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.attention.query_key_value.bias',\n",
       "                tensor([-0.1180,  0.0649, -0.0700,  ..., -0.0016,  0.0042,  0.0144],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.attention.dense.weight',\n",
       "                tensor([[-0.0917,  0.0687,  0.0077,  ..., -0.0183, -0.0377, -0.0582],\n",
       "                        [ 0.0573,  0.0217,  0.0583,  ..., -0.0026,  0.0365,  0.0229],\n",
       "                        [-0.1073,  0.0393, -0.0075,  ...,  0.1105, -0.0149, -0.0323],\n",
       "                        ...,\n",
       "                        [ 0.0681, -0.0283, -0.0369,  ...,  0.0948, -0.0512, -0.0283],\n",
       "                        [-0.0434, -0.0941,  0.1501,  ..., -0.0698, -0.0265,  0.0104],\n",
       "                        [-0.0397,  0.0046, -0.0044,  ...,  0.0040, -0.0223,  0.0559]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.attention.dense.bias',\n",
       "                tensor([-0.0394,  0.0492, -0.0449,  ..., -0.0236, -0.0504, -0.0137],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.post_attention_layernorm.weight',\n",
       "                tensor([1.0195, 0.9663, 0.9702,  ..., 0.9741, 1.0078, 0.9092],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.post_attention_layernorm.bias',\n",
       "                tensor([-0.0502, -0.0345, -0.0402,  ..., -0.0657,  0.0364, -0.0016],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0361,  0.0026,  0.0112,  ..., -0.0276,  0.0436, -0.0155],\n",
       "                        [-0.0457, -0.0583,  0.0346,  ...,  0.0503, -0.1226,  0.0186],\n",
       "                        [ 0.0334,  0.0154, -0.0438,  ..., -0.0110, -0.0146,  0.0519],\n",
       "                        ...,\n",
       "                        [ 0.0217,  0.0347, -0.0295,  ...,  0.1023, -0.0094,  0.0414],\n",
       "                        [ 0.0276,  0.0724, -0.0274,  ..., -0.0534, -0.0605,  0.0067],\n",
       "                        [ 0.0216,  0.0052, -0.0214,  ..., -0.0349, -0.0120,  0.0055]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0237, -0.0668, -0.0803,  ..., -0.0229, -0.0230, -0.0498],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0424, -0.0756, -0.0364,  ..., -0.0533, -0.0921,  0.0209],\n",
       "                        [ 0.0117, -0.0283,  0.0568,  ..., -0.0352,  0.0084, -0.0455],\n",
       "                        [ 0.0003, -0.0056, -0.0266,  ..., -0.0037,  0.0436,  0.0042],\n",
       "                        ...,\n",
       "                        [ 0.0135,  0.0619,  0.0662,  ..., -0.0898, -0.0031,  0.0267],\n",
       "                        [-0.0039, -0.0838,  0.0035,  ...,  0.0213,  0.0630,  0.0583],\n",
       "                        [-0.0185,  0.0163, -0.0108,  ..., -0.0268, -0.0098, -0.1002]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.12.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0692, -0.0153, -0.0361,  ..., -0.0457,  0.0339,  0.0372],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.input_layernorm.weight',\n",
       "                tensor([1.0205, 1.0244, 1.0400,  ..., 1.0244, 1.1123, 0.9702],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.input_layernorm.bias',\n",
       "                tensor([-0.0247,  0.0006,  0.0019,  ...,  0.0126, -0.0339, -0.0559],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.attention.query_key_value.weight',\n",
       "                tensor([[-0.0566, -0.0728, -0.0953,  ..., -0.0676,  0.0119,  0.0652],\n",
       "                        [ 0.0013, -0.0065, -0.0003,  ..., -0.0862, -0.0460,  0.0110],\n",
       "                        [ 0.1027, -0.0143, -0.0125,  ...,  0.0379, -0.0060, -0.0197],\n",
       "                        ...,\n",
       "                        [ 0.0343, -0.0072,  0.0388,  ..., -0.0906,  0.0820, -0.0353],\n",
       "                        [ 0.0663, -0.0275,  0.0002,  ..., -0.0374, -0.1018,  0.0414],\n",
       "                        [ 0.0246,  0.0512,  0.0012,  ..., -0.0792,  0.1022, -0.0404]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.attention.query_key_value.bias',\n",
       "                tensor([ 0.0743,  0.1541, -0.0747,  ..., -0.0033, -0.0127,  0.0039],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.attention.dense.weight',\n",
       "                tensor([[-0.0598,  0.0114, -0.0454,  ..., -0.0060, -0.0043, -0.0390],\n",
       "                        [ 0.0475, -0.0134,  0.0448,  ...,  0.0568,  0.0698, -0.0115],\n",
       "                        [-0.0122,  0.0265, -0.0238,  ...,  0.0629, -0.0551,  0.0165],\n",
       "                        ...,\n",
       "                        [-0.0140, -0.0250,  0.0131,  ...,  0.0169, -0.0075, -0.0114],\n",
       "                        [-0.0027,  0.0349,  0.0021,  ..., -0.0706,  0.0553, -0.0378],\n",
       "                        [ 0.0062,  0.0334,  0.0258,  ...,  0.0161,  0.0419,  0.0014]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.attention.dense.bias',\n",
       "                tensor([-0.0428,  0.0135, -0.0342,  ..., -0.0420, -0.0377, -0.0180],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.post_attention_layernorm.weight',\n",
       "                tensor([1.0273, 1.0146, 1.0176,  ..., 1.0078, 1.0176, 0.9365],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.post_attention_layernorm.bias',\n",
       "                tensor([-0.0392,  0.0031, -0.0459,  ..., -0.0762,  0.0193,  0.0152],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0147,  0.0019, -0.0081,  ...,  0.0441,  0.0303, -0.0154],\n",
       "                        [-0.0333,  0.0698,  0.0456,  ..., -0.0196,  0.0191, -0.0580],\n",
       "                        [ 0.0896,  0.0368,  0.0322,  ..., -0.0007,  0.0328,  0.0067],\n",
       "                        ...,\n",
       "                        [ 0.0356,  0.0472,  0.0521,  ..., -0.0333,  0.0451, -0.0198],\n",
       "                        [ 0.0155, -0.0239,  0.0800,  ...,  0.0707,  0.0085, -0.0413],\n",
       "                        [ 0.0289,  0.0667, -0.0512,  ...,  0.0836,  0.0279, -0.0145]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0106, -0.0692, -0.0230,  ..., -0.0557, -0.0237, -0.0296],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0085,  0.0042, -0.0954,  ..., -0.0813, -0.0617, -0.0269],\n",
       "                        [ 0.0204, -0.0096, -0.0024,  ...,  0.0327, -0.0201,  0.0156],\n",
       "                        [ 0.0092,  0.0205, -0.0563,  ...,  0.0291, -0.0775,  0.0452],\n",
       "                        ...,\n",
       "                        [-0.0215, -0.0539,  0.0087,  ...,  0.0148, -0.0161, -0.0601],\n",
       "                        [-0.0181,  0.0144, -0.0592,  ..., -0.0121,  0.0026, -0.0687],\n",
       "                        [-0.0195, -0.0770, -0.0012,  ..., -0.0428, -0.0094, -0.0178]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.13.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0934, -0.0586, -0.0348,  ..., -0.0667, -0.0001,  0.0370],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.input_layernorm.weight',\n",
       "                tensor([1.0713, 1.0459, 1.0215,  ..., 1.0059, 1.0830, 1.0186],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.input_layernorm.bias',\n",
       "                tensor([-0.0204,  0.0049, -0.0024,  ...,  0.0161, -0.0370, -0.0492],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0051, -0.0206,  0.0259,  ...,  0.0272, -0.0173, -0.0631],\n",
       "                        [ 0.0173, -0.0513, -0.0315,  ...,  0.0197,  0.0343,  0.0430],\n",
       "                        [-0.0183,  0.0299, -0.0199,  ...,  0.0249,  0.0599, -0.0815],\n",
       "                        ...,\n",
       "                        [ 0.0298, -0.0971,  0.0510,  ...,  0.0074, -0.0762, -0.0745],\n",
       "                        [-0.0361, -0.0265,  0.0592,  ..., -0.0452, -0.0818, -0.0059],\n",
       "                        [ 0.0587,  0.0095,  0.0390,  ..., -0.1057,  0.0132,  0.0511]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.attention.query_key_value.bias',\n",
       "                tensor([ 0.0976,  0.0235, -0.0560,  ..., -0.0231,  0.0052, -0.0207],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.attention.dense.weight',\n",
       "                tensor([[-0.0124,  0.0020,  0.0385,  ...,  0.0169,  0.0004,  0.0168],\n",
       "                        [-0.0955, -0.0211, -0.0482,  ...,  0.0107, -0.0056, -0.0782],\n",
       "                        [-0.0468,  0.0232,  0.0699,  ...,  0.0104, -0.0034, -0.0485],\n",
       "                        ...,\n",
       "                        [ 0.0578, -0.0205,  0.0141,  ...,  0.0108,  0.0031, -0.0267],\n",
       "                        [-0.0120, -0.0746, -0.0233,  ...,  0.0533,  0.0173,  0.0382],\n",
       "                        [-0.0131,  0.0098,  0.0017,  ...,  0.0485,  0.0821, -0.0018]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.attention.dense.bias',\n",
       "                tensor([-0.0878,  0.0490,  0.0015,  ..., -0.0486,  0.0226, -0.0031],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.post_attention_layernorm.weight',\n",
       "                tensor([1.0518, 1.0176, 1.0146,  ..., 1.0156, 1.0039, 0.9644],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.post_attention_layernorm.bias',\n",
       "                tensor([-0.0735, -0.0398, -0.0345,  ..., -0.1042, -0.0102, -0.0307],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-3.7861e-03,  3.7422e-03,  3.7628e-02,  ...,  5.4352e-02,\n",
       "                          3.1021e-02,  5.3644e-05],\n",
       "                        [-3.7659e-02,  4.2053e-02, -4.8737e-02,  ..., -4.5776e-03,\n",
       "                          1.9775e-02,  2.9572e-02],\n",
       "                        [ 2.3041e-02, -2.0309e-02, -6.3843e-02,  ..., -8.8440e-02,\n",
       "                         -2.8057e-03,  3.0655e-02],\n",
       "                        ...,\n",
       "                        [-2.9175e-02, -2.8934e-03,  4.1580e-04,  ...,  3.6926e-02,\n",
       "                          7.7209e-02, -8.0444e-02],\n",
       "                        [ 2.6764e-02, -4.2725e-02, -3.0991e-02,  ..., -2.3087e-02,\n",
       "                         -3.5004e-02,  2.0401e-02],\n",
       "                        [-7.8003e-02,  2.0237e-03,  8.5938e-02,  ..., -3.6125e-03,\n",
       "                         -3.0785e-03,  2.9037e-02]], dtype=torch.float16)),\n",
       "               ('layers.14.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0281, -0.0276, -0.0401,  ..., -0.0145, -0.0654, -0.0029],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0436,  0.0212, -0.0488,  ...,  0.0431, -0.0858,  0.0052],\n",
       "                        [-0.0215, -0.0046, -0.0406,  ...,  0.0152, -0.0984,  0.0309],\n",
       "                        [-0.0102,  0.0157,  0.0267,  ..., -0.0272, -0.0465,  0.0190],\n",
       "                        ...,\n",
       "                        [ 0.0073,  0.0265,  0.0811,  ..., -0.0342,  0.0468,  0.0249],\n",
       "                        [-0.0101, -0.0594, -0.0505,  ..., -0.0668,  0.1100, -0.0116],\n",
       "                        [-0.0324, -0.0290, -0.0473,  ...,  0.0680,  0.0618, -0.0238]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.14.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0755, -0.0157, -0.0346,  ...,  0.0030, -0.0085,  0.0133],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.input_layernorm.weight',\n",
       "                tensor([1.0225, 1.0146, 1.0273,  ..., 1.0166, 1.0654, 1.0107],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.input_layernorm.bias',\n",
       "                tensor([-0.0172,  0.0242, -0.0198,  ...,  0.0084, -0.0355, -0.0608],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0313,  0.0051,  0.0570,  ..., -0.0812,  0.0169, -0.0801],\n",
       "                        [-0.0332, -0.0744, -0.0325,  ...,  0.0399, -0.0576,  0.0600],\n",
       "                        [ 0.0505,  0.0450,  0.0801,  ...,  0.0012, -0.0434, -0.0022],\n",
       "                        ...,\n",
       "                        [-0.0125,  0.0082,  0.0510,  ...,  0.0280, -0.0257, -0.0374],\n",
       "                        [-0.0598,  0.0376,  0.0161,  ..., -0.0632, -0.0328,  0.0360],\n",
       "                        [ 0.0175, -0.0316, -0.0111,  ..., -0.0384,  0.0218,  0.0292]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.attention.query_key_value.bias',\n",
       "                tensor([ 0.0064, -0.0645, -0.1272,  ..., -0.0086,  0.0116,  0.0003],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.attention.dense.weight',\n",
       "                tensor([[ 0.0676,  0.0474,  0.0456,  ...,  0.0117, -0.0132,  0.0107],\n",
       "                        [-0.0316,  0.0003, -0.0794,  ..., -0.0699,  0.0039, -0.0597],\n",
       "                        [ 0.0023,  0.0757, -0.0629,  ...,  0.0550,  0.0241, -0.0121],\n",
       "                        ...,\n",
       "                        [-0.0575, -0.0272, -0.0277,  ...,  0.0817,  0.0319,  0.0527],\n",
       "                        [-0.0646,  0.0280, -0.0524,  ...,  0.0286,  0.0259, -0.0112],\n",
       "                        [ 0.0088,  0.0427,  0.0051,  ...,  0.0287, -0.0238,  0.0630]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.attention.dense.bias',\n",
       "                tensor([-0.0169, -0.0023, -0.0199,  ...,  0.0176, -0.0154,  0.0088],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.post_attention_layernorm.weight',\n",
       "                tensor([1.0977, 0.9844, 1.0615,  ..., 1.0420, 1.0723, 1.0137],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.post_attention_layernorm.bias',\n",
       "                tensor([-0.0507,  0.0016, -0.0292,  ..., -0.1093,  0.0188, -0.0244],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0177, -0.0378,  0.0195,  ...,  0.0281,  0.0030, -0.0292],\n",
       "                        [ 0.0030,  0.0100, -0.0134,  ...,  0.0616, -0.0335, -0.0112],\n",
       "                        [ 0.0235,  0.0269,  0.0137,  ...,  0.0332, -0.0321,  0.0181],\n",
       "                        ...,\n",
       "                        [ 0.0051,  0.0249,  0.0588,  ..., -0.0488,  0.0088,  0.0528],\n",
       "                        [ 0.0015,  0.0259, -0.0328,  ...,  0.0250, -0.0173, -0.0065],\n",
       "                        [-0.0753,  0.0889,  0.0767,  ...,  0.0411,  0.0111, -0.0112]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.mlp.dense_h_to_4h.bias',\n",
       "                tensor([ 0.0085, -0.0427, -0.0452,  ..., -0.0154, -0.0159, -0.0544],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0048, -0.0037, -0.0526,  ...,  0.0335, -0.0186,  0.0059],\n",
       "                        [ 0.0059, -0.0101, -0.0531,  ...,  0.0136, -0.0142,  0.0080],\n",
       "                        [-0.0129, -0.0467, -0.0205,  ...,  0.0327,  0.0296, -0.0831],\n",
       "                        ...,\n",
       "                        [-0.0255,  0.0129,  0.0060,  ...,  0.0481, -0.0083, -0.0209],\n",
       "                        [ 0.0097,  0.0340, -0.0092,  ..., -0.0234, -0.0206,  0.0692],\n",
       "                        [ 0.0038,  0.0480, -0.0502,  ...,  0.0114, -0.0320, -0.1501]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.15.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0412, -0.0275, -0.0269,  ..., -0.0551, -0.0167, -0.0178],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.input_layernorm.weight',\n",
       "                tensor([1.0186, 1.0342, 1.0400,  ..., 0.9961, 1.0723, 1.0068],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.input_layernorm.bias',\n",
       "                tensor([-0.0150,  0.0294, -0.0348,  ...,  0.0250, -0.0442, -0.0841],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0250, -0.0215,  0.0378,  ..., -0.0206,  0.0195, -0.0516],\n",
       "                        [-0.0114, -0.0718, -0.0456,  ..., -0.0179, -0.0755,  0.0063],\n",
       "                        [ 0.0173, -0.0314,  0.0322,  ..., -0.0546,  0.1193, -0.0493],\n",
       "                        ...,\n",
       "                        [ 0.0043,  0.0507,  0.0252,  ..., -0.1134,  0.0161,  0.0542],\n",
       "                        [-0.1343, -0.0153,  0.0341,  ...,  0.0290, -0.1048,  0.0461],\n",
       "                        [-0.0196, -0.0551, -0.0086,  ...,  0.0475,  0.0273, -0.0693]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.attention.query_key_value.bias',\n",
       "                tensor([-0.0238,  0.0233, -0.0238,  ..., -0.0176,  0.0287, -0.0122],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.attention.dense.weight',\n",
       "                tensor([[ 0.0149,  0.0009,  0.0290,  ..., -0.1054,  0.0399, -0.1172],\n",
       "                        [-0.0397, -0.0522,  0.0587,  ...,  0.0061,  0.0606,  0.0086],\n",
       "                        [-0.0010, -0.0363, -0.0711,  ...,  0.0335, -0.0251, -0.0052],\n",
       "                        ...,\n",
       "                        [ 0.0490,  0.0499,  0.0100,  ..., -0.0104, -0.0205,  0.0113],\n",
       "                        [-0.0478, -0.0612, -0.0393,  ..., -0.0169, -0.0071, -0.0895],\n",
       "                        [-0.0241, -0.0422, -0.0280,  ...,  0.0063,  0.0608,  0.0278]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.attention.dense.bias',\n",
       "                tensor([-0.0842, -0.0022, -0.0246,  ..., -0.0371, -0.0391, -0.0749],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.post_attention_layernorm.weight',\n",
       "                tensor([1.1377, 1.0127, 1.0635,  ..., 1.0811, 1.0693, 1.0244],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.post_attention_layernorm.bias',\n",
       "                tensor([-0.0663, -0.0069, -0.0355,  ..., -0.0956, -0.0014, -0.0524],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 6.0730e-03,  4.4128e-02, -9.3155e-03,  ..., -5.7861e-02,\n",
       "                          3.1525e-02,  6.3248e-03],\n",
       "                        [-3.5583e-02,  4.0344e-02, -2.0157e-02,  ..., -7.3731e-05,\n",
       "                         -3.7527e-04, -6.3171e-02],\n",
       "                        [ 5.0171e-02,  6.2408e-02, -3.8643e-03,  ..., -1.0513e-02,\n",
       "                         -4.2114e-02, -2.1667e-02],\n",
       "                        ...,\n",
       "                        [-6.7825e-03,  1.0742e-02, -1.4877e-02,  ..., -2.6169e-02,\n",
       "                         -9.1492e-02,  3.7598e-02],\n",
       "                        [-4.0710e-02, -3.2074e-02, -4.3716e-03,  ..., -4.1229e-02,\n",
       "                          4.6112e-02, -6.2447e-03],\n",
       "                        [-3.4607e-02,  2.9022e-02, -1.4854e-02,  ..., -6.2561e-02,\n",
       "                          3.5797e-02, -5.7373e-03]], dtype=torch.float16)),\n",
       "               ('layers.16.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0642, -0.0116, -0.0364,  ..., -0.0633, -0.0136, -0.0363],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0288,  0.0102, -0.0770,  ..., -0.0124,  0.0424,  0.0120],\n",
       "                        [-0.0381, -0.0281, -0.0442,  ..., -0.0048,  0.0438, -0.0340],\n",
       "                        [ 0.0635,  0.0278,  0.0243,  ...,  0.0042,  0.0340, -0.0432],\n",
       "                        ...,\n",
       "                        [ 0.0912,  0.0038,  0.0045,  ..., -0.0523, -0.0145,  0.0048],\n",
       "                        [-0.0581, -0.0128,  0.0339,  ..., -0.0101, -0.0529,  0.0069],\n",
       "                        [ 0.0224,  0.0748, -0.0158,  ...,  0.0164, -0.0290,  0.0118]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.16.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0674, -0.0421, -0.0408,  ..., -0.0258,  0.0052, -0.0191],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.input_layernorm.weight',\n",
       "                tensor([1.0615, 1.0156, 1.0322,  ..., 0.9897, 1.0430, 1.0430],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.input_layernorm.bias',\n",
       "                tensor([-0.0224, -0.0009, -0.0264,  ...,  0.0408, -0.0495, -0.1024],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0569, -0.0064, -0.0089,  ...,  0.0462,  0.0045, -0.0173],\n",
       "                        [-0.0164,  0.0258, -0.0011,  ...,  0.0361,  0.0252, -0.0547],\n",
       "                        [ 0.0145, -0.0508, -0.0808,  ..., -0.0619,  0.0448,  0.0037],\n",
       "                        ...,\n",
       "                        [ 0.0030, -0.0162, -0.0381,  ...,  0.0075, -0.0011, -0.0076],\n",
       "                        [ 0.0443, -0.0020, -0.0389,  ...,  0.0145,  0.0175,  0.0616],\n",
       "                        [-0.0275,  0.0364, -0.0818,  ...,  0.0166, -0.0390,  0.0432]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.attention.query_key_value.bias',\n",
       "                tensor([-0.1698,  0.0944,  0.1986,  ...,  0.0102,  0.0323, -0.0179],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.attention.dense.weight',\n",
       "                tensor([[-0.0445,  0.0311,  0.0372,  ..., -0.0704,  0.0842,  0.0036],\n",
       "                        [-0.0090,  0.0472, -0.0644,  ...,  0.0062,  0.0120,  0.0014],\n",
       "                        [-0.0259,  0.0961, -0.0237,  ...,  0.0396, -0.0254, -0.0262],\n",
       "                        ...,\n",
       "                        [ 0.0019,  0.0278, -0.0865,  ...,  0.0355, -0.0164,  0.0739],\n",
       "                        [ 0.0043,  0.0089, -0.0327,  ..., -0.0461,  0.0019,  0.0332],\n",
       "                        [ 0.0214, -0.0074, -0.0146,  ..., -0.0290,  0.0359,  0.0352]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.attention.dense.bias',\n",
       "                tensor([-0.0781,  0.0260, -0.0640,  ..., -0.1004, -0.0311, -0.0303],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.post_attention_layernorm.weight',\n",
       "                tensor([1.1514, 1.0303, 1.0811,  ..., 1.1035, 1.0664, 1.0752],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.post_attention_layernorm.bias',\n",
       "                tensor([-0.0895, -0.0154, -0.0070,  ..., -0.0878, -0.0082, -0.0636],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0105,  0.0714,  0.0290,  ..., -0.0200, -0.0404, -0.0546],\n",
       "                        [ 0.0181, -0.0951,  0.0279,  ...,  0.0009,  0.0450,  0.0102],\n",
       "                        [ 0.0627,  0.0575,  0.0883,  ...,  0.0391,  0.0291, -0.0619],\n",
       "                        ...,\n",
       "                        [-0.0549, -0.0558,  0.0870,  ...,  0.0303,  0.0143,  0.0193],\n",
       "                        [-0.0277,  0.0851,  0.0755,  ..., -0.0154,  0.0581,  0.0430],\n",
       "                        [-0.0401,  0.0198, -0.0871,  ..., -0.0210, -0.0365,  0.0125]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.1192, -0.0745, -0.0433,  ..., -0.0315, -0.0507, -0.0255],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.1172, -0.0486, -0.0456,  ..., -0.0596, -0.0026, -0.0080],\n",
       "                        [ 0.0710, -0.0920, -0.0377,  ...,  0.0101, -0.0259,  0.0044],\n",
       "                        [-0.0387, -0.0797, -0.0677,  ...,  0.0155,  0.0400,  0.0041],\n",
       "                        ...,\n",
       "                        [-0.0375,  0.0742, -0.0542,  ..., -0.0725, -0.0314,  0.0129],\n",
       "                        [-0.0119, -0.0315, -0.0128,  ...,  0.0593, -0.0136,  0.0202],\n",
       "                        [-0.0688,  0.0183,  0.0349,  ..., -0.0068,  0.0098, -0.0762]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.17.mlp.dense_4h_to_h.bias',\n",
       "                tensor([-0.0038, -0.0127,  0.0141,  ..., -0.0247,  0.0131, -0.0244],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.input_layernorm.weight',\n",
       "                tensor([1.0947, 1.0088, 1.0361,  ..., 1.0020, 1.0713, 1.0879],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.input_layernorm.bias',\n",
       "                tensor([-0.0182,  0.0065, -0.0371,  ...,  0.0399, -0.0373, -0.0998],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0352, -0.0478,  0.0561,  ...,  0.0229, -0.0019, -0.0011],\n",
       "                        [ 0.0260, -0.0165,  0.0361,  ..., -0.0360, -0.0264, -0.0797],\n",
       "                        [-0.0124,  0.0031,  0.0337,  ...,  0.0120, -0.0153,  0.0434],\n",
       "                        ...,\n",
       "                        [-0.0044, -0.0015, -0.0203,  ..., -0.0924,  0.0231, -0.1011],\n",
       "                        [-0.0036,  0.0291, -0.0548,  ..., -0.0275, -0.1152, -0.0059],\n",
       "                        [ 0.0721,  0.0135,  0.1613,  ..., -0.0052,  0.0589, -0.0396]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.attention.query_key_value.bias',\n",
       "                tensor([ 0.0770,  0.0536, -0.0306,  ..., -0.0007, -0.0146,  0.0140],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.attention.dense.weight',\n",
       "                tensor([[-0.0228,  0.0073, -0.0253,  ...,  0.0224, -0.0306,  0.0527],\n",
       "                        [-0.0856, -0.0271, -0.0013,  ..., -0.0070,  0.0124, -0.0136],\n",
       "                        [ 0.0076, -0.1218,  0.0234,  ...,  0.0509, -0.0355,  0.1074],\n",
       "                        ...,\n",
       "                        [-0.0152,  0.0228,  0.0178,  ...,  0.0164, -0.0291, -0.0239],\n",
       "                        [-0.0140,  0.0320,  0.0425,  ...,  0.0371, -0.0634,  0.0577],\n",
       "                        [-0.1295, -0.0308, -0.0540,  ..., -0.0616, -0.0393,  0.0755]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.attention.dense.bias',\n",
       "                tensor([-0.0610,  0.0732, -0.0206,  ..., -0.0680,  0.0081,  0.0320],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.post_attention_layernorm.weight',\n",
       "                tensor([1.1729, 1.0244, 1.1016,  ..., 1.1250, 1.1260, 1.0889],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.post_attention_layernorm.bias',\n",
       "                tensor([-0.1586, -0.0110, -0.0166,  ..., -0.0433, -0.0228, -0.0476],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0041, -0.0179,  0.0376,  ...,  0.0334, -0.0399,  0.0080],\n",
       "                        [ 0.0918, -0.0583, -0.0150,  ..., -0.0522, -0.0224, -0.0018],\n",
       "                        [ 0.0307,  0.0347,  0.0579,  ...,  0.0203, -0.0006,  0.0544],\n",
       "                        ...,\n",
       "                        [ 0.0428,  0.0682,  0.0007,  ...,  0.0710, -0.0456,  0.0129],\n",
       "                        [-0.0836, -0.0014,  0.0048,  ..., -0.0446, -0.0186,  0.0638],\n",
       "                        [ 0.0213,  0.0233,  0.0050,  ...,  0.0209,  0.0272, -0.0075]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0591, -0.0138, -0.0337,  ..., -0.0825, -0.0349, -0.0253],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0494, -0.0625, -0.0414,  ..., -0.1017,  0.0467, -0.0444],\n",
       "                        [-0.0549,  0.0461, -0.0097,  ...,  0.0560,  0.0803, -0.0267],\n",
       "                        [-0.0447, -0.0228,  0.0121,  ...,  0.0423,  0.0607,  0.0206],\n",
       "                        ...,\n",
       "                        [ 0.0171,  0.0338,  0.0247,  ...,  0.0980,  0.0453, -0.0483],\n",
       "                        [-0.0335,  0.0542,  0.0671,  ...,  0.0585,  0.0693, -0.0586],\n",
       "                        [ 0.0030, -0.0212, -0.0524,  ...,  0.0058,  0.0163, -0.0582]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.18.mlp.dense_4h_to_h.bias',\n",
       "                tensor([ 0.0343, -0.0679,  0.0117,  ..., -0.0518,  0.0174, -0.0071],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.input_layernorm.weight',\n",
       "                tensor([1.0361, 0.9854, 1.0088,  ..., 0.9849, 1.0303, 1.0186],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.input_layernorm.bias',\n",
       "                tensor([-0.0556, -0.0008, -0.0503,  ...,  0.0469, -0.0481, -0.0834],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.attention.query_key_value.weight',\n",
       "                tensor([[-0.0128, -0.0154,  0.0091,  ...,  0.0815, -0.0312, -0.0242],\n",
       "                        [-0.0791,  0.0147, -0.0120,  ..., -0.0302, -0.0311,  0.0144],\n",
       "                        [ 0.0522, -0.0254, -0.0649,  ..., -0.0123, -0.0920, -0.0441],\n",
       "                        ...,\n",
       "                        [-0.1215,  0.0280, -0.0459,  ...,  0.0230,  0.0450,  0.0363],\n",
       "                        [-0.0190,  0.0874, -0.0537,  ...,  0.0289, -0.0881,  0.0728],\n",
       "                        [ 0.0017,  0.0233,  0.0085,  ..., -0.0462, -0.0423, -0.0674]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.attention.query_key_value.bias',\n",
       "                tensor([-0.0088,  0.1082, -0.0336,  ...,  0.0104,  0.0027, -0.0306],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.attention.dense.weight',\n",
       "                tensor([[-0.0437, -0.0431, -0.0051,  ..., -0.0522, -0.0129,  0.0136],\n",
       "                        [-0.0540,  0.0693, -0.0166,  ..., -0.0049,  0.0367,  0.0690],\n",
       "                        [ 0.0134,  0.0036,  0.0301,  ..., -0.0732,  0.0137,  0.0404],\n",
       "                        ...,\n",
       "                        [ 0.0668, -0.0335,  0.0255,  ...,  0.0281,  0.0222,  0.0474],\n",
       "                        [-0.0203, -0.0854, -0.0145,  ..., -0.0262, -0.0591,  0.0167],\n",
       "                        [ 0.0197, -0.0374, -0.0390,  ..., -0.0349,  0.0372, -0.0971]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.attention.dense.bias',\n",
       "                tensor([-0.0202,  0.0052,  0.0576,  ..., -0.0405, -0.0211,  0.0033],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.post_attention_layernorm.weight',\n",
       "                tensor([1.1855, 1.0400, 1.0957,  ..., 1.1475, 1.1816, 1.1387],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.post_attention_layernorm.bias',\n",
       "                tensor([-0.1345,  0.0180, -0.0280,  ..., -0.0555, -0.0021, -0.0164],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0389, -0.0126, -0.0775,  ...,  0.0208, -0.0193, -0.0214],\n",
       "                        [-0.0227, -0.0652, -0.0445,  ...,  0.0119, -0.0427,  0.0108],\n",
       "                        [-0.0757, -0.0611, -0.0482,  ...,  0.0540, -0.0125, -0.0652],\n",
       "                        ...,\n",
       "                        [ 0.0482, -0.0488, -0.0699,  ..., -0.0342,  0.0644, -0.0426],\n",
       "                        [ 0.0274,  0.0108, -0.0765,  ..., -0.0106,  0.0352,  0.0198],\n",
       "                        [-0.0285,  0.0151,  0.0731,  ...,  0.0279, -0.0366,  0.0595]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0314, -0.0213, -0.0149,  ..., -0.0235, -0.0295, -0.0479],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0296,  0.0174,  0.0649,  ...,  0.0056,  0.0299, -0.0945],\n",
       "                        [ 0.0301,  0.0792,  0.0740,  ...,  0.0661, -0.0068,  0.0098],\n",
       "                        [ 0.0636,  0.0073, -0.0171,  ...,  0.0423,  0.0844,  0.0701],\n",
       "                        ...,\n",
       "                        [-0.0096, -0.0208, -0.0906,  ...,  0.0882, -0.0593,  0.0009],\n",
       "                        [ 0.0204, -0.0064,  0.0490,  ..., -0.0240,  0.0022, -0.0430],\n",
       "                        [-0.0671, -0.0701, -0.0275,  ..., -0.0058,  0.0001, -0.0231]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.19.mlp.dense_4h_to_h.bias',\n",
       "                tensor([ 0.0732, -0.0739,  0.0179,  ..., -0.0396,  0.0268, -0.0450],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.input_layernorm.weight',\n",
       "                tensor([1.0186, 1.0234, 1.0068,  ..., 0.9956, 1.0137, 1.0654],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.input_layernorm.bias',\n",
       "                tensor([-0.0731,  0.0060, -0.0681,  ...,  0.0529, -0.0368, -0.0620],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.attention.query_key_value.weight',\n",
       "                tensor([[-0.0241,  0.0059, -0.0515,  ..., -0.0556,  0.0355,  0.0310],\n",
       "                        [-0.0033,  0.0703, -0.0616,  ...,  0.0014, -0.0083, -0.0058],\n",
       "                        [ 0.0685,  0.0361,  0.0384,  ...,  0.0062,  0.0095,  0.0485],\n",
       "                        ...,\n",
       "                        [ 0.0771, -0.0260, -0.0060,  ..., -0.0321, -0.0098, -0.0124],\n",
       "                        [ 0.0114,  0.0441,  0.0292,  ..., -0.0209, -0.0231,  0.1324],\n",
       "                        [-0.0485,  0.0307,  0.0113,  ..., -0.0594,  0.0225,  0.0459]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.attention.query_key_value.bias',\n",
       "                tensor([ 0.1937,  0.0557, -0.0164,  ..., -0.0008,  0.0086, -0.0094],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.attention.dense.weight',\n",
       "                tensor([[-0.0185,  0.0332,  0.0182,  ..., -0.0186, -0.0586, -0.0046],\n",
       "                        [ 0.0481, -0.0703, -0.0727,  ..., -0.0837, -0.0158,  0.0711],\n",
       "                        [-0.0413,  0.0051,  0.0563,  ..., -0.0802,  0.0229,  0.0470],\n",
       "                        ...,\n",
       "                        [-0.0381, -0.0657, -0.0384,  ...,  0.0158,  0.0772, -0.0332],\n",
       "                        [ 0.0293, -0.0816,  0.0090,  ...,  0.0131, -0.0158,  0.0114],\n",
       "                        [ 0.0722, -0.0512,  0.0101,  ..., -0.0103,  0.0807,  0.1249]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.attention.dense.bias',\n",
       "                tensor([-0.0551,  0.0800,  0.0058,  ..., -0.1066, -0.0621,  0.0108],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.post_attention_layernorm.weight',\n",
       "                tensor([1.1895, 1.0508, 1.1279,  ..., 1.1729, 1.1982, 1.1396],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.post_attention_layernorm.bias',\n",
       "                tensor([-0.1487,  0.0461, -0.0009,  ..., -0.0551, -0.0228, -0.0177],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[ 0.0200, -0.0306,  0.0249,  ...,  0.0102, -0.0473,  0.0144],\n",
       "                        [ 0.0317,  0.0632, -0.0724,  ..., -0.0130,  0.0533,  0.0044],\n",
       "                        [ 0.0062, -0.0442,  0.0346,  ..., -0.0055, -0.0593, -0.0619],\n",
       "                        ...,\n",
       "                        [ 0.0344, -0.0576, -0.0491,  ...,  0.0391,  0.0488, -0.0745],\n",
       "                        [-0.0199, -0.0294,  0.0636,  ...,  0.0144,  0.0682, -0.0545],\n",
       "                        [-0.0411, -0.0006, -0.0224,  ..., -0.0107,  0.0664, -0.0155]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0126, -0.0255, -0.0128,  ..., -0.0253, -0.0219, -0.0237],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0880, -0.0828, -0.0291,  ...,  0.0558,  0.0961, -0.0231],\n",
       "                        [-0.0305,  0.0712, -0.0064,  ...,  0.0217,  0.0423,  0.0067],\n",
       "                        [-0.0840, -0.0048, -0.1270,  ..., -0.0336, -0.0209,  0.0544],\n",
       "                        ...,\n",
       "                        [-0.0149, -0.0124, -0.0337,  ..., -0.0783,  0.0664, -0.0551],\n",
       "                        [ 0.0239,  0.0140,  0.0335,  ..., -0.0906,  0.0547, -0.0426],\n",
       "                        [-0.0814,  0.0396,  0.0590,  ...,  0.0406,  0.0228,  0.0219]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.20.mlp.dense_4h_to_h.bias',\n",
       "                tensor([ 0.0346, -0.0269,  0.0037,  ..., -0.0156,  0.0188, -0.0263],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.input_layernorm.weight',\n",
       "                tensor([0.9478, 0.9951, 1.0029,  ..., 0.9521, 0.9668, 1.0020],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.input_layernorm.bias',\n",
       "                tensor([-0.0621,  0.0297, -0.0615,  ...,  0.0823, -0.0381, -0.0666],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0435,  0.0123, -0.0406,  ...,  0.0537, -0.0044,  0.0108],\n",
       "                        [ 0.0273, -0.0703, -0.0122,  ...,  0.0096,  0.0408, -0.0601],\n",
       "                        [ 0.0790, -0.0185,  0.0300,  ..., -0.0366,  0.0088,  0.0185],\n",
       "                        ...,\n",
       "                        [-0.0890, -0.0087, -0.0667,  ...,  0.0110, -0.0165,  0.0650],\n",
       "                        [ 0.0229, -0.0656, -0.0777,  ..., -0.1316,  0.0459, -0.0476],\n",
       "                        [ 0.0458, -0.0142,  0.0547,  ..., -0.0428,  0.0858,  0.0086]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.attention.query_key_value.bias',\n",
       "                tensor([ 0.0332,  0.0391,  0.0632,  ..., -0.0129, -0.0088, -0.0218],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.attention.dense.weight',\n",
       "                tensor([[-0.0273,  0.0346, -0.0767,  ..., -0.0380,  0.0231, -0.0072],\n",
       "                        [-0.0379, -0.0478, -0.0152,  ...,  0.0409, -0.0236,  0.0135],\n",
       "                        [ 0.1462, -0.0326, -0.0182,  ..., -0.0703,  0.0215,  0.0137],\n",
       "                        ...,\n",
       "                        [-0.0323,  0.0363,  0.0640,  ..., -0.0060, -0.0455, -0.0356],\n",
       "                        [ 0.0068, -0.0199, -0.0624,  ...,  0.0019,  0.0309,  0.0256],\n",
       "                        [-0.0289, -0.0091,  0.0458,  ..., -0.0064, -0.1350,  0.0869]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.attention.dense.bias',\n",
       "                tensor([-0.0188,  0.0166,  0.0410,  ..., -0.0308,  0.0392, -0.0030],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.post_attention_layernorm.weight',\n",
       "                tensor([1.1807, 1.0996, 1.1445,  ..., 1.1582, 1.1855, 1.1436],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.post_attention_layernorm.bias',\n",
       "                tensor([-0.0871,  0.0556,  0.0261,  ..., -0.0293, -0.0305, -0.0431],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0428,  0.0147,  0.0654,  ...,  0.0108, -0.0042,  0.0192],\n",
       "                        [-0.0365,  0.0153, -0.0196,  ...,  0.0150, -0.0363, -0.0643],\n",
       "                        [ 0.0505, -0.0181,  0.0037,  ..., -0.0144,  0.0354, -0.0069],\n",
       "                        ...,\n",
       "                        [ 0.0246,  0.0150, -0.0631,  ...,  0.0479, -0.0057, -0.0200],\n",
       "                        [-0.0243,  0.0625,  0.0442,  ..., -0.0318,  0.0787, -0.1027],\n",
       "                        [ 0.0102,  0.0084,  0.0952,  ...,  0.0599,  0.0286, -0.0307]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0400, -0.0674, -0.0410,  ..., -0.0233, -0.0186, -0.0518],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0100,  0.0671, -0.0762,  ..., -0.0194, -0.0042,  0.0008],\n",
       "                        [-0.0695,  0.0170, -0.0096,  ..., -0.0169,  0.0021, -0.0481],\n",
       "                        [ 0.0391,  0.0482, -0.1050,  ...,  0.0568, -0.0021, -0.0051],\n",
       "                        ...,\n",
       "                        [ 0.0368,  0.0264, -0.0749,  ..., -0.0249, -0.0501, -0.0519],\n",
       "                        [ 0.0289,  0.0966, -0.0928,  ...,  0.0169, -0.0817, -0.0151],\n",
       "                        [-0.0016,  0.0551,  0.0211,  ..., -0.0461, -0.0677, -0.0648]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.21.mlp.dense_4h_to_h.bias',\n",
       "                tensor([ 0.0550, -0.0344, -0.0588,  ...,  0.0290,  0.0219,  0.0191],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.input_layernorm.weight',\n",
       "                tensor([0.9473, 0.9961, 1.0176,  ..., 0.9473, 0.9751, 0.9702],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.input_layernorm.bias',\n",
       "                tensor([-0.0680,  0.0267, -0.0450,  ...,  0.1122, -0.0683, -0.0941],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.attention.query_key_value.weight',\n",
       "                tensor([[ 0.0192, -0.0054,  0.0169,  ..., -0.0052,  0.0151, -0.0529],\n",
       "                        [-0.0632,  0.0200, -0.0257,  ..., -0.0123,  0.0439, -0.0551],\n",
       "                        [ 0.0132,  0.0250,  0.1449,  ..., -0.0363, -0.0345, -0.0159],\n",
       "                        ...,\n",
       "                        [-0.0255, -0.0120, -0.0607,  ..., -0.0016,  0.0640, -0.0186],\n",
       "                        [ 0.0558,  0.0275, -0.0239,  ..., -0.0346, -0.0393, -0.0387],\n",
       "                        [-0.1108,  0.0757,  0.0337,  ..., -0.0135,  0.0651, -0.0347]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.attention.query_key_value.bias',\n",
       "                tensor([-0.0538, -0.0047, -0.0877,  ...,  0.0200,  0.0113,  0.0174],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.attention.dense.weight',\n",
       "                tensor([[ 0.0479, -0.0487,  0.0496,  ...,  0.0483,  0.0341,  0.0427],\n",
       "                        [ 0.0333,  0.0110, -0.0493,  ...,  0.0182, -0.0595, -0.0124],\n",
       "                        [-0.0496, -0.0246, -0.0429,  ...,  0.0200, -0.0286,  0.0166],\n",
       "                        ...,\n",
       "                        [-0.0839, -0.0504,  0.0016,  ...,  0.0717, -0.0375, -0.0501],\n",
       "                        [-0.0307, -0.0486, -0.0125,  ..., -0.0050, -0.0208,  0.0667],\n",
       "                        [-0.0683, -0.0489, -0.0072,  ..., -0.0110,  0.0064,  0.0721]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.attention.dense.bias',\n",
       "                tensor([ 0.0902, -0.0601, -0.0295,  ..., -0.0741, -0.0113,  0.0140],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.post_attention_layernorm.weight',\n",
       "                tensor([1.1582, 1.0605, 1.1270,  ..., 1.1523, 1.1895, 1.1650],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.post_attention_layernorm.bias',\n",
       "                tensor([-0.0895,  0.0557,  0.0380,  ..., -0.0518,  0.0240, -0.0278],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0107, -0.0339, -0.0385,  ..., -0.0779,  0.0267,  0.0009],\n",
       "                        [-0.0416,  0.0267, -0.0514,  ...,  0.0253,  0.0493, -0.0026],\n",
       "                        [-0.0054, -0.0469, -0.0181,  ..., -0.0534,  0.0602, -0.0761],\n",
       "                        ...,\n",
       "                        [ 0.1099,  0.0163,  0.0362,  ...,  0.0223, -0.0022, -0.0187],\n",
       "                        [ 0.0750,  0.0928, -0.0383,  ..., -0.0186, -0.0047,  0.0331],\n",
       "                        [ 0.0038,  0.0009,  0.0135,  ...,  0.0922,  0.0299,  0.0034]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0351, -0.0308, -0.0357,  ..., -0.1113, -0.0525, -0.0325],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[ 0.0172, -0.0030,  0.0272,  ...,  0.0900, -0.0573,  0.1234],\n",
       "                        [-0.0606, -0.0017, -0.0082,  ...,  0.0012, -0.0120,  0.0641],\n",
       "                        [ 0.0215, -0.0329, -0.0790,  ...,  0.1114,  0.0370,  0.0508],\n",
       "                        ...,\n",
       "                        [ 0.0039, -0.0645, -0.0305,  ..., -0.0508,  0.0999, -0.1036],\n",
       "                        [-0.0106,  0.0628, -0.0146,  ..., -0.0580,  0.0453,  0.0244],\n",
       "                        [ 0.0265,  0.0863,  0.0031,  ...,  0.2002, -0.0335,  0.0164]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.22.mlp.dense_4h_to_h.bias',\n",
       "                tensor([ 0.0269, -0.0267,  0.0428,  ...,  0.0419, -0.0491,  0.0174],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.input_layernorm.weight',\n",
       "                tensor([0.9907, 1.0547, 1.0127,  ..., 0.9907, 1.0156, 1.0391],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.input_layernorm.bias',\n",
       "                tensor([-0.0465,  0.0286, -0.0314,  ...,  0.0886, -0.0579, -0.1117],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.attention.query_key_value.weight',\n",
       "                tensor([[-0.0431, -0.0210,  0.0243,  ..., -0.0328, -0.0324, -0.0536],\n",
       "                        [ 0.0178,  0.0667,  0.0312,  ...,  0.0347, -0.0219, -0.0025],\n",
       "                        [ 0.0337,  0.0299,  0.0134,  ..., -0.0820,  0.0748,  0.0096],\n",
       "                        ...,\n",
       "                        [-0.0386, -0.0096,  0.0438,  ...,  0.0695,  0.0249,  0.0452],\n",
       "                        [-0.0037, -0.0575,  0.0206,  ...,  0.0336, -0.0958,  0.0006],\n",
       "                        [-0.0013,  0.0120, -0.0195,  ...,  0.0328,  0.0061, -0.0063]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.attention.query_key_value.bias',\n",
       "                tensor([ 0.0648, -0.0728,  0.0287,  ..., -0.0140, -0.0045,  0.0109],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.attention.dense.weight',\n",
       "                tensor([[ 0.0261,  0.0238,  0.0030,  ..., -0.0203,  0.0212,  0.0220],\n",
       "                        [ 0.0296,  0.0067,  0.0552,  ..., -0.0497, -0.0410, -0.0243],\n",
       "                        [ 0.0351,  0.0158, -0.0492,  ...,  0.0391,  0.0673,  0.0083],\n",
       "                        ...,\n",
       "                        [-0.0576, -0.1102, -0.0499,  ...,  0.0328,  0.0204,  0.0394],\n",
       "                        [-0.0326,  0.0010, -0.0461,  ...,  0.0187, -0.0317,  0.0144],\n",
       "                        [-0.0231, -0.0966, -0.0424,  ...,  0.0389,  0.0215, -0.0319]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.attention.dense.bias',\n",
       "                tensor([-0.0418,  0.1368,  0.0047,  ...,  0.0194, -0.0130,  0.0207],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.post_attention_layernorm.weight',\n",
       "                tensor([1.0645, 1.0303, 1.0459,  ..., 1.0674, 1.1035, 1.0371],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.post_attention_layernorm.bias',\n",
       "                tensor([-0.0243,  0.0511, -0.0053,  ..., -0.0599,  0.0246, -0.0594],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.mlp.dense_h_to_4h.weight',\n",
       "                tensor([[-0.0016,  0.0114, -0.0005,  ..., -0.0382, -0.0476, -0.0091],\n",
       "                        [-0.0215,  0.0596,  0.0332,  ...,  0.0599,  0.0287, -0.0231],\n",
       "                        [ 0.0450,  0.0209,  0.0468,  ...,  0.0852,  0.0125, -0.0653],\n",
       "                        ...,\n",
       "                        [-0.0732,  0.0718,  0.0896,  ...,  0.0180, -0.0150, -0.0002],\n",
       "                        [-0.0309, -0.0430, -0.0367,  ...,  0.1002,  0.0557, -0.1176],\n",
       "                        [ 0.0672, -0.0891,  0.0050,  ...,  0.0533,  0.0035,  0.0179]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.mlp.dense_h_to_4h.bias',\n",
       "                tensor([-0.0146, -0.0294,  0.0112,  ..., -0.0297, -0.0035, -0.0245],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.mlp.dense_4h_to_h.weight',\n",
       "                tensor([[-0.0062, -0.0397,  0.0627,  ...,  0.0932,  0.0083,  0.0388],\n",
       "                        [-0.0120,  0.0252, -0.0145,  ...,  0.0028,  0.0883,  0.0489],\n",
       "                        [ 0.0410, -0.0188,  0.0854,  ...,  0.0326,  0.0658, -0.0165],\n",
       "                        ...,\n",
       "                        [ 0.0871, -0.0901, -0.0159,  ...,  0.0454, -0.0046,  0.0039],\n",
       "                        [-0.0190, -0.0205,  0.0170,  ...,  0.0372, -0.1092,  0.0758],\n",
       "                        [-0.0237, -0.0473,  0.0341,  ..., -0.0650, -0.0033, -0.0521]],\n",
       "                       dtype=torch.float16)),\n",
       "               ('layers.23.mlp.dense_4h_to_h.bias',\n",
       "                tensor([ 0.0595,  0.1210,  0.0390,  ...,  0.0196, -0.0449,  0.0138],\n",
       "                       dtype=torch.float16)),\n",
       "               ('final_layernorm.weight',\n",
       "                tensor([0.9106, 0.9858, 0.9575,  ..., 0.9482, 0.9136, 0.9521],\n",
       "                       dtype=torch.float16)),\n",
       "               ('final_layernorm.bias',\n",
       "                tensor([-0.0126, -0.1917, -0.0938,  ..., -0.1656,  0.0500,  0.0865],\n",
       "                       dtype=torch.float16))])},\n",
       " 'qa_head': OrderedDict([('qa_dense.weight',\n",
       "               tensor([[ 0.0002, -0.0111,  0.0256,  ..., -0.0031, -0.0234,  0.0033],\n",
       "                       [ 0.0435,  0.0271, -0.0282,  ...,  0.0220, -0.0216, -0.0184]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('qa_dense.bias', tensor([0., 0.], dtype=torch.float16))])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ = \"/data/models/BioMegatron345mCased/MegatronBERT.pt\"\n",
    "state_dict = torch.load(file_, map_location=torch.device('cpu'))\n",
    "state_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc62813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.megatron_bert.convert_megatron_bert_checkpoint import convert_megatron_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "055c2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_megatron_checkpoint(args, input_state_dict):\n",
    "    # The converted output model.\n",
    "    output_state_dict = {}\n",
    "\n",
    "    # The model.\n",
    "    model = input_state_dict[\"model\"]\n",
    "    # The language model.\n",
    "    lm = model[\"language_model\"]\n",
    "    # The embeddings.\n",
    "    embeddings = lm[\"embedding\"]\n",
    "\n",
    "    # The word embeddings.\n",
    "    word_embeddings = embeddings[\"word_embeddings\"][\"weight\"]\n",
    "    # Store the word embeddings.\n",
    "    output_state_dict[\"bert.embeddings.word_embeddings.weight\"] = word_embeddings\n",
    "\n",
    "    # The position embeddings.\n",
    "    pos_embeddings = embeddings[\"position_embeddings\"][\"weight\"]\n",
    "    # Trained for 512 x 1024.\n",
    "    assert pos_embeddings.size(0) == 512 and pos_embeddings.size(1) == 1024\n",
    "    # Store the position embeddings.\n",
    "    output_state_dict[\"bert.embeddings.position_embeddings.weight\"] = pos_embeddings\n",
    "\n",
    "    # The token-type embeddings.\n",
    "    tokentype_embeddings = embeddings[\"tokentype_embeddings\"][\"weight\"]\n",
    "    # Store the position embeddings.\n",
    "    output_state_dict[\"bert.embeddings.token_type_embeddings.weight\"] = tokentype_embeddings\n",
    "\n",
    "    # The transformer.\n",
    "    transformer = lm[\"transformer\"]\n",
    "\n",
    "    # The regex to extract layer names.\n",
    "    layer_re = re.compile(\"layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)\")\n",
    "\n",
    "    # The simple map of names for \"automated\" rules.\n",
    "    megatron_to_transformers = {\n",
    "        \"attention.dense\": \".attention.output.dense.\",\n",
    "        \"mlp.dense_h_to_4h\": \".intermediate.dense.\",\n",
    "        \"mlp.dense_4h_to_h\": \".output.dense.\",\n",
    "    }\n",
    "\n",
    "    # Keep track of the attention/query/value tensor.\n",
    "    attention_qkv_weight = None\n",
    "\n",
    "    # Extract the layers.\n",
    "    for key, val in transformer.items():\n",
    "        # Match the name.\n",
    "        m = layer_re.match(key)\n",
    "\n",
    "        # Stop if that's not a layer\n",
    "        if m is None:\n",
    "            break\n",
    "\n",
    "        # The index of the layer.\n",
    "        layer_idx = int(m.group(1))\n",
    "        # The name of the operation.\n",
    "        op_name = m.group(2)\n",
    "        # Is it a weight or a bias?\n",
    "        weight_or_bias = m.group(3)\n",
    "\n",
    "        # The name of the layer.\n",
    "        layer_name = f\"bert.encoder.layer.{layer_idx}\"\n",
    "\n",
    "        # For layernorm(s), simply store the layer norm.\n",
    "        if op_name.endswith(\"layernorm\"):\n",
    "\n",
    "            ln_name = \"attention.ln\" if op_name.startswith(\"input\") else \"ln\"\n",
    "            output_state_dict[layer_name + \".\" + ln_name + \".\" + weight_or_bias] = val\n",
    "\n",
    "        # Transpose the QKV matrix.\n",
    "        elif op_name == \"attention.query_key_value\" and weight_or_bias == \"weight\":\n",
    "\n",
    "            # Make sure the QKV pointer is nil.\n",
    "            assert attention_qkv_weight is None, \"\"\n",
    "\n",
    "            # Store the tensor as we need the bias as well to interleave QKV and biases.\n",
    "            attention_qkv_weight = val\n",
    "\n",
    "        # Transpose the bias.\n",
    "        elif op_name == \"attention.query_key_value\" and weight_or_bias == \"bias\":\n",
    "\n",
    "            # Make sure we read the weight tensor.\n",
    "            assert attention_qkv_weight is not None, \"\"\n",
    "\n",
    "            # Split the QKV matrix into Q, K and V. Megatron stores Q,K,V interleaved.\n",
    "            q = attention_qkv_weight[0 * 1024 : 1 * 1024, :]\n",
    "            k = attention_qkv_weight[1 * 1024 : 2 * 1024, :]\n",
    "            v = attention_qkv_weight[2 * 1024 : 3 * 1024, :]\n",
    "\n",
    "            # Split the bias.\n",
    "            q_bias = val[0 * 1024 : 1 * 1024]\n",
    "            k_bias = val[1 * 1024 : 2 * 1024]\n",
    "            v_bias = val[2 * 1024 : 3 * 1024]\n",
    "\n",
    "            # Store.\n",
    "            output_state_dict[f\"{layer_name}.attention.self.query.weight\"] = q\n",
    "            output_state_dict[f\"{layer_name}.attention.self.query.bias\"] = q_bias\n",
    "            output_state_dict[f\"{layer_name}.attention.self.key.weight\"] = k\n",
    "            output_state_dict[f\"{layer_name}.attention.self.key.bias\"] = k_bias\n",
    "            output_state_dict[f\"{layer_name}.attention.self.value.weight\"] = v\n",
    "            output_state_dict[f\"{layer_name}.attention.self.value.bias\"] = v_bias\n",
    "\n",
    "            # Clear the stored tensor.\n",
    "            attention_qkv_weight = None\n",
    "\n",
    "        # Copy weights and biases as is.\n",
    "        elif weight_or_bias in [\"weight\", \"bias\"]:\n",
    "\n",
    "            out_name = megatron_to_transformers[op_name]\n",
    "            output_state_dict[layer_name + out_name + weight_or_bias] = val\n",
    "\n",
    "    # The final layernorm.\n",
    "    output_state_dict[\"bert.encoder.ln.weight\"] = transformer[\"final_layernorm.weight\"]\n",
    "    output_state_dict[\"bert.encoder.ln.bias\"] = transformer[\"final_layernorm.bias\"]\n",
    "\n",
    "    # The config.\n",
    "    output_config = {\n",
    "        \"vocab_size\": word_embeddings.size(0),\n",
    "        \"hidden_size\": 1024,\n",
    "        \"num_hidden_layers\": 24,\n",
    "        \"num_attention_heads\": 16,\n",
    "        \"hidden_act\": \"gelu_new\",\n",
    "        \"intermediate_size\": 4096,\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"type_vocab_size\": 2,\n",
    "        \"initializer_range\": 0.2,\n",
    "        \"layer_norm_eps\": 1e-12,\n",
    "        \"position_embedding_type\": \"absolute\",\n",
    "        \"use_cache\": False,\n",
    "    }\n",
    "\n",
    "    print(lm.keys())\n",
    "    print(lm['embedding'].keys())\n",
    "    print(lm['transformer'].keys())\n",
    "    # The pooler.\n",
    "    pooler = lm[\"pooler\"]\n",
    "\n",
    "    # Store the matrix and the bias.\n",
    "    output_state_dict[\"bert.pooler.dense.weight\"] = pooler[\"dense.weight\"]\n",
    "    output_state_dict[\"bert.pooler.dense.bias\"] = pooler[\"dense.bias\"]\n",
    "\n",
    "    # The LM head from Megatron (for RACE).\n",
    "    lm_head = model[\"lm_head\"]\n",
    "\n",
    "    # The transform matrix.\n",
    "    output_state_dict[\"cls.predictions.transform.dense.weight\"] = lm_head[\"dense.weight\"]\n",
    "    output_state_dict[\"cls.predictions.transform.dense.bias\"] = lm_head[\"dense.bias\"]\n",
    "\n",
    "    # The transform LN.\n",
    "    output_state_dict[\"cls.predictions.transform.LayerNorm.weight\"] = lm_head[\"layernorm.weight\"]\n",
    "    output_state_dict[\"cls.predictions.transform.LayerNorm.bias\"] = lm_head[\"layernorm.bias\"]\n",
    "\n",
    "    # For the decoder, we replicate the weights.\n",
    "    output_state_dict[\"cls.predictions.decoder.weight\"] = word_embeddings\n",
    "    output_state_dict[\"cls.predictions.bias\"] = lm_head[\"bias\"]\n",
    "\n",
    "    # The classifier from Megatron (for MLNI).\n",
    "    binary_head = model[\"binary_head\"]\n",
    "\n",
    "    # Store the classifier.\n",
    "    output_state_dict[\"cls.seq_relationship.weight\"] = binary_head[\"weight\"]\n",
    "    output_state_dict[\"cls.seq_relationship.bias\"] = binary_head[\"bias\"]\n",
    "\n",
    "    # It should be done!\n",
    "    return output_state_dict, output_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "335a438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding', 'transformer'])\n",
      "dict_keys(['word_embeddings', 'position_embeddings', 'tokentype_embeddings'])\n",
      "odict_keys(['layers.0.input_layernorm.weight', 'layers.0.input_layernorm.bias', 'layers.0.attention.query_key_value.weight', 'layers.0.attention.query_key_value.bias', 'layers.0.attention.dense.weight', 'layers.0.attention.dense.bias', 'layers.0.post_attention_layernorm.weight', 'layers.0.post_attention_layernorm.bias', 'layers.0.mlp.dense_h_to_4h.weight', 'layers.0.mlp.dense_h_to_4h.bias', 'layers.0.mlp.dense_4h_to_h.weight', 'layers.0.mlp.dense_4h_to_h.bias', 'layers.1.input_layernorm.weight', 'layers.1.input_layernorm.bias', 'layers.1.attention.query_key_value.weight', 'layers.1.attention.query_key_value.bias', 'layers.1.attention.dense.weight', 'layers.1.attention.dense.bias', 'layers.1.post_attention_layernorm.weight', 'layers.1.post_attention_layernorm.bias', 'layers.1.mlp.dense_h_to_4h.weight', 'layers.1.mlp.dense_h_to_4h.bias', 'layers.1.mlp.dense_4h_to_h.weight', 'layers.1.mlp.dense_4h_to_h.bias', 'layers.2.input_layernorm.weight', 'layers.2.input_layernorm.bias', 'layers.2.attention.query_key_value.weight', 'layers.2.attention.query_key_value.bias', 'layers.2.attention.dense.weight', 'layers.2.attention.dense.bias', 'layers.2.post_attention_layernorm.weight', 'layers.2.post_attention_layernorm.bias', 'layers.2.mlp.dense_h_to_4h.weight', 'layers.2.mlp.dense_h_to_4h.bias', 'layers.2.mlp.dense_4h_to_h.weight', 'layers.2.mlp.dense_4h_to_h.bias', 'layers.3.input_layernorm.weight', 'layers.3.input_layernorm.bias', 'layers.3.attention.query_key_value.weight', 'layers.3.attention.query_key_value.bias', 'layers.3.attention.dense.weight', 'layers.3.attention.dense.bias', 'layers.3.post_attention_layernorm.weight', 'layers.3.post_attention_layernorm.bias', 'layers.3.mlp.dense_h_to_4h.weight', 'layers.3.mlp.dense_h_to_4h.bias', 'layers.3.mlp.dense_4h_to_h.weight', 'layers.3.mlp.dense_4h_to_h.bias', 'layers.4.input_layernorm.weight', 'layers.4.input_layernorm.bias', 'layers.4.attention.query_key_value.weight', 'layers.4.attention.query_key_value.bias', 'layers.4.attention.dense.weight', 'layers.4.attention.dense.bias', 'layers.4.post_attention_layernorm.weight', 'layers.4.post_attention_layernorm.bias', 'layers.4.mlp.dense_h_to_4h.weight', 'layers.4.mlp.dense_h_to_4h.bias', 'layers.4.mlp.dense_4h_to_h.weight', 'layers.4.mlp.dense_4h_to_h.bias', 'layers.5.input_layernorm.weight', 'layers.5.input_layernorm.bias', 'layers.5.attention.query_key_value.weight', 'layers.5.attention.query_key_value.bias', 'layers.5.attention.dense.weight', 'layers.5.attention.dense.bias', 'layers.5.post_attention_layernorm.weight', 'layers.5.post_attention_layernorm.bias', 'layers.5.mlp.dense_h_to_4h.weight', 'layers.5.mlp.dense_h_to_4h.bias', 'layers.5.mlp.dense_4h_to_h.weight', 'layers.5.mlp.dense_4h_to_h.bias', 'layers.6.input_layernorm.weight', 'layers.6.input_layernorm.bias', 'layers.6.attention.query_key_value.weight', 'layers.6.attention.query_key_value.bias', 'layers.6.attention.dense.weight', 'layers.6.attention.dense.bias', 'layers.6.post_attention_layernorm.weight', 'layers.6.post_attention_layernorm.bias', 'layers.6.mlp.dense_h_to_4h.weight', 'layers.6.mlp.dense_h_to_4h.bias', 'layers.6.mlp.dense_4h_to_h.weight', 'layers.6.mlp.dense_4h_to_h.bias', 'layers.7.input_layernorm.weight', 'layers.7.input_layernorm.bias', 'layers.7.attention.query_key_value.weight', 'layers.7.attention.query_key_value.bias', 'layers.7.attention.dense.weight', 'layers.7.attention.dense.bias', 'layers.7.post_attention_layernorm.weight', 'layers.7.post_attention_layernorm.bias', 'layers.7.mlp.dense_h_to_4h.weight', 'layers.7.mlp.dense_h_to_4h.bias', 'layers.7.mlp.dense_4h_to_h.weight', 'layers.7.mlp.dense_4h_to_h.bias', 'layers.8.input_layernorm.weight', 'layers.8.input_layernorm.bias', 'layers.8.attention.query_key_value.weight', 'layers.8.attention.query_key_value.bias', 'layers.8.attention.dense.weight', 'layers.8.attention.dense.bias', 'layers.8.post_attention_layernorm.weight', 'layers.8.post_attention_layernorm.bias', 'layers.8.mlp.dense_h_to_4h.weight', 'layers.8.mlp.dense_h_to_4h.bias', 'layers.8.mlp.dense_4h_to_h.weight', 'layers.8.mlp.dense_4h_to_h.bias', 'layers.9.input_layernorm.weight', 'layers.9.input_layernorm.bias', 'layers.9.attention.query_key_value.weight', 'layers.9.attention.query_key_value.bias', 'layers.9.attention.dense.weight', 'layers.9.attention.dense.bias', 'layers.9.post_attention_layernorm.weight', 'layers.9.post_attention_layernorm.bias', 'layers.9.mlp.dense_h_to_4h.weight', 'layers.9.mlp.dense_h_to_4h.bias', 'layers.9.mlp.dense_4h_to_h.weight', 'layers.9.mlp.dense_4h_to_h.bias', 'layers.10.input_layernorm.weight', 'layers.10.input_layernorm.bias', 'layers.10.attention.query_key_value.weight', 'layers.10.attention.query_key_value.bias', 'layers.10.attention.dense.weight', 'layers.10.attention.dense.bias', 'layers.10.post_attention_layernorm.weight', 'layers.10.post_attention_layernorm.bias', 'layers.10.mlp.dense_h_to_4h.weight', 'layers.10.mlp.dense_h_to_4h.bias', 'layers.10.mlp.dense_4h_to_h.weight', 'layers.10.mlp.dense_4h_to_h.bias', 'layers.11.input_layernorm.weight', 'layers.11.input_layernorm.bias', 'layers.11.attention.query_key_value.weight', 'layers.11.attention.query_key_value.bias', 'layers.11.attention.dense.weight', 'layers.11.attention.dense.bias', 'layers.11.post_attention_layernorm.weight', 'layers.11.post_attention_layernorm.bias', 'layers.11.mlp.dense_h_to_4h.weight', 'layers.11.mlp.dense_h_to_4h.bias', 'layers.11.mlp.dense_4h_to_h.weight', 'layers.11.mlp.dense_4h_to_h.bias', 'layers.12.input_layernorm.weight', 'layers.12.input_layernorm.bias', 'layers.12.attention.query_key_value.weight', 'layers.12.attention.query_key_value.bias', 'layers.12.attention.dense.weight', 'layers.12.attention.dense.bias', 'layers.12.post_attention_layernorm.weight', 'layers.12.post_attention_layernorm.bias', 'layers.12.mlp.dense_h_to_4h.weight', 'layers.12.mlp.dense_h_to_4h.bias', 'layers.12.mlp.dense_4h_to_h.weight', 'layers.12.mlp.dense_4h_to_h.bias', 'layers.13.input_layernorm.weight', 'layers.13.input_layernorm.bias', 'layers.13.attention.query_key_value.weight', 'layers.13.attention.query_key_value.bias', 'layers.13.attention.dense.weight', 'layers.13.attention.dense.bias', 'layers.13.post_attention_layernorm.weight', 'layers.13.post_attention_layernorm.bias', 'layers.13.mlp.dense_h_to_4h.weight', 'layers.13.mlp.dense_h_to_4h.bias', 'layers.13.mlp.dense_4h_to_h.weight', 'layers.13.mlp.dense_4h_to_h.bias', 'layers.14.input_layernorm.weight', 'layers.14.input_layernorm.bias', 'layers.14.attention.query_key_value.weight', 'layers.14.attention.query_key_value.bias', 'layers.14.attention.dense.weight', 'layers.14.attention.dense.bias', 'layers.14.post_attention_layernorm.weight', 'layers.14.post_attention_layernorm.bias', 'layers.14.mlp.dense_h_to_4h.weight', 'layers.14.mlp.dense_h_to_4h.bias', 'layers.14.mlp.dense_4h_to_h.weight', 'layers.14.mlp.dense_4h_to_h.bias', 'layers.15.input_layernorm.weight', 'layers.15.input_layernorm.bias', 'layers.15.attention.query_key_value.weight', 'layers.15.attention.query_key_value.bias', 'layers.15.attention.dense.weight', 'layers.15.attention.dense.bias', 'layers.15.post_attention_layernorm.weight', 'layers.15.post_attention_layernorm.bias', 'layers.15.mlp.dense_h_to_4h.weight', 'layers.15.mlp.dense_h_to_4h.bias', 'layers.15.mlp.dense_4h_to_h.weight', 'layers.15.mlp.dense_4h_to_h.bias', 'layers.16.input_layernorm.weight', 'layers.16.input_layernorm.bias', 'layers.16.attention.query_key_value.weight', 'layers.16.attention.query_key_value.bias', 'layers.16.attention.dense.weight', 'layers.16.attention.dense.bias', 'layers.16.post_attention_layernorm.weight', 'layers.16.post_attention_layernorm.bias', 'layers.16.mlp.dense_h_to_4h.weight', 'layers.16.mlp.dense_h_to_4h.bias', 'layers.16.mlp.dense_4h_to_h.weight', 'layers.16.mlp.dense_4h_to_h.bias', 'layers.17.input_layernorm.weight', 'layers.17.input_layernorm.bias', 'layers.17.attention.query_key_value.weight', 'layers.17.attention.query_key_value.bias', 'layers.17.attention.dense.weight', 'layers.17.attention.dense.bias', 'layers.17.post_attention_layernorm.weight', 'layers.17.post_attention_layernorm.bias', 'layers.17.mlp.dense_h_to_4h.weight', 'layers.17.mlp.dense_h_to_4h.bias', 'layers.17.mlp.dense_4h_to_h.weight', 'layers.17.mlp.dense_4h_to_h.bias', 'layers.18.input_layernorm.weight', 'layers.18.input_layernorm.bias', 'layers.18.attention.query_key_value.weight', 'layers.18.attention.query_key_value.bias', 'layers.18.attention.dense.weight', 'layers.18.attention.dense.bias', 'layers.18.post_attention_layernorm.weight', 'layers.18.post_attention_layernorm.bias', 'layers.18.mlp.dense_h_to_4h.weight', 'layers.18.mlp.dense_h_to_4h.bias', 'layers.18.mlp.dense_4h_to_h.weight', 'layers.18.mlp.dense_4h_to_h.bias', 'layers.19.input_layernorm.weight', 'layers.19.input_layernorm.bias', 'layers.19.attention.query_key_value.weight', 'layers.19.attention.query_key_value.bias', 'layers.19.attention.dense.weight', 'layers.19.attention.dense.bias', 'layers.19.post_attention_layernorm.weight', 'layers.19.post_attention_layernorm.bias', 'layers.19.mlp.dense_h_to_4h.weight', 'layers.19.mlp.dense_h_to_4h.bias', 'layers.19.mlp.dense_4h_to_h.weight', 'layers.19.mlp.dense_4h_to_h.bias', 'layers.20.input_layernorm.weight', 'layers.20.input_layernorm.bias', 'layers.20.attention.query_key_value.weight', 'layers.20.attention.query_key_value.bias', 'layers.20.attention.dense.weight', 'layers.20.attention.dense.bias', 'layers.20.post_attention_layernorm.weight', 'layers.20.post_attention_layernorm.bias', 'layers.20.mlp.dense_h_to_4h.weight', 'layers.20.mlp.dense_h_to_4h.bias', 'layers.20.mlp.dense_4h_to_h.weight', 'layers.20.mlp.dense_4h_to_h.bias', 'layers.21.input_layernorm.weight', 'layers.21.input_layernorm.bias', 'layers.21.attention.query_key_value.weight', 'layers.21.attention.query_key_value.bias', 'layers.21.attention.dense.weight', 'layers.21.attention.dense.bias', 'layers.21.post_attention_layernorm.weight', 'layers.21.post_attention_layernorm.bias', 'layers.21.mlp.dense_h_to_4h.weight', 'layers.21.mlp.dense_h_to_4h.bias', 'layers.21.mlp.dense_4h_to_h.weight', 'layers.21.mlp.dense_4h_to_h.bias', 'layers.22.input_layernorm.weight', 'layers.22.input_layernorm.bias', 'layers.22.attention.query_key_value.weight', 'layers.22.attention.query_key_value.bias', 'layers.22.attention.dense.weight', 'layers.22.attention.dense.bias', 'layers.22.post_attention_layernorm.weight', 'layers.22.post_attention_layernorm.bias', 'layers.22.mlp.dense_h_to_4h.weight', 'layers.22.mlp.dense_h_to_4h.bias', 'layers.22.mlp.dense_4h_to_h.weight', 'layers.22.mlp.dense_4h_to_h.bias', 'layers.23.input_layernorm.weight', 'layers.23.input_layernorm.bias', 'layers.23.attention.query_key_value.weight', 'layers.23.attention.query_key_value.bias', 'layers.23.attention.dense.weight', 'layers.23.attention.dense.bias', 'layers.23.post_attention_layernorm.weight', 'layers.23.post_attention_layernorm.bias', 'layers.23.mlp.dense_h_to_4h.weight', 'layers.23.mlp.dense_h_to_4h.bias', 'layers.23.mlp.dense_4h_to_h.weight', 'layers.23.mlp.dense_4h_to_h.bias', 'final_layernorm.weight', 'final_layernorm.bias'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pooler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_935/752341143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_megatron_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_935/4126139918.py\u001b[0m in \u001b[0;36mconvert_megatron_checkpoint\u001b[0;34m(args, input_state_dict)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# The pooler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mpooler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pooler\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Store the matrix and the bias.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pooler'"
     ]
    }
   ],
   "source": [
    "output_state_dict, output_config = convert_megatron_checkpoint({}, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be0ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8fefc3cd0c416481d88d8b51061302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'labels', 'tag_mask'],\n",
       "    num_rows: 48771\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('EMBO/sd-nlp-non-tokenized', 'NER')\n",
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa359084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef54e1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e67128e7e84bd9911e9d9fefdd9b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('drAbreu/sd-nlp-2', 'NER')\n",
    "ds['train'][0]['tag_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16030c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "****************************************************************************************************\n",
    "Dataset({\n",
    "    features: ['input_ids', 'labels', 'tag_mask'],\n",
    "    num_rows: 48771\n",
    "})\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "****************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a99080",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318e5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062b126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1a6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14991a21",
   "metadata": {},
   "source": [
    "## Testing a loop for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5570ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from smtag.metrics import MetricsTOKCL\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from smtag.config import Config\n",
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32755d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\"NER\", \"GENEPROD_ROLES\", \"SMALL_MOL_ROLES\", \"BORING\", \"PANELIZATION\"]\n",
    "MODELS = [\"EMBO/bio-lm\", \"roberta-base\"]\n",
    "\n",
    "ROBERTA_DATASET = \"drAbreu/sd-nlp-2\"\n",
    "GENERAL_DATASET = \"EMBO/sd-nlp-non-tokenized\"\n",
    "\n",
    "HUB_TOKEN = \"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\"\n",
    "\n",
    "HUB_USER = \"EMBO\"\n",
    "\n",
    "load_dotenv()\n",
    "LM_MODEL_PATH = os.getenv('LM_MODEL_PATH')\n",
    "TOKENIZER_PATH = os.getenv('TOKENIZER_PATH')\n",
    "TOKCL_MODEL_PATH = os.getenv('TOKCL_MODEL_PATH')\n",
    "CACHE = os.getenv('CACHE')\n",
    "RUNS_DIR = os.getenv('RUNS_DIR')\n",
    "\n",
    "TRAINING_ARGS_DICT = {\"output_dir\": TOKCL_MODEL_PATH,\n",
    "                     \"overwrite_output_dir\": True,\n",
    "                    \"logging_steps\":1000,\n",
    "                    \"evaluation_strategy\":'epoch',\n",
    "                    \"lr_scheduler_type\":'linear', \n",
    "                    \"save_strategy\":'epoch', \n",
    "                    \"save_steps\":1, \n",
    "#                     \"eval_strategy\":'epoch', \n",
    "                    \"save_total_limit\":5, \n",
    "                    \"seed\":42, \n",
    "                    \"eval_steps\":1, \n",
    "                    \"past_index\":-1, \n",
    "                    \"run_name\":TOKCL_MODEL_PATH, \n",
    "                    \"disable_tqdm\":False, \n",
    "                    \"metric_for_best_model\":'overall_f1', \n",
    "                    \"load_best_model_at_end\":True, \n",
    "                    \"greater_is_better\":True, \n",
    "                    \"length_column_name\":'length', \n",
    "                    \"report_to\":['tensorboard'], \n",
    "                    \"push_to_hub\":False, \n",
    "                    \"resume_from_checkpoint\":None,  \n",
    "                    \"hub_strategy\":'every_save', \n",
    "                    \"hub_token\":HUB_TOKEN, \n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e39a6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7980f743c3c41d3a4117420c6c8b568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"drAbreu/sd-nlp-2\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c0acf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48771"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9462e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_labels(dataset):\n",
    "    num_labels = dataset.info.features['labels'].feature.num_classes\n",
    "    label_list = dataset.info.features['labels'].feature.names\n",
    "    id2label, label2id = {}, {}\n",
    "    for class_, label in zip(range(num_labels), label_list):\n",
    "        id2label[class_] = label \n",
    "        label2id[label] = class_ \n",
    "        return id2label, label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98a1bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_label(label):\n",
    "    # If the label is B-XXX we change it to I-XX\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    Expands the NER tags once the sub-word tokenization is added.\n",
    "    Arguments\n",
    "    ---------\n",
    "    labels list[int]:\n",
    "    word_ids list[int]\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            # As far as word_id matches the index of the current word\n",
    "            # We append the same label\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            new_labels.append(shift_label(labels[word_id]))\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['words'], \n",
    "                       truncation=True,\n",
    "                       is_split_into_words=True)\n",
    "    \n",
    "    all_labels = examples['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0549080e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab6b802e05a4661aa3d6e5acbdb117b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_586/1982323694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mid2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n\u001b[1;32m     17\u001b[0m                                                                \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1748\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0;31m# Second attempt. If we have not yet found tokenizer_class, let's try to use the config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m                 config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1806\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m                     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mconfig_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             configuration_file = get_configuration_file(\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_configuration_file\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \"\"\"\n\u001b[1;32m    842\u001b[0m     \u001b[0;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m     all_files = get_list_of_files(\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist_repo_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             args_msg = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mlist_repo_files\u001b[0;34m(self, repo_id, revision, repo_type, token, timeout)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \"\"\"\n\u001b[1;32m   1161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrepo_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrepo_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m             info = self.model_info(\n\u001b[0m\u001b[1;32m   1163\u001b[0m                 \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             args_msg = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout, securityStatus)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Bearer {token}\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mstatus_query_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"securityStatus\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msecurityStatus\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         r = requests.get(\n\u001b[0m\u001b[1;32m   1119\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_query_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_repo_names = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    for task in TASKS:\n",
    "        TRAINING_ARGS_DICT[\"hub_model_id\"] = f\"{HUB_USER}/{model_name.replace('/','_')}_{task}\"\n",
    "        if model_name in [\"EMBO/bio-lm\", \"roberta-base\"]:\n",
    "            \n",
    "            config = Config(model_type = \"Autoencoder\", \n",
    "                            from_pretrained = model_name, \n",
    "                            tokenizer = \"roberta-base\")\n",
    "            \n",
    "            data = load_dataset(ROBERTA_DATASET, task)\n",
    "            train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "            id2label, label2id = define_labels(train_dataset)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)\n",
    "            data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                                               padding=True,\n",
    "                                                               return_tensors='pt')\n",
    "            \n",
    "            compute_metrics = MetricsTOKCL(label_list=list(label2id.keys()))\n",
    "            \n",
    "            \n",
    "            # Get the training arguments\n",
    "            training_args = TrainingArgumentsTOKCL(**TRAINING_ARGS_DICT)\n",
    "\n",
    "            # Select the model (This is for Token Classification)\n",
    "            \n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                config.from_pretrained,\n",
    "                num_labels=len(list(id2label.keys())),\n",
    "                max_position_embeddings=config.max_length + 2,  \n",
    "                id2label = id2label,\n",
    "                label2id = label2id\n",
    "            )\n",
    "            model_config = model.config\n",
    "            \n",
    "            # Define the trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[ShowExampleTOKCL(tokenizer)]\n",
    "            )\n",
    "\n",
    "            # switch the Tensorboard callback to plot losses on same plot\n",
    "            trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "            trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "#             train()\n",
    "            list_repo_names.append(TRAINING_ARGS_DICT[\"hub_model_id\"])\n",
    "    \n",
    "        elif model_name in []:\n",
    "            data = load_dataset(GENERAL_DATASET, task)\n",
    "            list_repo_names.append(TRAINING_ARGS_DICT[\"hub_model_id\"])\n",
    "        else:\n",
    "            raise ValueError(f\"The selected model ({model_name}) is not contained in our Benchmark list. Please add it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63e75aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": 0.5,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", \n",
    "                                                num_labels=15,\n",
    "                                               classifier_dropout=0.5).config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2147b1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_config(\n",
    "                BertConfig(**{\n",
    "                    \"_name_or_path\": \"bert-base-uncased\",\n",
    "                    \"classifier_dropout\": 0.1,\n",
    "                    \"hidden_size\": 768,\n",
    "                    \"num_labels\": 15,\n",
    "                    \"max_position_embeddings\": 512}\n",
    "                              )\n",
    "            )\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "840bcbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, MegatronBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31020211",
   "metadata": {},
   "outputs": [],
   "source": [
    "megatron = AutoModelForTokenClassification.from_config(MegatronBertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6a73530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MegatronBertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"megatron-bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 29056\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "megatron.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "56ec47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "biomegatron_weights = torch.load(\"data/models/megatron/MegatronBERT.pt\",\n",
    "                                map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3984c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': {'word_embeddings': OrderedDict([('weight',\n",
       "                tensor([[ 0.0185, -0.0429, -0.0260,  ...,  0.0311, -0.0312, -0.0686],\n",
       "                        [ 0.0243, -0.0040, -0.0945,  ...,  0.0467, -0.0547, -0.0226],\n",
       "                        [ 0.0252, -0.0084, -0.0521,  ...,  0.0382, -0.0627, -0.0886],\n",
       "                        ...,\n",
       "                        [-0.0075,  0.0019, -0.0333,  ..., -0.0016, -0.0105, -0.0500],\n",
       "                        [-0.0059,  0.0071, -0.0423,  ..., -0.0025, -0.0124, -0.0368],\n",
       "                        [ 0.0005, -0.0040, -0.0260,  ..., -0.0439,  0.0103, -0.0366]],\n",
       "                       dtype=torch.float16))]),\n",
       "  'position_embeddings': OrderedDict([('weight',\n",
       "                tensor([[ 0.0553,  0.0024,  0.0171,  ...,  0.0003, -0.0299,  0.0214],\n",
       "                        [-0.0094, -0.0430,  0.0197,  ...,  0.0025, -0.0072,  0.0046],\n",
       "                        [ 0.0090, -0.0186, -0.0066,  ..., -0.0168, -0.0044, -0.0069],\n",
       "                        ...,\n",
       "                        [-0.0448, -0.0428,  0.0126,  ..., -0.0168,  0.0224,  0.0032],\n",
       "                        [-0.0467, -0.0109,  0.0104,  ...,  0.0051,  0.0394,  0.0011],\n",
       "                        [-0.0515,  0.0513,  0.0240,  ...,  0.0204, -0.0456,  0.1006]],\n",
       "                       dtype=torch.float16))]),\n",
       "  'tokentype_embeddings': OrderedDict([('weight',\n",
       "                tensor([[ 0.0047,  0.0026, -0.0037,  ..., -0.0019, -0.0014,  0.0034],\n",
       "                        [ 0.0021,  0.0037, -0.0037,  ..., -0.0023, -0.0043,  0.0038]],\n",
       "                       dtype=torch.float16))])},\n",
       " 'transformer': OrderedDict([('layers.0.input_layernorm.weight',\n",
       "               tensor([0.2949, 0.2754, 0.2812,  ..., 0.2991, 0.2778, 0.3103],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.input_layernorm.bias',\n",
       "               tensor([-0.0223,  0.0016, -0.0022,  ..., -0.0080,  0.0286, -0.0186],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.attention.query_key_value.weight',\n",
       "               tensor([[-0.0336, -0.0028, -0.0144,  ..., -0.0451, -0.0117,  0.0859],\n",
       "                       [ 0.0258, -0.0656, -0.0133,  ..., -0.0376,  0.0131,  0.0209],\n",
       "                       [-0.0048,  0.0249, -0.0284,  ..., -0.0016, -0.0750, -0.0110],\n",
       "                       ...,\n",
       "                       [-0.0285,  0.0092, -0.0029,  ...,  0.0234, -0.0325,  0.0079],\n",
       "                       [ 0.0359, -0.0001, -0.0358,  ..., -0.0400,  0.0255,  0.0310],\n",
       "                       [ 0.0333,  0.0137, -0.0386,  ..., -0.0751,  0.0259, -0.0021]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.attention.query_key_value.bias',\n",
       "               tensor([ 0.0319, -0.0452, -0.4368,  ...,  0.0078, -0.0072,  0.0366],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.attention.dense.weight',\n",
       "               tensor([[ 0.0184,  0.0120,  0.0051,  ...,  0.0073,  0.0156, -0.0104],\n",
       "                       [ 0.0126, -0.0079, -0.0025,  ..., -0.0229,  0.0168,  0.0013],\n",
       "                       [ 0.0055,  0.0008, -0.0044,  ..., -0.0024, -0.0254, -0.0453],\n",
       "                       ...,\n",
       "                       [-0.0110,  0.0014,  0.0006,  ...,  0.0061,  0.0030,  0.0010],\n",
       "                       [ 0.0062,  0.0117, -0.0075,  ...,  0.0281,  0.0098,  0.0274],\n",
       "                       [ 0.0058,  0.0070,  0.0200,  ..., -0.0126,  0.0163, -0.0095]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.attention.dense.bias',\n",
       "               tensor([ 0.0038,  0.0027, -0.0012,  ..., -0.0012, -0.0059, -0.0006],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.post_attention_layernorm.weight',\n",
       "               tensor([0.7603, 0.7339, 0.7627,  ..., 0.7642, 0.7373, 0.7510],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.post_attention_layernorm.bias',\n",
       "               tensor([-0.0135, -0.0158, -0.0370,  ..., -0.0650, -0.0301, -0.0235],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0495,  0.0037,  0.0090,  ..., -0.0506, -0.0246,  0.0033],\n",
       "                       [ 0.0482, -0.0737, -0.0040,  ..., -0.0069,  0.0129,  0.0374],\n",
       "                       [-0.0111,  0.0037, -0.0220,  ..., -0.0127,  0.0061,  0.0216],\n",
       "                       ...,\n",
       "                       [ 0.0561,  0.0448,  0.0237,  ...,  0.1069,  0.0152, -0.0206],\n",
       "                       [-0.0968,  0.0598, -0.0430,  ...,  0.0789, -0.0950,  0.0477],\n",
       "                       [-0.0163, -0.0126,  0.0095,  ..., -0.0567, -0.0372, -0.0439]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0402, -0.0434, -0.0682,  ..., -0.0467, -0.0756, -0.0364],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0119, -0.0379, -0.0137,  ...,  0.0081, -0.0640,  0.0205],\n",
       "                       [-0.0144, -0.0294,  0.0424,  ..., -0.0481,  0.0406, -0.0029],\n",
       "                       [ 0.0619,  0.0563, -0.0119,  ...,  0.0186,  0.0751, -0.0227],\n",
       "                       ...,\n",
       "                       [ 0.0082,  0.0240, -0.0342,  ...,  0.0355,  0.0581,  0.0334],\n",
       "                       [ 0.0087,  0.0225,  0.0470,  ..., -0.0178,  0.0649,  0.0254],\n",
       "                       [-0.0094,  0.0521,  0.0078,  ...,  0.0132, -0.0226, -0.0724]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.0.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0147,  0.0052, -0.0775,  ...,  0.0433,  0.0048, -0.0319],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.input_layernorm.weight',\n",
       "               tensor([0.5918, 0.6724, 0.5938,  ..., 0.6348, 0.6733, 0.5918],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.input_layernorm.bias',\n",
       "               tensor([-0.0044, -0.0941, -0.0003,  ..., -0.0532, -0.0516, -0.0674],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.attention.query_key_value.weight',\n",
       "               tensor([[-2.7328e-02, -1.6449e-02, -1.0521e-02,  ...,  3.9581e-02,\n",
       "                         1.2146e-02,  4.6021e-02],\n",
       "                       [-4.1046e-02, -2.4757e-03, -4.0092e-03,  ...,  1.4626e-02,\n",
       "                        -1.8356e-02,  2.4109e-02],\n",
       "                       [ 5.9485e-05,  3.1137e-04, -4.0100e-02,  ..., -6.2561e-02,\n",
       "                        -7.1960e-02, -8.8730e-03],\n",
       "                       ...,\n",
       "                       [-5.3978e-03,  8.6212e-03, -5.1971e-02,  ...,  1.3969e-02,\n",
       "                         2.6184e-02,  1.7487e-02],\n",
       "                       [ 1.4511e-02, -2.4281e-03,  3.3203e-02,  ..., -5.6496e-03,\n",
       "                         1.9169e-03, -5.7449e-03],\n",
       "                       [-3.4393e-02,  1.5160e-02, -7.3547e-03,  ..., -3.9673e-03,\n",
       "                        -3.5339e-02,  3.0991e-02]], dtype=torch.float16)),\n",
       "              ('layers.1.attention.query_key_value.bias',\n",
       "               tensor([ 0.0272, -0.0283,  0.0477,  ..., -0.0792,  0.0085, -0.0567],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.attention.dense.weight',\n",
       "               tensor([[ 0.0140, -0.0141, -0.0608,  ..., -0.0094,  0.0043,  0.0119],\n",
       "                       [ 0.0082,  0.0254,  0.0004,  ...,  0.0181, -0.0148, -0.0388],\n",
       "                       [ 0.0051,  0.0209,  0.0085,  ...,  0.0063, -0.0254,  0.0036],\n",
       "                       ...,\n",
       "                       [ 0.0044, -0.0030,  0.0061,  ..., -0.0035,  0.0106, -0.0095],\n",
       "                       [-0.0122,  0.0056,  0.0011,  ..., -0.0061, -0.0276,  0.0206],\n",
       "                       [-0.0359, -0.0189,  0.0094,  ..., -0.0017,  0.0720, -0.0350]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.attention.dense.bias',\n",
       "               tensor([-0.0238,  0.0038, -0.0209,  ..., -0.0471,  0.0430,  0.0253],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.post_attention_layernorm.weight',\n",
       "               tensor([0.8022, 0.8506, 0.8672,  ..., 0.7808, 0.8428, 0.7578],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.post_attention_layernorm.bias',\n",
       "               tensor([-0.0432,  0.0341, -0.1295,  ..., -0.0238, -0.0223, -0.0159],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0239,  0.0005, -0.0106,  ..., -0.0392,  0.0659, -0.0257],\n",
       "                       [ 0.0400,  0.0168,  0.0168,  ...,  0.0592, -0.0245, -0.0164],\n",
       "                       [ 0.0453, -0.0137,  0.0315,  ...,  0.0329, -0.0490, -0.0055],\n",
       "                       ...,\n",
       "                       [-0.0010, -0.0301,  0.0208,  ...,  0.0584,  0.0031, -0.0679],\n",
       "                       [ 0.0038, -0.0373,  0.1431,  ...,  0.0331,  0.0140, -0.0779],\n",
       "                       [ 0.0686,  0.0618, -0.1023,  ..., -0.0150,  0.0242, -0.0110]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0458,  0.0010, -0.0386,  ...,  0.0617, -0.0404, -0.0489],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0361,  0.0862,  0.0312,  ...,  0.0282,  0.0180,  0.0497],\n",
       "                       [ 0.0350, -0.0553, -0.0059,  ..., -0.0866,  0.0618,  0.0475],\n",
       "                       [ 0.0414,  0.0327,  0.0323,  ...,  0.0333,  0.0476, -0.0527],\n",
       "                       ...,\n",
       "                       [-0.0093, -0.0289, -0.0104,  ...,  0.0643, -0.0383,  0.0169],\n",
       "                       [ 0.0021,  0.0057, -0.0281,  ..., -0.0595,  0.0071,  0.0309],\n",
       "                       [-0.0556, -0.0383, -0.0640,  ..., -0.0060,  0.0409, -0.0301]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.1.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0583,  0.0467, -0.0525,  ...,  0.0347,  0.0207, -0.0341],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.input_layernorm.weight',\n",
       "               tensor([0.7686, 0.8130, 0.7646,  ..., 0.8022, 0.7954, 0.7529],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.input_layernorm.bias',\n",
       "               tensor([ 0.0121, -0.0918,  0.0210,  ..., -0.0349, -0.0784, -0.0728],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0769,  0.0405, -0.0395,  ..., -0.0812, -0.0032,  0.0880],\n",
       "                       [ 0.0459, -0.0007,  0.0233,  ..., -0.0454, -0.0284,  0.0549],\n",
       "                       [-0.0012, -0.0892, -0.0346,  ..., -0.0182, -0.0936, -0.0262],\n",
       "                       ...,\n",
       "                       [ 0.0228, -0.0105, -0.0279,  ...,  0.0142, -0.0041,  0.0305],\n",
       "                       [-0.0308,  0.0079, -0.0074,  ..., -0.0218,  0.0111,  0.0035],\n",
       "                       [ 0.0070, -0.0496, -0.0143,  ..., -0.0361,  0.0282, -0.0147]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.attention.query_key_value.bias',\n",
       "               tensor([-0.0186, -0.0002, -0.0127,  ..., -0.0263, -0.0298, -0.0068],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.attention.dense.weight',\n",
       "               tensor([[-0.0122,  0.0044,  0.0131,  ..., -0.0103,  0.0205, -0.0162],\n",
       "                       [-0.0363,  0.0469,  0.0485,  ..., -0.0199,  0.0176,  0.0281],\n",
       "                       [ 0.0169,  0.0175,  0.0195,  ...,  0.0190,  0.0106,  0.0010],\n",
       "                       ...,\n",
       "                       [-0.0144,  0.0532,  0.0013,  ..., -0.0410,  0.0003,  0.0087],\n",
       "                       [-0.0257,  0.0371, -0.0528,  ..., -0.0041, -0.0137, -0.0296],\n",
       "                       [-0.0098, -0.0478,  0.0053,  ...,  0.0088, -0.0140,  0.0249]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.attention.dense.bias',\n",
       "               tensor([ 0.0422, -0.0140, -0.0446,  ...,  0.0384, -0.0426,  0.0222],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.post_attention_layernorm.weight',\n",
       "               tensor([0.9751, 0.9570, 0.9512,  ..., 0.9092, 0.9009, 0.8682],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.post_attention_layernorm.bias',\n",
       "               tensor([-0.0723,  0.0651, -0.0934,  ...,  0.0303, -0.0099, -0.0372],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0493, -0.0742,  0.0891,  ..., -0.0354, -0.0074,  0.0053],\n",
       "                       [-0.0515, -0.0341, -0.0184,  ..., -0.0737, -0.0357,  0.0792],\n",
       "                       [-0.0041,  0.0162, -0.0598,  ..., -0.0059,  0.0033, -0.0250],\n",
       "                       ...,\n",
       "                       [ 0.0294,  0.0347,  0.0229,  ..., -0.0257,  0.1092, -0.0376],\n",
       "                       [ 0.0010,  0.0044,  0.0588,  ..., -0.0416,  0.0236, -0.0595],\n",
       "                       [-0.0088,  0.0248,  0.0337,  ...,  0.0659, -0.0503,  0.0123]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0244, -0.0363, -0.0138,  ..., -0.0230, -0.0728, -0.0464],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0343, -0.0421,  0.0141,  ...,  0.0094, -0.0005, -0.0154],\n",
       "                       [ 0.0748, -0.0194,  0.0041,  ..., -0.0320, -0.0319, -0.0629],\n",
       "                       [-0.0363,  0.0488,  0.0366,  ...,  0.0294, -0.0488,  0.0500],\n",
       "                       ...,\n",
       "                       [ 0.0275, -0.0483,  0.0181,  ...,  0.0300,  0.0365,  0.0332],\n",
       "                       [-0.0044, -0.0092,  0.0127,  ...,  0.0626, -0.0342, -0.0495],\n",
       "                       [-0.0389,  0.0890,  0.0207,  ..., -0.0502,  0.0275, -0.0630]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.2.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0468,  0.0186, -0.0246,  ...,  0.0345,  0.0329, -0.0192],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.input_layernorm.weight',\n",
       "               tensor([0.8926, 0.8789, 0.7910,  ..., 0.9121, 0.8325, 0.7842],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.input_layernorm.bias',\n",
       "               tensor([ 0.0152, -0.0764,  0.0085,  ..., -0.0165, -0.1031, -0.0822],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.attention.query_key_value.weight',\n",
       "               tensor([[-0.0291,  0.0559, -0.0334,  ...,  0.0146,  0.0238, -0.0157],\n",
       "                       [-0.0168, -0.0578, -0.0100,  ..., -0.0522, -0.0231,  0.0651],\n",
       "                       [ 0.0544, -0.0246, -0.0325,  ..., -0.0298,  0.0338,  0.0298],\n",
       "                       ...,\n",
       "                       [-0.0032, -0.0247,  0.0141,  ...,  0.0110,  0.0410, -0.0756],\n",
       "                       [-0.0833, -0.0266, -0.0519,  ..., -0.0281,  0.0385, -0.0108],\n",
       "                       [ 0.0025,  0.0225,  0.0250,  ...,  0.0208, -0.0381,  0.0326]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.attention.query_key_value.bias',\n",
       "               tensor([ 0.0256,  0.0533,  0.0429,  ...,  0.0030, -0.0170,  0.0115],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.attention.dense.weight',\n",
       "               tensor([[ 0.0080, -0.0146, -0.0429,  ..., -0.0292, -0.0189, -0.0054],\n",
       "                       [-0.0011, -0.0080,  0.0192,  ...,  0.0180, -0.0132,  0.0153],\n",
       "                       [-0.0308, -0.0525, -0.0424,  ...,  0.0541, -0.0343, -0.0026],\n",
       "                       ...,\n",
       "                       [-0.0056, -0.0220,  0.0049,  ..., -0.0179,  0.0287, -0.0059],\n",
       "                       [-0.0548, -0.0159,  0.0034,  ..., -0.0054, -0.0546,  0.0291],\n",
       "                       [ 0.0025, -0.0728,  0.0212,  ...,  0.0247,  0.0299, -0.0230]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.attention.dense.bias',\n",
       "               tensor([ 0.0365,  0.0228, -0.0105,  ...,  0.0235, -0.0413, -0.0300],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.post_attention_layernorm.weight',\n",
       "               tensor([0.9380, 1.0693, 0.9595,  ..., 0.9956, 0.9912, 0.8496],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.post_attention_layernorm.bias',\n",
       "               tensor([-0.0598,  0.0341, -0.0610,  ...,  0.0333,  0.0033, -0.0313],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0039, -0.0546,  0.0282,  ..., -0.0187,  0.0125, -0.0748],\n",
       "                       [ 0.0487,  0.0798, -0.0044,  ...,  0.0084, -0.0515, -0.0323],\n",
       "                       [-0.0252, -0.0032,  0.0481,  ...,  0.0418, -0.0114,  0.0293],\n",
       "                       ...,\n",
       "                       [ 0.0665, -0.0630,  0.0443,  ..., -0.0055, -0.0627, -0.0428],\n",
       "                       [ 0.0065, -0.0221, -0.0103,  ..., -0.0847, -0.0189,  0.1135],\n",
       "                       [ 0.0419,  0.0456, -0.0277,  ...,  0.0261, -0.0044, -0.0026]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0564, -0.0554, -0.0415,  ..., -0.0505, -0.0425, -0.0364],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0606, -0.0449,  0.0014,  ...,  0.0461, -0.1027, -0.0542],\n",
       "                       [ 0.0561,  0.0037, -0.0095,  ..., -0.0567, -0.0583, -0.0450],\n",
       "                       [ 0.1011,  0.0361, -0.0177,  ...,  0.0744, -0.0089,  0.0462],\n",
       "                       ...,\n",
       "                       [-0.0228,  0.0956, -0.0136,  ..., -0.0017,  0.0011, -0.0304],\n",
       "                       [-0.0878,  0.0404, -0.0075,  ..., -0.0391, -0.0490,  0.0027],\n",
       "                       [-0.0138, -0.0024, -0.0260,  ...,  0.0081, -0.0084, -0.0580]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.3.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0424,  0.0119, -0.0150,  ..., -0.0255,  0.0296,  0.0102],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.input_layernorm.weight',\n",
       "               tensor([0.8569, 0.9111, 0.7358,  ..., 0.8350, 0.8086, 0.7754],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.input_layernorm.bias',\n",
       "               tensor([ 0.0090, -0.0444,  0.0008,  ..., -0.0138, -0.0933, -0.0901],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0416,  0.0812,  0.0282,  ..., -0.0260, -0.0822, -0.0587],\n",
       "                       [ 0.0042,  0.0267, -0.0276,  ..., -0.0452, -0.0144,  0.0111],\n",
       "                       [-0.0284, -0.0482, -0.0082,  ...,  0.0023,  0.0210, -0.0623],\n",
       "                       ...,\n",
       "                       [-0.0026,  0.0134,  0.0195,  ..., -0.0934, -0.0318,  0.0671],\n",
       "                       [-0.0051, -0.0525,  0.0294,  ..., -0.0182, -0.0452,  0.0208],\n",
       "                       [-0.0454, -0.0401, -0.0184,  ...,  0.0254,  0.0011,  0.1036]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.attention.query_key_value.bias',\n",
       "               tensor([-0.0167, -0.0929, -0.0619,  ...,  0.0008,  0.0210,  0.0204],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.attention.dense.weight',\n",
       "               tensor([[ 0.0226, -0.0290, -0.0154,  ...,  0.0317, -0.0093,  0.0331],\n",
       "                       [ 0.0244, -0.0359,  0.0342,  ..., -0.0190,  0.0358, -0.0146],\n",
       "                       [ 0.0501, -0.0033,  0.0089,  ..., -0.0021,  0.0177, -0.0258],\n",
       "                       ...,\n",
       "                       [-0.0213, -0.0108,  0.0124,  ...,  0.0143,  0.0259, -0.0638],\n",
       "                       [-0.0357,  0.0327,  0.0486,  ...,  0.0429,  0.0426, -0.0762],\n",
       "                       [-0.0364, -0.0029, -0.0171,  ..., -0.0553,  0.0066, -0.0462]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.attention.dense.bias',\n",
       "               tensor([ 0.0077, -0.0966,  0.0212,  ..., -0.0130, -0.0275,  0.0116],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.post_attention_layernorm.weight',\n",
       "               tensor([1.0127, 1.0488, 0.9434,  ..., 0.9473, 1.0117, 0.8940],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.post_attention_layernorm.bias',\n",
       "               tensor([-0.0772,  0.0441, -0.0640,  ..., -0.0147,  0.0076,  0.0133],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0356, -0.0392, -0.0299,  ...,  0.0809,  0.0163, -0.0230],\n",
       "                       [-0.0264, -0.0028,  0.0583,  ...,  0.0174,  0.0492,  0.0169],\n",
       "                       [-0.0563,  0.0431,  0.0248,  ...,  0.0008, -0.0315,  0.0090],\n",
       "                       ...,\n",
       "                       [ 0.0186, -0.0424, -0.0318,  ...,  0.0527,  0.0550, -0.0067],\n",
       "                       [-0.0246,  0.0707,  0.0349,  ..., -0.0281, -0.0358, -0.0021],\n",
       "                       [ 0.0290,  0.0358, -0.0552,  ...,  0.0253, -0.0161,  0.0242]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0368, -0.0603, -0.0379,  ..., -0.0479, -0.0453, -0.0304],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0168, -0.0489,  0.0845,  ..., -0.0175, -0.0136, -0.0321],\n",
       "                       [-0.0515, -0.0135, -0.0230,  ..., -0.0380, -0.0221, -0.0122],\n",
       "                       [ 0.0050,  0.1126, -0.0282,  ..., -0.0055,  0.0171,  0.0269],\n",
       "                       ...,\n",
       "                       [ 0.0378,  0.0516,  0.0006,  ...,  0.0105, -0.0040, -0.0344],\n",
       "                       [-0.0045,  0.0034,  0.0192,  ..., -0.0456, -0.0125,  0.0138],\n",
       "                       [-0.0959, -0.0340, -0.0070,  ...,  0.0410,  0.0263, -0.0293]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.4.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0310,  0.0405, -0.0009,  ..., -0.0177,  0.0237,  0.0131],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.input_layernorm.weight',\n",
       "               tensor([0.8838, 0.9771, 0.8315,  ..., 0.9077, 0.8877, 0.8364],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.input_layernorm.bias',\n",
       "               tensor([-0.0091, -0.0332,  0.0035,  ..., -0.0162, -0.0939, -0.0779],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.attention.query_key_value.weight',\n",
       "               tensor([[-0.0194,  0.0346, -0.0015,  ...,  0.0592,  0.0052, -0.0587],\n",
       "                       [-0.0497, -0.0798, -0.0194,  ..., -0.0540,  0.0837,  0.0279],\n",
       "                       [-0.0144, -0.0095,  0.0752,  ...,  0.0405, -0.0005, -0.0750],\n",
       "                       ...,\n",
       "                       [ 0.0246, -0.0767,  0.0490,  ..., -0.0234,  0.0014, -0.0037],\n",
       "                       [-0.0215,  0.0119, -0.0299,  ...,  0.0063, -0.0109,  0.0012],\n",
       "                       [ 0.0737, -0.0257, -0.0094,  ...,  0.0112, -0.0035, -0.0135]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.attention.query_key_value.bias',\n",
       "               tensor([ 0.0038,  0.0105,  0.0022,  ...,  0.0016,  0.0261, -0.0170],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.attention.dense.weight',\n",
       "               tensor([[-0.0183, -0.0261,  0.0093,  ..., -0.0909,  0.0346, -0.0255],\n",
       "                       [-0.0923, -0.0249, -0.0359,  ..., -0.0074,  0.0370, -0.0232],\n",
       "                       [ 0.0277,  0.0463, -0.0129,  ...,  0.0172,  0.0284,  0.0313],\n",
       "                       ...,\n",
       "                       [-0.0531, -0.0248,  0.0087,  ..., -0.0421,  0.0204, -0.0231],\n",
       "                       [-0.0712, -0.0061, -0.0258,  ...,  0.0016,  0.0234, -0.0140],\n",
       "                       [-0.0077, -0.0140,  0.0682,  ...,  0.0138,  0.0218,  0.0228]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.attention.dense.bias',\n",
       "               tensor([ 0.0534,  0.0402, -0.0157,  ...,  0.0240,  0.0156, -0.0243],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.post_attention_layernorm.weight',\n",
       "               tensor([1.0459, 1.0488, 0.9648,  ..., 1.0518, 1.0615, 0.8857],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.post_attention_layernorm.bias',\n",
       "               tensor([-0.0247,  0.0556, -0.0755,  ..., -0.0033,  0.0294,  0.0156],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0478, -0.0281,  0.0283,  ..., -0.0137,  0.0275,  0.1224],\n",
       "                       [ 0.0244, -0.1212,  0.0109,  ...,  0.0448,  0.0082, -0.0587],\n",
       "                       [ 0.0342,  0.0170, -0.0593,  ..., -0.0377,  0.0374, -0.0343],\n",
       "                       ...,\n",
       "                       [ 0.0185,  0.0649,  0.0072,  ..., -0.0273,  0.0432, -0.0171],\n",
       "                       [-0.0042, -0.0144,  0.0173,  ..., -0.0273, -0.0568, -0.0264],\n",
       "                       [ 0.0018, -0.0276,  0.1035,  ..., -0.0048, -0.0461,  0.0286]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0289, -0.0739, -0.0671,  ...,  0.0399, -0.0380, -0.0273],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0352,  0.0388,  0.0120,  ...,  0.0192,  0.0341,  0.0056],\n",
       "                       [-0.0565,  0.0189,  0.0002,  ...,  0.0589,  0.0101,  0.0515],\n",
       "                       [ 0.0410, -0.0151, -0.0401,  ..., -0.0243,  0.0092, -0.0259],\n",
       "                       ...,\n",
       "                       [ 0.0490,  0.0028,  0.0003,  ...,  0.0320,  0.0518,  0.0276],\n",
       "                       [ 0.0309,  0.0932,  0.0355,  ..., -0.0327,  0.0624,  0.0410],\n",
       "                       [-0.0297,  0.0353,  0.0012,  ..., -0.0318,  0.0095, -0.0375]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.5.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0211,  0.0221, -0.0062,  ..., -0.0442,  0.0399,  0.0313],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.input_layernorm.weight',\n",
       "               tensor([0.8916, 0.8740, 0.7881,  ..., 0.9150, 0.8799, 0.7930],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.input_layernorm.bias',\n",
       "               tensor([ 2.3232e-03, -2.5574e-02,  9.4271e-04,  ..., -4.3511e-06,\n",
       "                       -7.4951e-02, -6.8481e-02], dtype=torch.float16)),\n",
       "              ('layers.6.attention.query_key_value.weight',\n",
       "               tensor([[-0.0260, -0.0665, -0.0627,  ...,  0.0504,  0.0532,  0.0065],\n",
       "                       [ 0.1223, -0.0353, -0.0987,  ...,  0.0366, -0.0520,  0.0360],\n",
       "                       [-0.0682,  0.0461,  0.0891,  ...,  0.0058, -0.0020,  0.0167],\n",
       "                       ...,\n",
       "                       [-0.0667,  0.0200,  0.0504,  ...,  0.0216,  0.0146, -0.0398],\n",
       "                       [ 0.0228, -0.0277,  0.0334,  ...,  0.0017, -0.0939, -0.0402],\n",
       "                       [ 0.0068, -0.0142, -0.0409,  ...,  0.0468,  0.0472, -0.0372]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.attention.query_key_value.bias',\n",
       "               tensor([ 0.0685,  0.0072, -0.0065,  ...,  0.0242,  0.0154, -0.0156],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.attention.dense.weight',\n",
       "               tensor([[-0.0439,  0.0122,  0.0710,  ..., -0.0012,  0.0252,  0.0403],\n",
       "                       [ 0.0039, -0.0224,  0.0149,  ..., -0.0519, -0.0351,  0.0287],\n",
       "                       [-0.0050, -0.0335,  0.0030,  ...,  0.0221, -0.0522,  0.0156],\n",
       "                       ...,\n",
       "                       [-0.0729,  0.0102,  0.0250,  ...,  0.0443,  0.0733,  0.0700],\n",
       "                       [-0.0111,  0.0024,  0.0308,  ..., -0.0061,  0.0562, -0.0013],\n",
       "                       [ 0.0183,  0.0383, -0.0532,  ..., -0.0014, -0.0489, -0.0183]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.attention.dense.bias',\n",
       "               tensor([ 0.0346,  0.0079, -0.0150,  ..., -0.0148,  0.0224,  0.0507],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.post_attention_layernorm.weight',\n",
       "               tensor([0.9980, 0.9785, 0.9854,  ..., 1.0439, 1.0215, 0.9272],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.post_attention_layernorm.bias',\n",
       "               tensor([-0.0122,  0.0459, -0.0643,  ..., -0.0288,  0.0764,  0.0246],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 2.2476e-02,  1.7487e-02,  2.3537e-03,  ...,  3.5553e-02,\n",
       "                         5.1910e-02, -9.2407e-02],\n",
       "                       [-1.9623e-02, -3.2715e-02, -7.0143e-04,  ..., -3.6407e-02,\n",
       "                        -1.1688e-02, -2.9236e-02],\n",
       "                       [ 7.7784e-05,  2.0309e-02,  3.2349e-02,  ..., -6.5613e-02,\n",
       "                         1.0979e-02, -3.8513e-02],\n",
       "                       ...,\n",
       "                       [-5.9509e-03, -5.7983e-02,  3.4454e-02,  ...,  6.2370e-03,\n",
       "                        -2.5864e-02, -6.3660e-02],\n",
       "                       [-1.1292e-02, -5.3650e-02, -1.5610e-02,  ...,  1.3245e-01,\n",
       "                        -4.7272e-02,  5.3635e-03],\n",
       "                       [ 5.0995e-02, -4.2450e-02,  1.2878e-01,  ...,  3.7567e-02,\n",
       "                        -2.9175e-02,  3.6865e-02]], dtype=torch.float16)),\n",
       "              ('layers.6.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0120, -0.0865, -0.0363,  ..., -0.0311, -0.0075, -0.0604],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0359,  0.0330, -0.0999,  ...,  0.0027,  0.0264, -0.0359],\n",
       "                       [ 0.0064,  0.0506,  0.0421,  ..., -0.0400, -0.0017, -0.0254],\n",
       "                       [-0.0497, -0.0307, -0.0591,  ...,  0.0737,  0.0476,  0.1432],\n",
       "                       ...,\n",
       "                       [ 0.0117,  0.0396,  0.0373,  ...,  0.0423, -0.0640,  0.0583],\n",
       "                       [-0.0217, -0.0125, -0.0392,  ..., -0.0333, -0.0191, -0.0177],\n",
       "                       [ 0.0382,  0.0093,  0.0171,  ..., -0.0318, -0.0215, -0.0467]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.6.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0239,  0.0212, -0.0213,  ..., -0.0137,  0.0750,  0.0509],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.input_layernorm.weight',\n",
       "               tensor([0.9302, 0.9277, 0.8525,  ..., 0.9756, 0.9067, 0.8804],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.input_layernorm.bias',\n",
       "               tensor([-0.0028, -0.0242,  0.0105,  ...,  0.0101, -0.0634, -0.0598],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.attention.query_key_value.weight',\n",
       "               tensor([[-0.0352, -0.0465, -0.0181,  ..., -0.0264, -0.0100, -0.0873],\n",
       "                       [ 0.1215,  0.0409,  0.0007,  ...,  0.0353, -0.0219, -0.0464],\n",
       "                       [ 0.0447,  0.0905, -0.0276,  ..., -0.0289, -0.0035, -0.0938],\n",
       "                       ...,\n",
       "                       [ 0.0124,  0.0367, -0.0446,  ...,  0.0435, -0.0145,  0.0042],\n",
       "                       [ 0.0385,  0.0140, -0.0077,  ..., -0.0895,  0.0282, -0.0395],\n",
       "                       [ 0.0952,  0.0109,  0.0021,  ...,  0.0424,  0.0009,  0.0418]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.attention.query_key_value.bias',\n",
       "               tensor([-0.0975, -0.0954, -0.0648,  ...,  0.0069, -0.0162, -0.0018],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.attention.dense.weight',\n",
       "               tensor([[-0.0413,  0.0202, -0.0623,  ...,  0.0004,  0.0394, -0.0179],\n",
       "                       [ 0.0242,  0.0757,  0.0416,  ..., -0.0218, -0.0063,  0.0009],\n",
       "                       [ 0.0826,  0.0444,  0.0078,  ..., -0.0182,  0.0375, -0.0655],\n",
       "                       ...,\n",
       "                       [-0.0167, -0.0601, -0.0419,  ..., -0.0169,  0.0268, -0.0173],\n",
       "                       [ 0.0352,  0.0126, -0.0122,  ..., -0.0275,  0.0213,  0.0080],\n",
       "                       [-0.0216,  0.0582, -0.0274,  ..., -0.0632, -0.0385,  0.0469]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.attention.dense.bias',\n",
       "               tensor([-0.0209,  0.0270, -0.0734,  ..., -0.0699,  0.0287, -0.0046],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.post_attention_layernorm.weight',\n",
       "               tensor([1.0146, 0.9727, 0.9551,  ..., 1.0596, 1.0127, 0.9224],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.post_attention_layernorm.bias',\n",
       "               tensor([-0.0452,  0.0221, -0.0710,  ..., -0.0182,  0.0726,  0.0841],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0984,  0.0456, -0.0461,  ...,  0.0165,  0.0241, -0.0159],\n",
       "                       [ 0.0431, -0.0446, -0.0128,  ..., -0.0255,  0.0469, -0.0299],\n",
       "                       [ 0.0233,  0.0123, -0.0120,  ...,  0.0252, -0.0069, -0.0848],\n",
       "                       ...,\n",
       "                       [ 0.0602, -0.0713,  0.0930,  ...,  0.0678,  0.0035, -0.0207],\n",
       "                       [-0.0110, -0.0883,  0.0188,  ...,  0.0281,  0.0144, -0.0009],\n",
       "                       [-0.0311, -0.0712,  0.0013,  ..., -0.0374, -0.0784, -0.0157]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0719, -0.0577, -0.0599,  ..., -0.0631, -0.0157, -0.0508],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0598,  0.0408,  0.0047,  ...,  0.0319, -0.0119,  0.0016],\n",
       "                       [ 0.0419, -0.0056,  0.0067,  ..., -0.0012,  0.0508, -0.0256],\n",
       "                       [-0.0105,  0.0072,  0.0972,  ...,  0.0384, -0.0184,  0.0138],\n",
       "                       ...,\n",
       "                       [-0.0171, -0.0880, -0.0246,  ...,  0.0494, -0.0352, -0.0088],\n",
       "                       [ 0.0359,  0.0212,  0.0253,  ..., -0.0105, -0.0029, -0.0528],\n",
       "                       [-0.0182, -0.0009,  0.0311,  ...,  0.0038, -0.0094,  0.0063]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.7.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0540, -0.0210, -0.0122,  ..., -0.0334,  0.0751,  0.0569],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.input_layernorm.weight',\n",
       "               tensor([0.9487, 0.9727, 0.9185,  ..., 1.0566, 0.9785, 0.9023],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.input_layernorm.bias',\n",
       "               tensor([-0.0072, -0.0208, -0.0092,  ..., -0.0047, -0.0635, -0.0439],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.attention.query_key_value.weight',\n",
       "               tensor([[-0.0163,  0.0374,  0.0751,  ..., -0.0071,  0.0519, -0.0638],\n",
       "                       [ 0.0179, -0.0707,  0.0226,  ..., -0.0114,  0.0289,  0.0133],\n",
       "                       [-0.0928,  0.0171,  0.0056,  ...,  0.0472,  0.0399, -0.0022],\n",
       "                       ...,\n",
       "                       [ 0.0007,  0.0240, -0.0433,  ...,  0.0254, -0.0914,  0.0120],\n",
       "                       [-0.0957, -0.0338,  0.0113,  ..., -0.1196, -0.0509,  0.0465],\n",
       "                       [ 0.0213, -0.0311, -0.0241,  ..., -0.0060, -0.0469,  0.0303]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.attention.query_key_value.bias',\n",
       "               tensor([ 0.0141,  0.0544, -0.0131,  ..., -0.0418, -0.0283, -0.0325],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.attention.dense.weight',\n",
       "               tensor([[-0.0283, -0.0635,  0.0355,  ...,  0.0177, -0.0308,  0.0078],\n",
       "                       [ 0.0167,  0.0685, -0.0743,  ...,  0.0035,  0.0062, -0.0180],\n",
       "                       [-0.0005, -0.0281, -0.0146,  ...,  0.0632,  0.0191, -0.0387],\n",
       "                       ...,\n",
       "                       [ 0.0606, -0.0113, -0.0107,  ...,  0.0535,  0.0171,  0.0343],\n",
       "                       [ 0.0073, -0.0273, -0.0067,  ..., -0.1057,  0.0583,  0.0452],\n",
       "                       [-0.0235,  0.0473,  0.0236,  ..., -0.0757, -0.0270,  0.0588]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.attention.dense.bias',\n",
       "               tensor([-3.5405e-05,  2.5909e-02, -3.8391e-02,  ...,  3.2673e-03,\n",
       "                       -3.0365e-02,  4.0680e-02], dtype=torch.float16)),\n",
       "              ('layers.8.post_attention_layernorm.weight',\n",
       "               tensor([0.9722, 0.9492, 0.9487,  ..., 1.0215, 1.0000, 0.9263],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.post_attention_layernorm.bias',\n",
       "               tensor([-0.0006, -0.0200, -0.0891,  ..., -0.0449,  0.0571,  0.0875],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0626, -0.0319,  0.0977,  ...,  0.0394, -0.0211, -0.0172],\n",
       "                       [ 0.0228,  0.0500, -0.0030,  ...,  0.0054, -0.0055, -0.0008],\n",
       "                       [ 0.0336,  0.1449, -0.0166,  ..., -0.0883, -0.0632,  0.0306],\n",
       "                       ...,\n",
       "                       [-0.0206, -0.0117, -0.0496,  ...,  0.0345,  0.0002, -0.0671],\n",
       "                       [-0.0686,  0.0365,  0.0449,  ...,  0.0614, -0.0201, -0.0485],\n",
       "                       [ 0.0064, -0.0296,  0.0667,  ...,  0.0895, -0.0619,  0.0077]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0500, -0.0209, -0.0488,  ..., -0.0600, -0.0296, -0.0509],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0454, -0.1176, -0.0043,  ...,  0.0145,  0.0753,  0.0146],\n",
       "                       [-0.0501, -0.0025,  0.0477,  ..., -0.0310,  0.0292,  0.0028],\n",
       "                       [-0.0067, -0.0665,  0.0144,  ..., -0.0555,  0.0008, -0.0194],\n",
       "                       ...,\n",
       "                       [-0.0032,  0.0045, -0.0385,  ..., -0.0341, -0.0470,  0.0703],\n",
       "                       [-0.0256,  0.0466, -0.0345,  ...,  0.0455, -0.0070,  0.0372],\n",
       "                       [-0.0094,  0.0073, -0.0206,  ...,  0.0170,  0.0640,  0.0432]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.8.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0516,  0.0022,  0.0153,  ..., -0.0105,  0.0803,  0.0320],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.input_layernorm.weight',\n",
       "               tensor([0.8965, 0.9653, 0.9189,  ..., 1.0156, 1.0205, 0.8647],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.input_layernorm.bias',\n",
       "               tensor([-0.0006, -0.0044,  0.0005,  ...,  0.0010, -0.0508, -0.0484],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.attention.query_key_value.weight',\n",
       "               tensor([[-0.0847, -0.0066,  0.0520,  ..., -0.0776, -0.1005, -0.0469],\n",
       "                       [ 0.0354,  0.0341,  0.0162,  ..., -0.0163, -0.0556,  0.0226],\n",
       "                       [-0.0066, -0.0536,  0.0115,  ..., -0.0269,  0.0194,  0.0297],\n",
       "                       ...,\n",
       "                       [-0.0030, -0.0213,  0.0648,  ..., -0.0457, -0.0259,  0.0468],\n",
       "                       [-0.0259,  0.0285, -0.0629,  ..., -0.0398, -0.0347,  0.0276],\n",
       "                       [-0.0084,  0.0314, -0.0503,  ...,  0.0287, -0.0124,  0.0455]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.attention.query_key_value.bias',\n",
       "               tensor([-0.0275, -0.0135, -0.0321,  ..., -0.0078, -0.0011,  0.0234],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.attention.dense.weight',\n",
       "               tensor([[ 0.0910,  0.0115, -0.0526,  ...,  0.0322,  0.0513, -0.0607],\n",
       "                       [-0.0374, -0.0345, -0.0041,  ...,  0.0436, -0.0729, -0.0463],\n",
       "                       [ 0.0260, -0.0526,  0.0594,  ...,  0.0349,  0.0470, -0.0361],\n",
       "                       ...,\n",
       "                       [-0.0369, -0.0486, -0.0399,  ...,  0.0106,  0.0271,  0.0572],\n",
       "                       [ 0.0048, -0.0160,  0.0202,  ...,  0.1256, -0.0265, -0.0361],\n",
       "                       [-0.0325, -0.0637,  0.0271,  ..., -0.0179, -0.0102, -0.0546]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.attention.dense.bias',\n",
       "               tensor([-0.0311, -0.0430,  0.0127,  ..., -0.0267,  0.0314,  0.0560],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.post_attention_layernorm.weight',\n",
       "               tensor([0.9839, 0.9360, 0.9487,  ..., 0.9678, 0.9814, 0.8926],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.post_attention_layernorm.bias',\n",
       "               tensor([-0.0255, -0.0082, -0.0486,  ..., -0.0760,  0.0454,  0.0906],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 3.9764e-02, -4.3335e-03, -1.0048e-02,  ..., -2.6062e-02,\n",
       "                         1.5656e-02, -4.1840e-02],\n",
       "                       [-6.9046e-03,  3.6713e-02,  3.4332e-02,  ...,  1.4160e-02,\n",
       "                         5.8716e-02, -7.0152e-03],\n",
       "                       [ 2.8168e-02,  9.1736e-02,  3.2379e-02,  ...,  1.4626e-02,\n",
       "                        -2.3651e-02,  2.8515e-03],\n",
       "                       ...,\n",
       "                       [ 3.6713e-02, -1.4793e-02,  6.0852e-02,  ..., -2.7039e-02,\n",
       "                        -5.3772e-02,  1.1627e-02],\n",
       "                       [-3.2349e-02, -4.4373e-02, -4.7363e-02,  ...,  7.9956e-03,\n",
       "                         2.7863e-02,  4.2175e-02],\n",
       "                       [ 3.7018e-02, -2.0432e-02, -9.7513e-05,  ...,  2.0782e-02,\n",
       "                         1.5564e-03,  1.3626e-02]], dtype=torch.float16)),\n",
       "              ('layers.9.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0287, -0.0443, -0.0225,  ..., -0.0080, -0.0222, -0.0352],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0057, -0.0221, -0.0443,  ..., -0.0769,  0.0294, -0.0131],\n",
       "                       [ 0.0281, -0.0500, -0.0202,  ..., -0.0099,  0.0213, -0.0094],\n",
       "                       [ 0.0435,  0.0295, -0.0259,  ..., -0.0590,  0.0697,  0.0453],\n",
       "                       ...,\n",
       "                       [-0.0035,  0.0012, -0.0326,  ...,  0.0095, -0.0205,  0.0330],\n",
       "                       [-0.0422,  0.0044,  0.0459,  ...,  0.0723, -0.0717, -0.0172],\n",
       "                       [ 0.0267,  0.0126,  0.0041,  ..., -0.0006, -0.0278, -0.0159]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.9.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0909, -0.0309, -0.0119,  ..., -0.0321,  0.0485,  0.0631],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.input_layernorm.weight',\n",
       "               tensor([0.9937, 1.0166, 0.9624,  ..., 1.0537, 1.0547, 0.9321],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.input_layernorm.bias',\n",
       "               tensor([-0.0230, -0.0206,  0.0072,  ..., -0.0017, -0.0325, -0.0388],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0242,  0.0175, -0.0169,  ...,  0.0580, -0.0142,  0.0105],\n",
       "                       [ 0.0664,  0.0025, -0.0562,  ..., -0.0132, -0.0154,  0.1149],\n",
       "                       [-0.0300,  0.0021,  0.0349,  ..., -0.0131,  0.0656,  0.0734],\n",
       "                       ...,\n",
       "                       [ 0.0043, -0.0445, -0.0361,  ..., -0.0573, -0.0263,  0.0108],\n",
       "                       [ 0.0136, -0.0497, -0.0385,  ..., -0.0056, -0.0378,  0.0743],\n",
       "                       [-0.0258,  0.0502,  0.0540,  ..., -0.0003,  0.0625, -0.0005]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.attention.query_key_value.bias',\n",
       "               tensor([ 0.0351, -0.0403, -0.0908,  ...,  0.0063,  0.0062, -0.0420],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.attention.dense.weight',\n",
       "               tensor([[ 0.0260,  0.0430, -0.0026,  ..., -0.0503,  0.0268,  0.0709],\n",
       "                       [ 0.0437, -0.0535, -0.0114,  ...,  0.0071, -0.0492,  0.0868],\n",
       "                       [ 0.0498, -0.0468,  0.0200,  ..., -0.0235, -0.0244, -0.0258],\n",
       "                       ...,\n",
       "                       [-0.0217, -0.0030,  0.0421,  ..., -0.0661, -0.0329, -0.0379],\n",
       "                       [ 0.0059, -0.0676,  0.0321,  ..., -0.0224,  0.0533, -0.0621],\n",
       "                       [ 0.0855,  0.0111,  0.0174,  ...,  0.0046, -0.0368,  0.0277]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.attention.dense.bias',\n",
       "               tensor([-0.0186, -0.0118, -0.0187,  ..., -0.0526,  0.0474, -0.0264],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.post_attention_layernorm.weight',\n",
       "               tensor([0.9771, 0.9434, 0.9531,  ..., 0.9692, 0.9590, 0.8848],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.post_attention_layernorm.bias',\n",
       "               tensor([-0.0375, -0.0416, -0.0550,  ..., -0.0626,  0.0581,  0.0666],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0053,  0.0178,  0.0406,  ...,  0.0244, -0.0297, -0.0017],\n",
       "                       [-0.0605, -0.0144,  0.0164,  ...,  0.0168, -0.0740,  0.1011],\n",
       "                       [-0.0342,  0.0223, -0.0249,  ..., -0.0035, -0.0529, -0.0117],\n",
       "                       ...,\n",
       "                       [-0.0862,  0.0570,  0.0321,  ...,  0.0632,  0.0213, -0.0156],\n",
       "                       [-0.0061,  0.0818, -0.0119,  ..., -0.0203, -0.0199,  0.0207],\n",
       "                       [-0.0410, -0.0340,  0.1245,  ..., -0.0337, -0.0562, -0.0339]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0067, -0.0430, -0.0167,  ..., -0.0354, -0.0374, -0.0546],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0420, -0.0930,  0.0152,  ..., -0.0941,  0.0035, -0.0168],\n",
       "                       [-0.0129,  0.0649, -0.0378,  ...,  0.0652,  0.0720,  0.0844],\n",
       "                       [-0.0059,  0.0169,  0.0130,  ...,  0.0015,  0.0357, -0.0120],\n",
       "                       ...,\n",
       "                       [ 0.0219,  0.0732, -0.0241,  ...,  0.0534, -0.0297, -0.0393],\n",
       "                       [ 0.0375, -0.0367,  0.0135,  ...,  0.0136,  0.0052, -0.0066],\n",
       "                       [ 0.0178, -0.0418, -0.0028,  ...,  0.0682, -0.0011, -0.0333]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.10.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.1199, -0.0322, -0.0272,  ..., -0.0288,  0.0065,  0.0260],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.input_layernorm.weight',\n",
       "               tensor([0.9438, 1.0068, 0.9448,  ..., 1.0020, 1.0479, 0.8955],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.input_layernorm.bias',\n",
       "               tensor([ 0.0002, -0.0020,  0.0061,  ...,  0.0026, -0.0378, -0.0594],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0544, -0.0416, -0.0002,  ...,  0.0100, -0.0196,  0.0054],\n",
       "                       [ 0.0862, -0.0046, -0.0239,  ...,  0.0100,  0.0260,  0.0570],\n",
       "                       [-0.0085,  0.0715, -0.0063,  ..., -0.0049, -0.0092, -0.0112],\n",
       "                       ...,\n",
       "                       [-0.0258, -0.0439,  0.0320,  ..., -0.0635, -0.0311, -0.0054],\n",
       "                       [-0.0468,  0.0039,  0.0142,  ...,  0.0585, -0.0493, -0.0867],\n",
       "                       [-0.0688, -0.0276,  0.0352,  ...,  0.0594,  0.0365, -0.0969]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.attention.query_key_value.bias',\n",
       "               tensor([ 0.0591,  0.0915,  0.2551,  ..., -0.0048, -0.0153,  0.0040],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.attention.dense.weight',\n",
       "               tensor([[ 0.0685,  0.0308, -0.0513,  ...,  0.0307, -0.0074,  0.0394],\n",
       "                       [-0.0734,  0.0152, -0.0720,  ..., -0.0423, -0.0721,  0.0233],\n",
       "                       [-0.0122,  0.0302,  0.0157,  ..., -0.0112, -0.0398, -0.0394],\n",
       "                       ...,\n",
       "                       [ 0.0778,  0.0209,  0.0327,  ..., -0.0009, -0.0676,  0.0467],\n",
       "                       [-0.0469, -0.0847,  0.0073,  ...,  0.0266, -0.0858, -0.0820],\n",
       "                       [ 0.0732, -0.0137, -0.0220,  ..., -0.0047,  0.0095, -0.0447]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.attention.dense.bias',\n",
       "               tensor([-0.0890,  0.0220, -0.0073,  ..., -0.0622,  0.0471,  0.0070],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.post_attention_layernorm.weight',\n",
       "               tensor([0.9683, 0.9736, 0.9521,  ..., 0.9458, 0.9551, 0.8931],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.post_attention_layernorm.bias',\n",
       "               tensor([-0.0726, -0.0353, -0.0815,  ..., -0.0621,  0.0162,  0.0583],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0797,  0.0365,  0.0722,  ...,  0.0200, -0.0388, -0.0056],\n",
       "                       [ 0.0323,  0.0087,  0.0313,  ..., -0.0053, -0.0022, -0.0444],\n",
       "                       [-0.0084,  0.0180, -0.0661,  ...,  0.0223, -0.0450, -0.0704],\n",
       "                       ...,\n",
       "                       [ 0.1033, -0.0583, -0.0527,  ..., -0.0844,  0.0098, -0.0306],\n",
       "                       [ 0.0631, -0.0410,  0.0704,  ...,  0.0305,  0.0177,  0.0149],\n",
       "                       [ 0.0057, -0.1016, -0.0447,  ..., -0.0056, -0.0305, -0.0112]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0456, -0.0684, -0.0847,  ..., -0.0734, -0.0307, -0.0526],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0533, -0.0302,  0.0219,  ..., -0.0305, -0.0407,  0.0407],\n",
       "                       [-0.0562, -0.0170,  0.0681,  ...,  0.0472, -0.0068, -0.0481],\n",
       "                       [ 0.1246,  0.0505, -0.0802,  ...,  0.0464, -0.0316, -0.0570],\n",
       "                       ...,\n",
       "                       [-0.0184,  0.0235,  0.0224,  ...,  0.0626, -0.0044,  0.0032],\n",
       "                       [-0.0299, -0.0230,  0.0512,  ...,  0.0233,  0.0427, -0.0041],\n",
       "                       [-0.0455,  0.0505, -0.0504,  ...,  0.0040,  0.0255, -0.0051]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.11.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0613, -0.0269, -0.0410,  ..., -0.0391,  0.0250,  0.0125],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.input_layernorm.weight',\n",
       "               tensor([0.9912, 1.0312, 1.0049,  ..., 1.0332, 1.0625, 0.9443],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.input_layernorm.bias',\n",
       "               tensor([-0.0171, -0.0023, -0.0088,  ..., -0.0052, -0.0248, -0.0520],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0471, -0.0666, -0.0111,  ..., -0.0193,  0.1056, -0.0476],\n",
       "                       [ 0.0444, -0.0016,  0.0755,  ...,  0.0665,  0.0378,  0.0609],\n",
       "                       [ 0.0083,  0.0183, -0.0205,  ...,  0.0439,  0.0662, -0.0091],\n",
       "                       ...,\n",
       "                       [-0.0523, -0.0603, -0.1021,  ..., -0.1033,  0.0297,  0.0375],\n",
       "                       [-0.0085,  0.0300, -0.0292,  ...,  0.0018, -0.0117,  0.0258],\n",
       "                       [ 0.0177, -0.0435,  0.0605,  ...,  0.0766,  0.0020,  0.0400]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.attention.query_key_value.bias',\n",
       "               tensor([-0.1180,  0.0649, -0.0700,  ..., -0.0016,  0.0042,  0.0144],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.attention.dense.weight',\n",
       "               tensor([[-0.0917,  0.0687,  0.0077,  ..., -0.0183, -0.0377, -0.0582],\n",
       "                       [ 0.0573,  0.0217,  0.0583,  ..., -0.0026,  0.0365,  0.0229],\n",
       "                       [-0.1073,  0.0393, -0.0075,  ...,  0.1105, -0.0149, -0.0323],\n",
       "                       ...,\n",
       "                       [ 0.0681, -0.0283, -0.0369,  ...,  0.0948, -0.0512, -0.0283],\n",
       "                       [-0.0434, -0.0941,  0.1501,  ..., -0.0698, -0.0265,  0.0104],\n",
       "                       [-0.0397,  0.0046, -0.0044,  ...,  0.0040, -0.0223,  0.0559]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.attention.dense.bias',\n",
       "               tensor([-0.0394,  0.0492, -0.0449,  ..., -0.0236, -0.0504, -0.0137],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.post_attention_layernorm.weight',\n",
       "               tensor([1.0195, 0.9663, 0.9702,  ..., 0.9741, 1.0078, 0.9092],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.post_attention_layernorm.bias',\n",
       "               tensor([-0.0502, -0.0345, -0.0402,  ..., -0.0657,  0.0364, -0.0016],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0361,  0.0026,  0.0112,  ..., -0.0276,  0.0436, -0.0155],\n",
       "                       [-0.0457, -0.0583,  0.0346,  ...,  0.0503, -0.1226,  0.0186],\n",
       "                       [ 0.0334,  0.0154, -0.0438,  ..., -0.0110, -0.0146,  0.0519],\n",
       "                       ...,\n",
       "                       [ 0.0217,  0.0347, -0.0295,  ...,  0.1023, -0.0094,  0.0414],\n",
       "                       [ 0.0276,  0.0724, -0.0274,  ..., -0.0534, -0.0605,  0.0067],\n",
       "                       [ 0.0216,  0.0052, -0.0214,  ..., -0.0349, -0.0120,  0.0055]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0237, -0.0668, -0.0803,  ..., -0.0229, -0.0230, -0.0498],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0424, -0.0756, -0.0364,  ..., -0.0533, -0.0921,  0.0209],\n",
       "                       [ 0.0117, -0.0283,  0.0568,  ..., -0.0352,  0.0084, -0.0455],\n",
       "                       [ 0.0003, -0.0056, -0.0266,  ..., -0.0037,  0.0436,  0.0042],\n",
       "                       ...,\n",
       "                       [ 0.0135,  0.0619,  0.0662,  ..., -0.0898, -0.0031,  0.0267],\n",
       "                       [-0.0039, -0.0838,  0.0035,  ...,  0.0213,  0.0630,  0.0583],\n",
       "                       [-0.0185,  0.0163, -0.0108,  ..., -0.0268, -0.0098, -0.1002]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.12.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0692, -0.0153, -0.0361,  ..., -0.0457,  0.0339,  0.0372],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.input_layernorm.weight',\n",
       "               tensor([1.0205, 1.0244, 1.0400,  ..., 1.0244, 1.1123, 0.9702],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.input_layernorm.bias',\n",
       "               tensor([-0.0247,  0.0006,  0.0019,  ...,  0.0126, -0.0339, -0.0559],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.attention.query_key_value.weight',\n",
       "               tensor([[-0.0566, -0.0728, -0.0953,  ..., -0.0676,  0.0119,  0.0652],\n",
       "                       [ 0.0013, -0.0065, -0.0003,  ..., -0.0862, -0.0460,  0.0110],\n",
       "                       [ 0.1027, -0.0143, -0.0125,  ...,  0.0379, -0.0060, -0.0197],\n",
       "                       ...,\n",
       "                       [ 0.0343, -0.0072,  0.0388,  ..., -0.0906,  0.0820, -0.0353],\n",
       "                       [ 0.0663, -0.0275,  0.0002,  ..., -0.0374, -0.1018,  0.0414],\n",
       "                       [ 0.0246,  0.0512,  0.0012,  ..., -0.0792,  0.1022, -0.0404]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.attention.query_key_value.bias',\n",
       "               tensor([ 0.0743,  0.1541, -0.0747,  ..., -0.0033, -0.0127,  0.0039],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.attention.dense.weight',\n",
       "               tensor([[-0.0598,  0.0114, -0.0454,  ..., -0.0060, -0.0043, -0.0390],\n",
       "                       [ 0.0475, -0.0134,  0.0448,  ...,  0.0568,  0.0698, -0.0115],\n",
       "                       [-0.0122,  0.0265, -0.0238,  ...,  0.0629, -0.0551,  0.0165],\n",
       "                       ...,\n",
       "                       [-0.0140, -0.0250,  0.0131,  ...,  0.0169, -0.0075, -0.0114],\n",
       "                       [-0.0027,  0.0349,  0.0021,  ..., -0.0706,  0.0553, -0.0378],\n",
       "                       [ 0.0062,  0.0334,  0.0258,  ...,  0.0161,  0.0419,  0.0014]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.attention.dense.bias',\n",
       "               tensor([-0.0428,  0.0135, -0.0342,  ..., -0.0420, -0.0377, -0.0180],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.post_attention_layernorm.weight',\n",
       "               tensor([1.0273, 1.0146, 1.0176,  ..., 1.0078, 1.0176, 0.9365],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.post_attention_layernorm.bias',\n",
       "               tensor([-0.0392,  0.0031, -0.0459,  ..., -0.0762,  0.0193,  0.0152],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0147,  0.0019, -0.0081,  ...,  0.0441,  0.0303, -0.0154],\n",
       "                       [-0.0333,  0.0698,  0.0456,  ..., -0.0196,  0.0191, -0.0580],\n",
       "                       [ 0.0896,  0.0368,  0.0322,  ..., -0.0007,  0.0328,  0.0067],\n",
       "                       ...,\n",
       "                       [ 0.0356,  0.0472,  0.0521,  ..., -0.0333,  0.0451, -0.0198],\n",
       "                       [ 0.0155, -0.0239,  0.0800,  ...,  0.0707,  0.0085, -0.0413],\n",
       "                       [ 0.0289,  0.0667, -0.0512,  ...,  0.0836,  0.0279, -0.0145]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0106, -0.0692, -0.0230,  ..., -0.0557, -0.0237, -0.0296],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0085,  0.0042, -0.0954,  ..., -0.0813, -0.0617, -0.0269],\n",
       "                       [ 0.0204, -0.0096, -0.0024,  ...,  0.0327, -0.0201,  0.0156],\n",
       "                       [ 0.0092,  0.0205, -0.0563,  ...,  0.0291, -0.0775,  0.0452],\n",
       "                       ...,\n",
       "                       [-0.0215, -0.0539,  0.0087,  ...,  0.0148, -0.0161, -0.0601],\n",
       "                       [-0.0181,  0.0144, -0.0592,  ..., -0.0121,  0.0026, -0.0687],\n",
       "                       [-0.0195, -0.0770, -0.0012,  ..., -0.0428, -0.0094, -0.0178]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.13.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0934, -0.0586, -0.0348,  ..., -0.0667, -0.0001,  0.0370],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.input_layernorm.weight',\n",
       "               tensor([1.0713, 1.0459, 1.0215,  ..., 1.0059, 1.0830, 1.0186],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.input_layernorm.bias',\n",
       "               tensor([-0.0204,  0.0049, -0.0024,  ...,  0.0161, -0.0370, -0.0492],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0051, -0.0206,  0.0259,  ...,  0.0272, -0.0173, -0.0631],\n",
       "                       [ 0.0173, -0.0513, -0.0315,  ...,  0.0197,  0.0343,  0.0430],\n",
       "                       [-0.0183,  0.0299, -0.0199,  ...,  0.0249,  0.0599, -0.0815],\n",
       "                       ...,\n",
       "                       [ 0.0298, -0.0971,  0.0510,  ...,  0.0074, -0.0762, -0.0745],\n",
       "                       [-0.0361, -0.0265,  0.0592,  ..., -0.0452, -0.0818, -0.0059],\n",
       "                       [ 0.0587,  0.0095,  0.0390,  ..., -0.1057,  0.0132,  0.0511]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.attention.query_key_value.bias',\n",
       "               tensor([ 0.0976,  0.0235, -0.0560,  ..., -0.0231,  0.0052, -0.0207],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.attention.dense.weight',\n",
       "               tensor([[-0.0124,  0.0020,  0.0385,  ...,  0.0169,  0.0004,  0.0168],\n",
       "                       [-0.0955, -0.0211, -0.0482,  ...,  0.0107, -0.0056, -0.0782],\n",
       "                       [-0.0468,  0.0232,  0.0699,  ...,  0.0104, -0.0034, -0.0485],\n",
       "                       ...,\n",
       "                       [ 0.0578, -0.0205,  0.0141,  ...,  0.0108,  0.0031, -0.0267],\n",
       "                       [-0.0120, -0.0746, -0.0233,  ...,  0.0533,  0.0173,  0.0382],\n",
       "                       [-0.0131,  0.0098,  0.0017,  ...,  0.0485,  0.0821, -0.0018]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.attention.dense.bias',\n",
       "               tensor([-0.0878,  0.0490,  0.0015,  ..., -0.0486,  0.0226, -0.0031],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.post_attention_layernorm.weight',\n",
       "               tensor([1.0518, 1.0176, 1.0146,  ..., 1.0156, 1.0039, 0.9644],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.post_attention_layernorm.bias',\n",
       "               tensor([-0.0735, -0.0398, -0.0345,  ..., -0.1042, -0.0102, -0.0307],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-3.7861e-03,  3.7422e-03,  3.7628e-02,  ...,  5.4352e-02,\n",
       "                         3.1021e-02,  5.3644e-05],\n",
       "                       [-3.7659e-02,  4.2053e-02, -4.8737e-02,  ..., -4.5776e-03,\n",
       "                         1.9775e-02,  2.9572e-02],\n",
       "                       [ 2.3041e-02, -2.0309e-02, -6.3843e-02,  ..., -8.8440e-02,\n",
       "                        -2.8057e-03,  3.0655e-02],\n",
       "                       ...,\n",
       "                       [-2.9175e-02, -2.8934e-03,  4.1580e-04,  ...,  3.6926e-02,\n",
       "                         7.7209e-02, -8.0444e-02],\n",
       "                       [ 2.6764e-02, -4.2725e-02, -3.0991e-02,  ..., -2.3087e-02,\n",
       "                        -3.5004e-02,  2.0401e-02],\n",
       "                       [-7.8003e-02,  2.0237e-03,  8.5938e-02,  ..., -3.6125e-03,\n",
       "                        -3.0785e-03,  2.9037e-02]], dtype=torch.float16)),\n",
       "              ('layers.14.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0281, -0.0276, -0.0401,  ..., -0.0145, -0.0654, -0.0029],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0436,  0.0212, -0.0488,  ...,  0.0431, -0.0858,  0.0052],\n",
       "                       [-0.0215, -0.0046, -0.0406,  ...,  0.0152, -0.0984,  0.0309],\n",
       "                       [-0.0102,  0.0157,  0.0267,  ..., -0.0272, -0.0465,  0.0190],\n",
       "                       ...,\n",
       "                       [ 0.0073,  0.0265,  0.0811,  ..., -0.0342,  0.0468,  0.0249],\n",
       "                       [-0.0101, -0.0594, -0.0505,  ..., -0.0668,  0.1100, -0.0116],\n",
       "                       [-0.0324, -0.0290, -0.0473,  ...,  0.0680,  0.0618, -0.0238]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.14.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0755, -0.0157, -0.0346,  ...,  0.0030, -0.0085,  0.0133],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.input_layernorm.weight',\n",
       "               tensor([1.0225, 1.0146, 1.0273,  ..., 1.0166, 1.0654, 1.0107],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.input_layernorm.bias',\n",
       "               tensor([-0.0172,  0.0242, -0.0198,  ...,  0.0084, -0.0355, -0.0608],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0313,  0.0051,  0.0570,  ..., -0.0812,  0.0169, -0.0801],\n",
       "                       [-0.0332, -0.0744, -0.0325,  ...,  0.0399, -0.0576,  0.0600],\n",
       "                       [ 0.0505,  0.0450,  0.0801,  ...,  0.0012, -0.0434, -0.0022],\n",
       "                       ...,\n",
       "                       [-0.0125,  0.0082,  0.0510,  ...,  0.0280, -0.0257, -0.0374],\n",
       "                       [-0.0598,  0.0376,  0.0161,  ..., -0.0632, -0.0328,  0.0360],\n",
       "                       [ 0.0175, -0.0316, -0.0111,  ..., -0.0384,  0.0218,  0.0292]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.attention.query_key_value.bias',\n",
       "               tensor([ 0.0064, -0.0645, -0.1272,  ..., -0.0086,  0.0116,  0.0003],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.attention.dense.weight',\n",
       "               tensor([[ 0.0676,  0.0474,  0.0456,  ...,  0.0117, -0.0132,  0.0107],\n",
       "                       [-0.0316,  0.0003, -0.0794,  ..., -0.0699,  0.0039, -0.0597],\n",
       "                       [ 0.0023,  0.0757, -0.0629,  ...,  0.0550,  0.0241, -0.0121],\n",
       "                       ...,\n",
       "                       [-0.0575, -0.0272, -0.0277,  ...,  0.0817,  0.0319,  0.0527],\n",
       "                       [-0.0646,  0.0280, -0.0524,  ...,  0.0286,  0.0259, -0.0112],\n",
       "                       [ 0.0088,  0.0427,  0.0051,  ...,  0.0287, -0.0238,  0.0630]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.attention.dense.bias',\n",
       "               tensor([-0.0169, -0.0023, -0.0199,  ...,  0.0176, -0.0154,  0.0088],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.post_attention_layernorm.weight',\n",
       "               tensor([1.0977, 0.9844, 1.0615,  ..., 1.0420, 1.0723, 1.0137],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.post_attention_layernorm.bias',\n",
       "               tensor([-0.0507,  0.0016, -0.0292,  ..., -0.1093,  0.0188, -0.0244],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0177, -0.0378,  0.0195,  ...,  0.0281,  0.0030, -0.0292],\n",
       "                       [ 0.0030,  0.0100, -0.0134,  ...,  0.0616, -0.0335, -0.0112],\n",
       "                       [ 0.0235,  0.0269,  0.0137,  ...,  0.0332, -0.0321,  0.0181],\n",
       "                       ...,\n",
       "                       [ 0.0051,  0.0249,  0.0588,  ..., -0.0488,  0.0088,  0.0528],\n",
       "                       [ 0.0015,  0.0259, -0.0328,  ...,  0.0250, -0.0173, -0.0065],\n",
       "                       [-0.0753,  0.0889,  0.0767,  ...,  0.0411,  0.0111, -0.0112]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.mlp.dense_h_to_4h.bias',\n",
       "               tensor([ 0.0085, -0.0427, -0.0452,  ..., -0.0154, -0.0159, -0.0544],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0048, -0.0037, -0.0526,  ...,  0.0335, -0.0186,  0.0059],\n",
       "                       [ 0.0059, -0.0101, -0.0531,  ...,  0.0136, -0.0142,  0.0080],\n",
       "                       [-0.0129, -0.0467, -0.0205,  ...,  0.0327,  0.0296, -0.0831],\n",
       "                       ...,\n",
       "                       [-0.0255,  0.0129,  0.0060,  ...,  0.0481, -0.0083, -0.0209],\n",
       "                       [ 0.0097,  0.0340, -0.0092,  ..., -0.0234, -0.0206,  0.0692],\n",
       "                       [ 0.0038,  0.0480, -0.0502,  ...,  0.0114, -0.0320, -0.1501]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.15.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0412, -0.0275, -0.0269,  ..., -0.0551, -0.0167, -0.0178],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.input_layernorm.weight',\n",
       "               tensor([1.0186, 1.0342, 1.0400,  ..., 0.9961, 1.0723, 1.0068],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.input_layernorm.bias',\n",
       "               tensor([-0.0150,  0.0294, -0.0348,  ...,  0.0250, -0.0442, -0.0841],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0250, -0.0215,  0.0378,  ..., -0.0206,  0.0195, -0.0516],\n",
       "                       [-0.0114, -0.0718, -0.0456,  ..., -0.0179, -0.0755,  0.0063],\n",
       "                       [ 0.0173, -0.0314,  0.0322,  ..., -0.0546,  0.1193, -0.0493],\n",
       "                       ...,\n",
       "                       [ 0.0043,  0.0507,  0.0252,  ..., -0.1134,  0.0161,  0.0542],\n",
       "                       [-0.1343, -0.0153,  0.0341,  ...,  0.0290, -0.1048,  0.0461],\n",
       "                       [-0.0196, -0.0551, -0.0086,  ...,  0.0475,  0.0273, -0.0693]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.attention.query_key_value.bias',\n",
       "               tensor([-0.0238,  0.0233, -0.0238,  ..., -0.0176,  0.0287, -0.0122],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.attention.dense.weight',\n",
       "               tensor([[ 0.0149,  0.0009,  0.0290,  ..., -0.1054,  0.0399, -0.1172],\n",
       "                       [-0.0397, -0.0522,  0.0587,  ...,  0.0061,  0.0606,  0.0086],\n",
       "                       [-0.0010, -0.0363, -0.0711,  ...,  0.0335, -0.0251, -0.0052],\n",
       "                       ...,\n",
       "                       [ 0.0490,  0.0499,  0.0100,  ..., -0.0104, -0.0205,  0.0113],\n",
       "                       [-0.0478, -0.0612, -0.0393,  ..., -0.0169, -0.0071, -0.0895],\n",
       "                       [-0.0241, -0.0422, -0.0280,  ...,  0.0063,  0.0608,  0.0278]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.attention.dense.bias',\n",
       "               tensor([-0.0842, -0.0022, -0.0246,  ..., -0.0371, -0.0391, -0.0749],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.post_attention_layernorm.weight',\n",
       "               tensor([1.1377, 1.0127, 1.0635,  ..., 1.0811, 1.0693, 1.0244],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.post_attention_layernorm.bias',\n",
       "               tensor([-0.0663, -0.0069, -0.0355,  ..., -0.0956, -0.0014, -0.0524],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 6.0730e-03,  4.4128e-02, -9.3155e-03,  ..., -5.7861e-02,\n",
       "                         3.1525e-02,  6.3248e-03],\n",
       "                       [-3.5583e-02,  4.0344e-02, -2.0157e-02,  ..., -7.3731e-05,\n",
       "                        -3.7527e-04, -6.3171e-02],\n",
       "                       [ 5.0171e-02,  6.2408e-02, -3.8643e-03,  ..., -1.0513e-02,\n",
       "                        -4.2114e-02, -2.1667e-02],\n",
       "                       ...,\n",
       "                       [-6.7825e-03,  1.0742e-02, -1.4877e-02,  ..., -2.6169e-02,\n",
       "                        -9.1492e-02,  3.7598e-02],\n",
       "                       [-4.0710e-02, -3.2074e-02, -4.3716e-03,  ..., -4.1229e-02,\n",
       "                         4.6112e-02, -6.2447e-03],\n",
       "                       [-3.4607e-02,  2.9022e-02, -1.4854e-02,  ..., -6.2561e-02,\n",
       "                         3.5797e-02, -5.7373e-03]], dtype=torch.float16)),\n",
       "              ('layers.16.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0642, -0.0116, -0.0364,  ..., -0.0633, -0.0136, -0.0363],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0288,  0.0102, -0.0770,  ..., -0.0124,  0.0424,  0.0120],\n",
       "                       [-0.0381, -0.0281, -0.0442,  ..., -0.0048,  0.0438, -0.0340],\n",
       "                       [ 0.0635,  0.0278,  0.0243,  ...,  0.0042,  0.0340, -0.0432],\n",
       "                       ...,\n",
       "                       [ 0.0912,  0.0038,  0.0045,  ..., -0.0523, -0.0145,  0.0048],\n",
       "                       [-0.0581, -0.0128,  0.0339,  ..., -0.0101, -0.0529,  0.0069],\n",
       "                       [ 0.0224,  0.0748, -0.0158,  ...,  0.0164, -0.0290,  0.0118]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.16.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0674, -0.0421, -0.0408,  ..., -0.0258,  0.0052, -0.0191],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.input_layernorm.weight',\n",
       "               tensor([1.0615, 1.0156, 1.0322,  ..., 0.9897, 1.0430, 1.0430],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.input_layernorm.bias',\n",
       "               tensor([-0.0224, -0.0009, -0.0264,  ...,  0.0408, -0.0495, -0.1024],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0569, -0.0064, -0.0089,  ...,  0.0462,  0.0045, -0.0173],\n",
       "                       [-0.0164,  0.0258, -0.0011,  ...,  0.0361,  0.0252, -0.0547],\n",
       "                       [ 0.0145, -0.0508, -0.0808,  ..., -0.0619,  0.0448,  0.0037],\n",
       "                       ...,\n",
       "                       [ 0.0030, -0.0162, -0.0381,  ...,  0.0075, -0.0011, -0.0076],\n",
       "                       [ 0.0443, -0.0020, -0.0389,  ...,  0.0145,  0.0175,  0.0616],\n",
       "                       [-0.0275,  0.0364, -0.0818,  ...,  0.0166, -0.0390,  0.0432]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.attention.query_key_value.bias',\n",
       "               tensor([-0.1698,  0.0944,  0.1986,  ...,  0.0102,  0.0323, -0.0179],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.attention.dense.weight',\n",
       "               tensor([[-0.0445,  0.0311,  0.0372,  ..., -0.0704,  0.0842,  0.0036],\n",
       "                       [-0.0090,  0.0472, -0.0644,  ...,  0.0062,  0.0120,  0.0014],\n",
       "                       [-0.0259,  0.0961, -0.0237,  ...,  0.0396, -0.0254, -0.0262],\n",
       "                       ...,\n",
       "                       [ 0.0019,  0.0278, -0.0865,  ...,  0.0355, -0.0164,  0.0739],\n",
       "                       [ 0.0043,  0.0089, -0.0327,  ..., -0.0461,  0.0019,  0.0332],\n",
       "                       [ 0.0214, -0.0074, -0.0146,  ..., -0.0290,  0.0359,  0.0352]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.attention.dense.bias',\n",
       "               tensor([-0.0781,  0.0260, -0.0640,  ..., -0.1004, -0.0311, -0.0303],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.post_attention_layernorm.weight',\n",
       "               tensor([1.1514, 1.0303, 1.0811,  ..., 1.1035, 1.0664, 1.0752],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.post_attention_layernorm.bias',\n",
       "               tensor([-0.0895, -0.0154, -0.0070,  ..., -0.0878, -0.0082, -0.0636],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0105,  0.0714,  0.0290,  ..., -0.0200, -0.0404, -0.0546],\n",
       "                       [ 0.0181, -0.0951,  0.0279,  ...,  0.0009,  0.0450,  0.0102],\n",
       "                       [ 0.0627,  0.0575,  0.0883,  ...,  0.0391,  0.0291, -0.0619],\n",
       "                       ...,\n",
       "                       [-0.0549, -0.0558,  0.0870,  ...,  0.0303,  0.0143,  0.0193],\n",
       "                       [-0.0277,  0.0851,  0.0755,  ..., -0.0154,  0.0581,  0.0430],\n",
       "                       [-0.0401,  0.0198, -0.0871,  ..., -0.0210, -0.0365,  0.0125]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.1192, -0.0745, -0.0433,  ..., -0.0315, -0.0507, -0.0255],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.1172, -0.0486, -0.0456,  ..., -0.0596, -0.0026, -0.0080],\n",
       "                       [ 0.0710, -0.0920, -0.0377,  ...,  0.0101, -0.0259,  0.0044],\n",
       "                       [-0.0387, -0.0797, -0.0677,  ...,  0.0155,  0.0400,  0.0041],\n",
       "                       ...,\n",
       "                       [-0.0375,  0.0742, -0.0542,  ..., -0.0725, -0.0314,  0.0129],\n",
       "                       [-0.0119, -0.0315, -0.0128,  ...,  0.0593, -0.0136,  0.0202],\n",
       "                       [-0.0688,  0.0183,  0.0349,  ..., -0.0068,  0.0098, -0.0762]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.17.mlp.dense_4h_to_h.bias',\n",
       "               tensor([-0.0038, -0.0127,  0.0141,  ..., -0.0247,  0.0131, -0.0244],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.input_layernorm.weight',\n",
       "               tensor([1.0947, 1.0088, 1.0361,  ..., 1.0020, 1.0713, 1.0879],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.input_layernorm.bias',\n",
       "               tensor([-0.0182,  0.0065, -0.0371,  ...,  0.0399, -0.0373, -0.0998],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0352, -0.0478,  0.0561,  ...,  0.0229, -0.0019, -0.0011],\n",
       "                       [ 0.0260, -0.0165,  0.0361,  ..., -0.0360, -0.0264, -0.0797],\n",
       "                       [-0.0124,  0.0031,  0.0337,  ...,  0.0120, -0.0153,  0.0434],\n",
       "                       ...,\n",
       "                       [-0.0044, -0.0015, -0.0203,  ..., -0.0924,  0.0231, -0.1011],\n",
       "                       [-0.0036,  0.0291, -0.0548,  ..., -0.0275, -0.1152, -0.0059],\n",
       "                       [ 0.0721,  0.0135,  0.1613,  ..., -0.0052,  0.0589, -0.0396]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.attention.query_key_value.bias',\n",
       "               tensor([ 0.0770,  0.0536, -0.0306,  ..., -0.0007, -0.0146,  0.0140],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.attention.dense.weight',\n",
       "               tensor([[-0.0228,  0.0073, -0.0253,  ...,  0.0224, -0.0306,  0.0527],\n",
       "                       [-0.0856, -0.0271, -0.0013,  ..., -0.0070,  0.0124, -0.0136],\n",
       "                       [ 0.0076, -0.1218,  0.0234,  ...,  0.0509, -0.0355,  0.1074],\n",
       "                       ...,\n",
       "                       [-0.0152,  0.0228,  0.0178,  ...,  0.0164, -0.0291, -0.0239],\n",
       "                       [-0.0140,  0.0320,  0.0425,  ...,  0.0371, -0.0634,  0.0577],\n",
       "                       [-0.1295, -0.0308, -0.0540,  ..., -0.0616, -0.0393,  0.0755]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.attention.dense.bias',\n",
       "               tensor([-0.0610,  0.0732, -0.0206,  ..., -0.0680,  0.0081,  0.0320],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.post_attention_layernorm.weight',\n",
       "               tensor([1.1729, 1.0244, 1.1016,  ..., 1.1250, 1.1260, 1.0889],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.post_attention_layernorm.bias',\n",
       "               tensor([-0.1586, -0.0110, -0.0166,  ..., -0.0433, -0.0228, -0.0476],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0041, -0.0179,  0.0376,  ...,  0.0334, -0.0399,  0.0080],\n",
       "                       [ 0.0918, -0.0583, -0.0150,  ..., -0.0522, -0.0224, -0.0018],\n",
       "                       [ 0.0307,  0.0347,  0.0579,  ...,  0.0203, -0.0006,  0.0544],\n",
       "                       ...,\n",
       "                       [ 0.0428,  0.0682,  0.0007,  ...,  0.0710, -0.0456,  0.0129],\n",
       "                       [-0.0836, -0.0014,  0.0048,  ..., -0.0446, -0.0186,  0.0638],\n",
       "                       [ 0.0213,  0.0233,  0.0050,  ...,  0.0209,  0.0272, -0.0075]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0591, -0.0138, -0.0337,  ..., -0.0825, -0.0349, -0.0253],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0494, -0.0625, -0.0414,  ..., -0.1017,  0.0467, -0.0444],\n",
       "                       [-0.0549,  0.0461, -0.0097,  ...,  0.0560,  0.0803, -0.0267],\n",
       "                       [-0.0447, -0.0228,  0.0121,  ...,  0.0423,  0.0607,  0.0206],\n",
       "                       ...,\n",
       "                       [ 0.0171,  0.0338,  0.0247,  ...,  0.0980,  0.0453, -0.0483],\n",
       "                       [-0.0335,  0.0542,  0.0671,  ...,  0.0585,  0.0693, -0.0586],\n",
       "                       [ 0.0030, -0.0212, -0.0524,  ...,  0.0058,  0.0163, -0.0582]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.18.mlp.dense_4h_to_h.bias',\n",
       "               tensor([ 0.0343, -0.0679,  0.0117,  ..., -0.0518,  0.0174, -0.0071],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.input_layernorm.weight',\n",
       "               tensor([1.0361, 0.9854, 1.0088,  ..., 0.9849, 1.0303, 1.0186],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.input_layernorm.bias',\n",
       "               tensor([-0.0556, -0.0008, -0.0503,  ...,  0.0469, -0.0481, -0.0834],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.attention.query_key_value.weight',\n",
       "               tensor([[-0.0128, -0.0154,  0.0091,  ...,  0.0815, -0.0312, -0.0242],\n",
       "                       [-0.0791,  0.0147, -0.0120,  ..., -0.0302, -0.0311,  0.0144],\n",
       "                       [ 0.0522, -0.0254, -0.0649,  ..., -0.0123, -0.0920, -0.0441],\n",
       "                       ...,\n",
       "                       [-0.1215,  0.0280, -0.0459,  ...,  0.0230,  0.0450,  0.0363],\n",
       "                       [-0.0190,  0.0874, -0.0537,  ...,  0.0289, -0.0881,  0.0728],\n",
       "                       [ 0.0017,  0.0233,  0.0085,  ..., -0.0462, -0.0423, -0.0674]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.attention.query_key_value.bias',\n",
       "               tensor([-0.0088,  0.1082, -0.0336,  ...,  0.0104,  0.0027, -0.0306],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.attention.dense.weight',\n",
       "               tensor([[-0.0437, -0.0431, -0.0051,  ..., -0.0522, -0.0129,  0.0136],\n",
       "                       [-0.0540,  0.0693, -0.0166,  ..., -0.0049,  0.0367,  0.0690],\n",
       "                       [ 0.0134,  0.0036,  0.0301,  ..., -0.0732,  0.0137,  0.0404],\n",
       "                       ...,\n",
       "                       [ 0.0668, -0.0335,  0.0255,  ...,  0.0281,  0.0222,  0.0474],\n",
       "                       [-0.0203, -0.0854, -0.0145,  ..., -0.0262, -0.0591,  0.0167],\n",
       "                       [ 0.0197, -0.0374, -0.0390,  ..., -0.0349,  0.0372, -0.0971]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.attention.dense.bias',\n",
       "               tensor([-0.0202,  0.0052,  0.0576,  ..., -0.0405, -0.0211,  0.0033],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.post_attention_layernorm.weight',\n",
       "               tensor([1.1855, 1.0400, 1.0957,  ..., 1.1475, 1.1816, 1.1387],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.post_attention_layernorm.bias',\n",
       "               tensor([-0.1345,  0.0180, -0.0280,  ..., -0.0555, -0.0021, -0.0164],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0389, -0.0126, -0.0775,  ...,  0.0208, -0.0193, -0.0214],\n",
       "                       [-0.0227, -0.0652, -0.0445,  ...,  0.0119, -0.0427,  0.0108],\n",
       "                       [-0.0757, -0.0611, -0.0482,  ...,  0.0540, -0.0125, -0.0652],\n",
       "                       ...,\n",
       "                       [ 0.0482, -0.0488, -0.0699,  ..., -0.0342,  0.0644, -0.0426],\n",
       "                       [ 0.0274,  0.0108, -0.0765,  ..., -0.0106,  0.0352,  0.0198],\n",
       "                       [-0.0285,  0.0151,  0.0731,  ...,  0.0279, -0.0366,  0.0595]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0314, -0.0213, -0.0149,  ..., -0.0235, -0.0295, -0.0479],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0296,  0.0174,  0.0649,  ...,  0.0056,  0.0299, -0.0945],\n",
       "                       [ 0.0301,  0.0792,  0.0740,  ...,  0.0661, -0.0068,  0.0098],\n",
       "                       [ 0.0636,  0.0073, -0.0171,  ...,  0.0423,  0.0844,  0.0701],\n",
       "                       ...,\n",
       "                       [-0.0096, -0.0208, -0.0906,  ...,  0.0882, -0.0593,  0.0009],\n",
       "                       [ 0.0204, -0.0064,  0.0490,  ..., -0.0240,  0.0022, -0.0430],\n",
       "                       [-0.0671, -0.0701, -0.0275,  ..., -0.0058,  0.0001, -0.0231]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.19.mlp.dense_4h_to_h.bias',\n",
       "               tensor([ 0.0732, -0.0739,  0.0179,  ..., -0.0396,  0.0268, -0.0450],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.input_layernorm.weight',\n",
       "               tensor([1.0186, 1.0234, 1.0068,  ..., 0.9956, 1.0137, 1.0654],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.input_layernorm.bias',\n",
       "               tensor([-0.0731,  0.0060, -0.0681,  ...,  0.0529, -0.0368, -0.0620],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.attention.query_key_value.weight',\n",
       "               tensor([[-0.0241,  0.0059, -0.0515,  ..., -0.0556,  0.0355,  0.0310],\n",
       "                       [-0.0033,  0.0703, -0.0616,  ...,  0.0014, -0.0083, -0.0058],\n",
       "                       [ 0.0685,  0.0361,  0.0384,  ...,  0.0062,  0.0095,  0.0485],\n",
       "                       ...,\n",
       "                       [ 0.0771, -0.0260, -0.0060,  ..., -0.0321, -0.0098, -0.0124],\n",
       "                       [ 0.0114,  0.0441,  0.0292,  ..., -0.0209, -0.0231,  0.1324],\n",
       "                       [-0.0485,  0.0307,  0.0113,  ..., -0.0594,  0.0225,  0.0459]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.attention.query_key_value.bias',\n",
       "               tensor([ 0.1937,  0.0557, -0.0164,  ..., -0.0008,  0.0086, -0.0094],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.attention.dense.weight',\n",
       "               tensor([[-0.0185,  0.0332,  0.0182,  ..., -0.0186, -0.0586, -0.0046],\n",
       "                       [ 0.0481, -0.0703, -0.0727,  ..., -0.0837, -0.0158,  0.0711],\n",
       "                       [-0.0413,  0.0051,  0.0563,  ..., -0.0802,  0.0229,  0.0470],\n",
       "                       ...,\n",
       "                       [-0.0381, -0.0657, -0.0384,  ...,  0.0158,  0.0772, -0.0332],\n",
       "                       [ 0.0293, -0.0816,  0.0090,  ...,  0.0131, -0.0158,  0.0114],\n",
       "                       [ 0.0722, -0.0512,  0.0101,  ..., -0.0103,  0.0807,  0.1249]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.attention.dense.bias',\n",
       "               tensor([-0.0551,  0.0800,  0.0058,  ..., -0.1066, -0.0621,  0.0108],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.post_attention_layernorm.weight',\n",
       "               tensor([1.1895, 1.0508, 1.1279,  ..., 1.1729, 1.1982, 1.1396],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.post_attention_layernorm.bias',\n",
       "               tensor([-0.1487,  0.0461, -0.0009,  ..., -0.0551, -0.0228, -0.0177],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[ 0.0200, -0.0306,  0.0249,  ...,  0.0102, -0.0473,  0.0144],\n",
       "                       [ 0.0317,  0.0632, -0.0724,  ..., -0.0130,  0.0533,  0.0044],\n",
       "                       [ 0.0062, -0.0442,  0.0346,  ..., -0.0055, -0.0593, -0.0619],\n",
       "                       ...,\n",
       "                       [ 0.0344, -0.0576, -0.0491,  ...,  0.0391,  0.0488, -0.0745],\n",
       "                       [-0.0199, -0.0294,  0.0636,  ...,  0.0144,  0.0682, -0.0545],\n",
       "                       [-0.0411, -0.0006, -0.0224,  ..., -0.0107,  0.0664, -0.0155]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0126, -0.0255, -0.0128,  ..., -0.0253, -0.0219, -0.0237],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0880, -0.0828, -0.0291,  ...,  0.0558,  0.0961, -0.0231],\n",
       "                       [-0.0305,  0.0712, -0.0064,  ...,  0.0217,  0.0423,  0.0067],\n",
       "                       [-0.0840, -0.0048, -0.1270,  ..., -0.0336, -0.0209,  0.0544],\n",
       "                       ...,\n",
       "                       [-0.0149, -0.0124, -0.0337,  ..., -0.0783,  0.0664, -0.0551],\n",
       "                       [ 0.0239,  0.0140,  0.0335,  ..., -0.0906,  0.0547, -0.0426],\n",
       "                       [-0.0814,  0.0396,  0.0590,  ...,  0.0406,  0.0228,  0.0219]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.20.mlp.dense_4h_to_h.bias',\n",
       "               tensor([ 0.0346, -0.0269,  0.0037,  ..., -0.0156,  0.0188, -0.0263],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.input_layernorm.weight',\n",
       "               tensor([0.9478, 0.9951, 1.0029,  ..., 0.9521, 0.9668, 1.0020],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.input_layernorm.bias',\n",
       "               tensor([-0.0621,  0.0297, -0.0615,  ...,  0.0823, -0.0381, -0.0666],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0435,  0.0123, -0.0406,  ...,  0.0537, -0.0044,  0.0108],\n",
       "                       [ 0.0273, -0.0703, -0.0122,  ...,  0.0096,  0.0408, -0.0601],\n",
       "                       [ 0.0790, -0.0185,  0.0300,  ..., -0.0366,  0.0088,  0.0185],\n",
       "                       ...,\n",
       "                       [-0.0890, -0.0087, -0.0667,  ...,  0.0110, -0.0165,  0.0650],\n",
       "                       [ 0.0229, -0.0656, -0.0777,  ..., -0.1316,  0.0459, -0.0476],\n",
       "                       [ 0.0458, -0.0142,  0.0547,  ..., -0.0428,  0.0858,  0.0086]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.attention.query_key_value.bias',\n",
       "               tensor([ 0.0332,  0.0391,  0.0632,  ..., -0.0129, -0.0088, -0.0218],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.attention.dense.weight',\n",
       "               tensor([[-0.0273,  0.0346, -0.0767,  ..., -0.0380,  0.0231, -0.0072],\n",
       "                       [-0.0379, -0.0478, -0.0152,  ...,  0.0409, -0.0236,  0.0135],\n",
       "                       [ 0.1462, -0.0326, -0.0182,  ..., -0.0703,  0.0215,  0.0137],\n",
       "                       ...,\n",
       "                       [-0.0323,  0.0363,  0.0640,  ..., -0.0060, -0.0455, -0.0356],\n",
       "                       [ 0.0068, -0.0199, -0.0624,  ...,  0.0019,  0.0309,  0.0256],\n",
       "                       [-0.0289, -0.0091,  0.0458,  ..., -0.0064, -0.1350,  0.0869]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.attention.dense.bias',\n",
       "               tensor([-0.0188,  0.0166,  0.0410,  ..., -0.0308,  0.0392, -0.0030],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.post_attention_layernorm.weight',\n",
       "               tensor([1.1807, 1.0996, 1.1445,  ..., 1.1582, 1.1855, 1.1436],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.post_attention_layernorm.bias',\n",
       "               tensor([-0.0871,  0.0556,  0.0261,  ..., -0.0293, -0.0305, -0.0431],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0428,  0.0147,  0.0654,  ...,  0.0108, -0.0042,  0.0192],\n",
       "                       [-0.0365,  0.0153, -0.0196,  ...,  0.0150, -0.0363, -0.0643],\n",
       "                       [ 0.0505, -0.0181,  0.0037,  ..., -0.0144,  0.0354, -0.0069],\n",
       "                       ...,\n",
       "                       [ 0.0246,  0.0150, -0.0631,  ...,  0.0479, -0.0057, -0.0200],\n",
       "                       [-0.0243,  0.0625,  0.0442,  ..., -0.0318,  0.0787, -0.1027],\n",
       "                       [ 0.0102,  0.0084,  0.0952,  ...,  0.0599,  0.0286, -0.0307]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0400, -0.0674, -0.0410,  ..., -0.0233, -0.0186, -0.0518],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0100,  0.0671, -0.0762,  ..., -0.0194, -0.0042,  0.0008],\n",
       "                       [-0.0695,  0.0170, -0.0096,  ..., -0.0169,  0.0021, -0.0481],\n",
       "                       [ 0.0391,  0.0482, -0.1050,  ...,  0.0568, -0.0021, -0.0051],\n",
       "                       ...,\n",
       "                       [ 0.0368,  0.0264, -0.0749,  ..., -0.0249, -0.0501, -0.0519],\n",
       "                       [ 0.0289,  0.0966, -0.0928,  ...,  0.0169, -0.0817, -0.0151],\n",
       "                       [-0.0016,  0.0551,  0.0211,  ..., -0.0461, -0.0677, -0.0648]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.21.mlp.dense_4h_to_h.bias',\n",
       "               tensor([ 0.0550, -0.0344, -0.0588,  ...,  0.0290,  0.0219,  0.0191],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.input_layernorm.weight',\n",
       "               tensor([0.9473, 0.9961, 1.0176,  ..., 0.9473, 0.9751, 0.9702],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.input_layernorm.bias',\n",
       "               tensor([-0.0680,  0.0267, -0.0450,  ...,  0.1122, -0.0683, -0.0941],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.attention.query_key_value.weight',\n",
       "               tensor([[ 0.0192, -0.0054,  0.0169,  ..., -0.0052,  0.0151, -0.0529],\n",
       "                       [-0.0632,  0.0200, -0.0257,  ..., -0.0123,  0.0439, -0.0551],\n",
       "                       [ 0.0132,  0.0250,  0.1449,  ..., -0.0363, -0.0345, -0.0159],\n",
       "                       ...,\n",
       "                       [-0.0255, -0.0120, -0.0607,  ..., -0.0016,  0.0640, -0.0186],\n",
       "                       [ 0.0558,  0.0275, -0.0239,  ..., -0.0346, -0.0393, -0.0387],\n",
       "                       [-0.1108,  0.0757,  0.0337,  ..., -0.0135,  0.0651, -0.0347]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.attention.query_key_value.bias',\n",
       "               tensor([-0.0538, -0.0047, -0.0877,  ...,  0.0200,  0.0113,  0.0174],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.attention.dense.weight',\n",
       "               tensor([[ 0.0479, -0.0487,  0.0496,  ...,  0.0483,  0.0341,  0.0427],\n",
       "                       [ 0.0333,  0.0110, -0.0493,  ...,  0.0182, -0.0595, -0.0124],\n",
       "                       [-0.0496, -0.0246, -0.0429,  ...,  0.0200, -0.0286,  0.0166],\n",
       "                       ...,\n",
       "                       [-0.0839, -0.0504,  0.0016,  ...,  0.0717, -0.0375, -0.0501],\n",
       "                       [-0.0307, -0.0486, -0.0125,  ..., -0.0050, -0.0208,  0.0667],\n",
       "                       [-0.0683, -0.0489, -0.0072,  ..., -0.0110,  0.0064,  0.0721]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.attention.dense.bias',\n",
       "               tensor([ 0.0902, -0.0601, -0.0295,  ..., -0.0741, -0.0113,  0.0140],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.post_attention_layernorm.weight',\n",
       "               tensor([1.1582, 1.0605, 1.1270,  ..., 1.1523, 1.1895, 1.1650],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.post_attention_layernorm.bias',\n",
       "               tensor([-0.0895,  0.0557,  0.0380,  ..., -0.0518,  0.0240, -0.0278],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0107, -0.0339, -0.0385,  ..., -0.0779,  0.0267,  0.0009],\n",
       "                       [-0.0416,  0.0267, -0.0514,  ...,  0.0253,  0.0493, -0.0026],\n",
       "                       [-0.0054, -0.0469, -0.0181,  ..., -0.0534,  0.0602, -0.0761],\n",
       "                       ...,\n",
       "                       [ 0.1099,  0.0163,  0.0362,  ...,  0.0223, -0.0022, -0.0187],\n",
       "                       [ 0.0750,  0.0928, -0.0383,  ..., -0.0186, -0.0047,  0.0331],\n",
       "                       [ 0.0038,  0.0009,  0.0135,  ...,  0.0922,  0.0299,  0.0034]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0351, -0.0308, -0.0357,  ..., -0.1113, -0.0525, -0.0325],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[ 0.0172, -0.0030,  0.0272,  ...,  0.0900, -0.0573,  0.1234],\n",
       "                       [-0.0606, -0.0017, -0.0082,  ...,  0.0012, -0.0120,  0.0641],\n",
       "                       [ 0.0215, -0.0329, -0.0790,  ...,  0.1114,  0.0370,  0.0508],\n",
       "                       ...,\n",
       "                       [ 0.0039, -0.0645, -0.0305,  ..., -0.0508,  0.0999, -0.1036],\n",
       "                       [-0.0106,  0.0628, -0.0146,  ..., -0.0580,  0.0453,  0.0244],\n",
       "                       [ 0.0265,  0.0863,  0.0031,  ...,  0.2002, -0.0335,  0.0164]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.22.mlp.dense_4h_to_h.bias',\n",
       "               tensor([ 0.0269, -0.0267,  0.0428,  ...,  0.0419, -0.0491,  0.0174],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.input_layernorm.weight',\n",
       "               tensor([0.9907, 1.0547, 1.0127,  ..., 0.9907, 1.0156, 1.0391],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.input_layernorm.bias',\n",
       "               tensor([-0.0465,  0.0286, -0.0314,  ...,  0.0886, -0.0579, -0.1117],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.attention.query_key_value.weight',\n",
       "               tensor([[-0.0431, -0.0210,  0.0243,  ..., -0.0328, -0.0324, -0.0536],\n",
       "                       [ 0.0178,  0.0667,  0.0312,  ...,  0.0347, -0.0219, -0.0025],\n",
       "                       [ 0.0337,  0.0299,  0.0134,  ..., -0.0820,  0.0748,  0.0096],\n",
       "                       ...,\n",
       "                       [-0.0386, -0.0096,  0.0438,  ...,  0.0695,  0.0249,  0.0452],\n",
       "                       [-0.0037, -0.0575,  0.0206,  ...,  0.0336, -0.0958,  0.0006],\n",
       "                       [-0.0013,  0.0120, -0.0195,  ...,  0.0328,  0.0061, -0.0063]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.attention.query_key_value.bias',\n",
       "               tensor([ 0.0648, -0.0728,  0.0287,  ..., -0.0140, -0.0045,  0.0109],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.attention.dense.weight',\n",
       "               tensor([[ 0.0261,  0.0238,  0.0030,  ..., -0.0203,  0.0212,  0.0220],\n",
       "                       [ 0.0296,  0.0067,  0.0552,  ..., -0.0497, -0.0410, -0.0243],\n",
       "                       [ 0.0351,  0.0158, -0.0492,  ...,  0.0391,  0.0673,  0.0083],\n",
       "                       ...,\n",
       "                       [-0.0576, -0.1102, -0.0499,  ...,  0.0328,  0.0204,  0.0394],\n",
       "                       [-0.0326,  0.0010, -0.0461,  ...,  0.0187, -0.0317,  0.0144],\n",
       "                       [-0.0231, -0.0966, -0.0424,  ...,  0.0389,  0.0215, -0.0319]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.attention.dense.bias',\n",
       "               tensor([-0.0418,  0.1368,  0.0047,  ...,  0.0194, -0.0130,  0.0207],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.post_attention_layernorm.weight',\n",
       "               tensor([1.0645, 1.0303, 1.0459,  ..., 1.0674, 1.1035, 1.0371],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.post_attention_layernorm.bias',\n",
       "               tensor([-0.0243,  0.0511, -0.0053,  ..., -0.0599,  0.0246, -0.0594],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.mlp.dense_h_to_4h.weight',\n",
       "               tensor([[-0.0016,  0.0114, -0.0005,  ..., -0.0382, -0.0476, -0.0091],\n",
       "                       [-0.0215,  0.0596,  0.0332,  ...,  0.0599,  0.0287, -0.0231],\n",
       "                       [ 0.0450,  0.0209,  0.0468,  ...,  0.0852,  0.0125, -0.0653],\n",
       "                       ...,\n",
       "                       [-0.0732,  0.0718,  0.0896,  ...,  0.0180, -0.0150, -0.0002],\n",
       "                       [-0.0309, -0.0430, -0.0367,  ...,  0.1002,  0.0557, -0.1176],\n",
       "                       [ 0.0672, -0.0891,  0.0050,  ...,  0.0533,  0.0035,  0.0179]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.mlp.dense_h_to_4h.bias',\n",
       "               tensor([-0.0146, -0.0294,  0.0112,  ..., -0.0297, -0.0035, -0.0245],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.mlp.dense_4h_to_h.weight',\n",
       "               tensor([[-0.0062, -0.0397,  0.0627,  ...,  0.0932,  0.0083,  0.0388],\n",
       "                       [-0.0120,  0.0252, -0.0145,  ...,  0.0028,  0.0883,  0.0489],\n",
       "                       [ 0.0410, -0.0188,  0.0854,  ...,  0.0326,  0.0658, -0.0165],\n",
       "                       ...,\n",
       "                       [ 0.0871, -0.0901, -0.0159,  ...,  0.0454, -0.0046,  0.0039],\n",
       "                       [-0.0190, -0.0205,  0.0170,  ...,  0.0372, -0.1092,  0.0758],\n",
       "                       [-0.0237, -0.0473,  0.0341,  ..., -0.0650, -0.0033, -0.0521]],\n",
       "                      dtype=torch.float16)),\n",
       "              ('layers.23.mlp.dense_4h_to_h.bias',\n",
       "               tensor([ 0.0595,  0.1210,  0.0390,  ...,  0.0196, -0.0449,  0.0138],\n",
       "                      dtype=torch.float16)),\n",
       "              ('final_layernorm.weight',\n",
       "               tensor([0.9106, 0.9858, 0.9575,  ..., 0.9482, 0.9136, 0.9521],\n",
       "                      dtype=torch.float16)),\n",
       "              ('final_layernorm.bias',\n",
       "               tensor([-0.0126, -0.1917, -0.0938,  ..., -0.1656,  0.0500,  0.0865],\n",
       "                      dtype=torch.float16))])}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomegatron_weights['model']['language_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02dbf091",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MegatronBertForTokenClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.encoder.layer.0.attention.ln.weight\", \"bert.encoder.layer.0.attention.ln.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.ln.weight\", \"bert.encoder.layer.0.ln.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.1.attention.ln.weight\", \"bert.encoder.layer.1.attention.ln.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.ln.weight\", \"bert.encoder.layer.1.ln.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.2.attention.ln.weight\", \"bert.encoder.layer.2.attention.ln.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.ln.weight\", \"bert.encoder.layer.2.ln.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.3.attention.ln.weight\", \"bert.encoder.layer.3.attention.ln.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.ln.weight\", \"bert.encoder.layer.3.ln.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.4.attention.ln.weight\", \"bert.encoder.layer.4.attention.ln.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.ln.weight\", \"bert.encoder.layer.4.ln.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.5.attention.ln.weight\", \"bert.encoder.layer.5.attention.ln.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.ln.weight\", \"bert.encoder.layer.5.ln.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.6.attention.ln.weight\", \"bert.encoder.layer.6.attention.ln.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.ln.weight\", \"bert.encoder.layer.6.ln.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.7.attention.ln.weight\", \"bert.encoder.layer.7.attention.ln.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.ln.weight\", \"bert.encoder.layer.7.ln.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.8.attention.ln.weight\", \"bert.encoder.layer.8.attention.ln.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.ln.weight\", \"bert.encoder.layer.8.ln.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.9.attention.ln.weight\", \"bert.encoder.layer.9.attention.ln.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.ln.weight\", \"bert.encoder.layer.9.ln.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.10.attention.ln.weight\", \"bert.encoder.layer.10.attention.ln.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.ln.weight\", \"bert.encoder.layer.10.ln.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.11.attention.ln.weight\", \"bert.encoder.layer.11.attention.ln.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.ln.weight\", \"bert.encoder.layer.11.ln.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.12.attention.ln.weight\", \"bert.encoder.layer.12.attention.ln.bias\", \"bert.encoder.layer.12.attention.self.query.weight\", \"bert.encoder.layer.12.attention.self.query.bias\", \"bert.encoder.layer.12.attention.self.key.weight\", \"bert.encoder.layer.12.attention.self.key.bias\", \"bert.encoder.layer.12.attention.self.value.weight\", \"bert.encoder.layer.12.attention.self.value.bias\", \"bert.encoder.layer.12.attention.output.dense.weight\", \"bert.encoder.layer.12.attention.output.dense.bias\", \"bert.encoder.layer.12.ln.weight\", \"bert.encoder.layer.12.ln.bias\", \"bert.encoder.layer.12.intermediate.dense.weight\", \"bert.encoder.layer.12.intermediate.dense.bias\", \"bert.encoder.layer.12.output.dense.weight\", \"bert.encoder.layer.12.output.dense.bias\", \"bert.encoder.layer.13.attention.ln.weight\", \"bert.encoder.layer.13.attention.ln.bias\", \"bert.encoder.layer.13.attention.self.query.weight\", \"bert.encoder.layer.13.attention.self.query.bias\", \"bert.encoder.layer.13.attention.self.key.weight\", \"bert.encoder.layer.13.attention.self.key.bias\", \"bert.encoder.layer.13.attention.self.value.weight\", \"bert.encoder.layer.13.attention.self.value.bias\", \"bert.encoder.layer.13.attention.output.dense.weight\", \"bert.encoder.layer.13.attention.output.dense.bias\", \"bert.encoder.layer.13.ln.weight\", \"bert.encoder.layer.13.ln.bias\", \"bert.encoder.layer.13.intermediate.dense.weight\", \"bert.encoder.layer.13.intermediate.dense.bias\", \"bert.encoder.layer.13.output.dense.weight\", \"bert.encoder.layer.13.output.dense.bias\", \"bert.encoder.layer.14.attention.ln.weight\", \"bert.encoder.layer.14.attention.ln.bias\", \"bert.encoder.layer.14.attention.self.query.weight\", \"bert.encoder.layer.14.attention.self.query.bias\", \"bert.encoder.layer.14.attention.self.key.weight\", \"bert.encoder.layer.14.attention.self.key.bias\", \"bert.encoder.layer.14.attention.self.value.weight\", \"bert.encoder.layer.14.attention.self.value.bias\", \"bert.encoder.layer.14.attention.output.dense.weight\", \"bert.encoder.layer.14.attention.output.dense.bias\", \"bert.encoder.layer.14.ln.weight\", \"bert.encoder.layer.14.ln.bias\", \"bert.encoder.layer.14.intermediate.dense.weight\", \"bert.encoder.layer.14.intermediate.dense.bias\", \"bert.encoder.layer.14.output.dense.weight\", \"bert.encoder.layer.14.output.dense.bias\", \"bert.encoder.layer.15.attention.ln.weight\", \"bert.encoder.layer.15.attention.ln.bias\", \"bert.encoder.layer.15.attention.self.query.weight\", \"bert.encoder.layer.15.attention.self.query.bias\", \"bert.encoder.layer.15.attention.self.key.weight\", \"bert.encoder.layer.15.attention.self.key.bias\", \"bert.encoder.layer.15.attention.self.value.weight\", \"bert.encoder.layer.15.attention.self.value.bias\", \"bert.encoder.layer.15.attention.output.dense.weight\", \"bert.encoder.layer.15.attention.output.dense.bias\", \"bert.encoder.layer.15.ln.weight\", \"bert.encoder.layer.15.ln.bias\", \"bert.encoder.layer.15.intermediate.dense.weight\", \"bert.encoder.layer.15.intermediate.dense.bias\", \"bert.encoder.layer.15.output.dense.weight\", \"bert.encoder.layer.15.output.dense.bias\", \"bert.encoder.layer.16.attention.ln.weight\", \"bert.encoder.layer.16.attention.ln.bias\", \"bert.encoder.layer.16.attention.self.query.weight\", \"bert.encoder.layer.16.attention.self.query.bias\", \"bert.encoder.layer.16.attention.self.key.weight\", \"bert.encoder.layer.16.attention.self.key.bias\", \"bert.encoder.layer.16.attention.self.value.weight\", \"bert.encoder.layer.16.attention.self.value.bias\", \"bert.encoder.layer.16.attention.output.dense.weight\", \"bert.encoder.layer.16.attention.output.dense.bias\", \"bert.encoder.layer.16.ln.weight\", \"bert.encoder.layer.16.ln.bias\", \"bert.encoder.layer.16.intermediate.dense.weight\", \"bert.encoder.layer.16.intermediate.dense.bias\", \"bert.encoder.layer.16.output.dense.weight\", \"bert.encoder.layer.16.output.dense.bias\", \"bert.encoder.layer.17.attention.ln.weight\", \"bert.encoder.layer.17.attention.ln.bias\", \"bert.encoder.layer.17.attention.self.query.weight\", \"bert.encoder.layer.17.attention.self.query.bias\", \"bert.encoder.layer.17.attention.self.key.weight\", \"bert.encoder.layer.17.attention.self.key.bias\", \"bert.encoder.layer.17.attention.self.value.weight\", \"bert.encoder.layer.17.attention.self.value.bias\", \"bert.encoder.layer.17.attention.output.dense.weight\", \"bert.encoder.layer.17.attention.output.dense.bias\", \"bert.encoder.layer.17.ln.weight\", \"bert.encoder.layer.17.ln.bias\", \"bert.encoder.layer.17.intermediate.dense.weight\", \"bert.encoder.layer.17.intermediate.dense.bias\", \"bert.encoder.layer.17.output.dense.weight\", \"bert.encoder.layer.17.output.dense.bias\", \"bert.encoder.layer.18.attention.ln.weight\", \"bert.encoder.layer.18.attention.ln.bias\", \"bert.encoder.layer.18.attention.self.query.weight\", \"bert.encoder.layer.18.attention.self.query.bias\", \"bert.encoder.layer.18.attention.self.key.weight\", \"bert.encoder.layer.18.attention.self.key.bias\", \"bert.encoder.layer.18.attention.self.value.weight\", \"bert.encoder.layer.18.attention.self.value.bias\", \"bert.encoder.layer.18.attention.output.dense.weight\", \"bert.encoder.layer.18.attention.output.dense.bias\", \"bert.encoder.layer.18.ln.weight\", \"bert.encoder.layer.18.ln.bias\", \"bert.encoder.layer.18.intermediate.dense.weight\", \"bert.encoder.layer.18.intermediate.dense.bias\", \"bert.encoder.layer.18.output.dense.weight\", \"bert.encoder.layer.18.output.dense.bias\", \"bert.encoder.layer.19.attention.ln.weight\", \"bert.encoder.layer.19.attention.ln.bias\", \"bert.encoder.layer.19.attention.self.query.weight\", \"bert.encoder.layer.19.attention.self.query.bias\", \"bert.encoder.layer.19.attention.self.key.weight\", \"bert.encoder.layer.19.attention.self.key.bias\", \"bert.encoder.layer.19.attention.self.value.weight\", \"bert.encoder.layer.19.attention.self.value.bias\", \"bert.encoder.layer.19.attention.output.dense.weight\", \"bert.encoder.layer.19.attention.output.dense.bias\", \"bert.encoder.layer.19.ln.weight\", \"bert.encoder.layer.19.ln.bias\", \"bert.encoder.layer.19.intermediate.dense.weight\", \"bert.encoder.layer.19.intermediate.dense.bias\", \"bert.encoder.layer.19.output.dense.weight\", \"bert.encoder.layer.19.output.dense.bias\", \"bert.encoder.layer.20.attention.ln.weight\", \"bert.encoder.layer.20.attention.ln.bias\", \"bert.encoder.layer.20.attention.self.query.weight\", \"bert.encoder.layer.20.attention.self.query.bias\", \"bert.encoder.layer.20.attention.self.key.weight\", \"bert.encoder.layer.20.attention.self.key.bias\", \"bert.encoder.layer.20.attention.self.value.weight\", \"bert.encoder.layer.20.attention.self.value.bias\", \"bert.encoder.layer.20.attention.output.dense.weight\", \"bert.encoder.layer.20.attention.output.dense.bias\", \"bert.encoder.layer.20.ln.weight\", \"bert.encoder.layer.20.ln.bias\", \"bert.encoder.layer.20.intermediate.dense.weight\", \"bert.encoder.layer.20.intermediate.dense.bias\", \"bert.encoder.layer.20.output.dense.weight\", \"bert.encoder.layer.20.output.dense.bias\", \"bert.encoder.layer.21.attention.ln.weight\", \"bert.encoder.layer.21.attention.ln.bias\", \"bert.encoder.layer.21.attention.self.query.weight\", \"bert.encoder.layer.21.attention.self.query.bias\", \"bert.encoder.layer.21.attention.self.key.weight\", \"bert.encoder.layer.21.attention.self.key.bias\", \"bert.encoder.layer.21.attention.self.value.weight\", \"bert.encoder.layer.21.attention.self.value.bias\", \"bert.encoder.layer.21.attention.output.dense.weight\", \"bert.encoder.layer.21.attention.output.dense.bias\", \"bert.encoder.layer.21.ln.weight\", \"bert.encoder.layer.21.ln.bias\", \"bert.encoder.layer.21.intermediate.dense.weight\", \"bert.encoder.layer.21.intermediate.dense.bias\", \"bert.encoder.layer.21.output.dense.weight\", \"bert.encoder.layer.21.output.dense.bias\", \"bert.encoder.layer.22.attention.ln.weight\", \"bert.encoder.layer.22.attention.ln.bias\", \"bert.encoder.layer.22.attention.self.query.weight\", \"bert.encoder.layer.22.attention.self.query.bias\", \"bert.encoder.layer.22.attention.self.key.weight\", \"bert.encoder.layer.22.attention.self.key.bias\", \"bert.encoder.layer.22.attention.self.value.weight\", \"bert.encoder.layer.22.attention.self.value.bias\", \"bert.encoder.layer.22.attention.output.dense.weight\", \"bert.encoder.layer.22.attention.output.dense.bias\", \"bert.encoder.layer.22.ln.weight\", \"bert.encoder.layer.22.ln.bias\", \"bert.encoder.layer.22.intermediate.dense.weight\", \"bert.encoder.layer.22.intermediate.dense.bias\", \"bert.encoder.layer.22.output.dense.weight\", \"bert.encoder.layer.22.output.dense.bias\", \"bert.encoder.layer.23.attention.ln.weight\", \"bert.encoder.layer.23.attention.ln.bias\", \"bert.encoder.layer.23.attention.self.query.weight\", \"bert.encoder.layer.23.attention.self.query.bias\", \"bert.encoder.layer.23.attention.self.key.weight\", \"bert.encoder.layer.23.attention.self.key.bias\", \"bert.encoder.layer.23.attention.self.value.weight\", \"bert.encoder.layer.23.attention.self.value.bias\", \"bert.encoder.layer.23.attention.output.dense.weight\", \"bert.encoder.layer.23.attention.output.dense.bias\", \"bert.encoder.layer.23.ln.weight\", \"bert.encoder.layer.23.ln.bias\", \"bert.encoder.layer.23.intermediate.dense.weight\", \"bert.encoder.layer.23.intermediate.dense.bias\", \"bert.encoder.layer.23.output.dense.weight\", \"bert.encoder.layer.23.output.dense.bias\", \"bert.encoder.ln.weight\", \"bert.encoder.ln.bias\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"embedding\", \"transformer\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_586/2550534358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmegatron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiomegatron_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MegatronBertForTokenClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.encoder.layer.0.attention.ln.weight\", \"bert.encoder.layer.0.attention.ln.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.ln.weight\", \"bert.encoder.layer.0.ln.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.1.attention.ln.weight\", \"bert.encoder.layer.1.attention.ln.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.ln.weight\", \"bert.encoder.layer.1.ln.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.2.attention.ln.weight\", \"bert.encoder.layer.2.attention.ln.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.ln.weight\", \"bert.encoder.layer.2.ln.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.3.attention.ln.weight\", \"bert.encoder.layer.3.attention.ln.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.ln.weight\", \"bert.encoder.layer.3.ln.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.4.attention.ln.weight\", \"bert.encoder.layer.4.attention.ln.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.ln.weight\", \"bert.encoder.layer.4.ln.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.5.attention.ln.weight\", \"bert.encoder.layer.5.attention.ln.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.ln.weight\", \"bert.encoder.layer.5.ln.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.6.attention.ln.weight\", \"bert.encoder.layer.6.attention.ln.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.ln.weight\", \"bert.encoder.layer.6.ln.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.7.attention.ln.weight\", \"bert.encoder.layer.7.attention.ln.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.ln.weight\", \"bert.encoder.layer.7.ln.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.8.attention.ln.weight\", \"bert.encoder.layer.8.attention.ln.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.ln.weight\", \"bert.encoder.layer.8.ln.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.9.attention.ln.weight\", \"bert.encoder.layer.9.attention.ln.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.ln.weight\", \"bert.encoder.layer.9.ln.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.10.attention.ln.weight\", \"bert.encoder.layer.10.attention.ln.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.ln.weight\", \"bert.encoder.layer.10.ln.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.11.attention.ln.weight\", \"bert.encoder.layer.11.attention.ln.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.ln.weight\", \"bert.encoder.layer.11.ln.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.12.attention.ln.weight\", \"bert.encoder.layer.12.attention.ln.bias\", \"bert.encoder.layer.12.attention.self.query.weight\", \"bert.encoder.layer.12.attention.self.query.bias\", \"bert.encoder.layer.12.attention.self.key.weight\", \"bert.encoder.layer.12.attention.self.key.bias\", \"bert.encoder.layer.12.attention.self.value.weight\", \"bert.encoder.layer.12.attention.self.value.bias\", \"bert.encoder.layer.12.attention.output.dense.weight\", \"bert.encoder.layer.12.attention.output.dense.bias\", \"bert.encoder.layer.12.ln.weight\", \"bert.encoder.layer.12.ln.bias\", \"bert.encoder.layer.12.intermediate.dense.weight\", \"bert.encoder.layer.12.intermediate.dense.bias\", \"bert.encoder.layer.12.output.dense.weight\", \"bert.encoder.layer.12.output.dense.bias\", \"bert.encoder.layer.13.attention.ln.weight\", \"bert.encoder.layer.13.attention.ln.bias\", \"bert.encoder.layer.13.attention.self.query.weight\", \"bert.encoder.layer.13.attention.self.query.bias\", \"bert.encoder.layer.13.attention.self.key.weight\", \"bert.encoder.layer.13.attention.self.key.bias\", \"bert.encoder.layer.13.attention.self.value.weight\", \"bert.encoder.layer.13.attention.self.value.bias\", \"bert.encoder.layer.13.attention.output.dense.weight\", \"bert.encoder.layer.13.attention.output.dense.bias\", \"bert.encoder.layer.13.ln.weight\", \"bert.encoder.layer.13.ln.bias\", \"bert.encoder.layer.13.intermediate.dense.weight\", \"bert.encoder.layer.13.intermediate.dense.bias\", \"bert.encoder.layer.13.output.dense.weight\", \"bert.encoder.layer.13.output.dense.bias\", \"bert.encoder.layer.14.attention.ln.weight\", \"bert.encoder.layer.14.attention.ln.bias\", \"bert.encoder.layer.14.attention.self.query.weight\", \"bert.encoder.layer.14.attention.self.query.bias\", \"bert.encoder.layer.14.attention.self.key.weight\", \"bert.encoder.layer.14.attention.self.key.bias\", \"bert.encoder.layer.14.attention.self.value.weight\", \"bert.encoder.layer.14.attention.self.value.bias\", \"bert.encoder.layer.14.attention.output.dense.weight\", \"bert.encoder.layer.14.attention.output.dense.bias\", \"bert.encoder.layer.14.ln.weight\", \"bert.encoder.layer.14.ln.bias\", \"bert.encoder.layer.14.intermediate.dense.weight\", \"bert.encoder.layer.14.intermediate.dense.bias\", \"bert.encoder.layer.14.output.dense.weight\", \"bert.encoder.layer.14.output.dense.bias\", \"bert.encoder.layer.15.attention.ln.weight\", \"bert.encoder.layer.15.attention.ln.bias\", \"bert.encoder.layer.15.attention.self.query.weight\", \"bert.encoder.layer.15.attention.self.query.bias\", \"bert.encoder.layer.15.attention.self.key.weight\", \"bert.encoder.layer.15.attention.self.key.bias\", \"bert.encoder.layer.15.attention.self.value.weight\", \"bert.encoder.layer.15.attention.self.value.bias\", \"bert.encoder.layer.15.attention.output.dense.weight\", \"bert.encoder.layer.15.attention.output.dense.bias\", \"bert.encoder.layer.15.ln.weight\", \"bert.encoder.layer.15.ln.bias\", \"bert.encoder.layer.15.intermediate.dense.weight\", \"bert.encoder.layer.15.intermediate.dense.bias\", \"bert.encoder.layer.15.output.dense.weight\", \"bert.encoder.layer.15.output.dense.bias\", \"bert.encoder.layer.16.attention.ln.weight\", \"bert.encoder.layer.16.attention.ln.bias\", \"bert.encoder.layer.16.attention.self.query.weight\", \"bert.encoder.layer.16.attention.self.query.bias\", \"bert.encoder.layer.16.attention.self.key.weight\", \"bert.encoder.layer.16.attention.self.key.bias\", \"bert.encoder.layer.16.attention.self.value.weight\", \"bert.encoder.layer.16.attention.self.value.bias\", \"bert.encoder.layer.16.attention.output.dense.weight\", \"bert.encoder.layer.16.attention.output.dense.bias\", \"bert.encoder.layer.16.ln.weight\", \"bert.encoder.layer.16.ln.bias\", \"bert.encoder.layer.16.intermediate.dense.weight\", \"bert.encoder.layer.16.intermediate.dense.bias\", \"bert.encoder.layer.16.output.dense.weight\", \"bert.encoder.layer.16.output.dense.bias\", \"bert.encoder.layer.17.attention.ln.weight\", \"bert.encoder.layer.17.attention.ln.bias\", \"bert.encoder.layer.17.attention.self.query.weight\", \"bert.encoder.layer.17.attention.self.query.bias\", \"bert.encoder.layer.17.attention.self.key.weight\", \"bert.encoder.layer.17.attention.self.key.bias\", \"bert.encoder.layer.17.attention.self.value.weight\", \"bert.encoder.layer.17.attention.self.value.bias\", \"bert.encoder.layer.17.attention.output.dense.weight\", \"bert.encoder.layer.17.attention.output.dense.bias\", \"bert.encoder.layer.17.ln.weight\", \"bert.encoder.layer.17.ln.bias\", \"bert.encoder.layer.17.intermediate.dense.weight\", \"bert.encoder.layer.17.intermediate.dense.bias\", \"bert.encoder.layer.17.output.dense.weight\", \"bert.encoder.layer.17.output.dense.bias\", \"bert.encoder.layer.18.attention.ln.weight\", \"bert.encoder.layer.18.attention.ln.bias\", \"bert.encoder.layer.18.attention.self.query.weight\", \"bert.encoder.layer.18.attention.self.query.bias\", \"bert.encoder.layer.18.attention.self.key.weight\", \"bert.encoder.layer.18.attention.self.key.bias\", \"bert.encoder.layer.18.attention.self.value.weight\", \"bert.encoder.layer.18.attention.self.value.bias\", \"bert.encoder.layer.18.attention.output.dense.weight\", \"bert.encoder.layer.18.attention.output.dense.bias\", \"bert.encoder.layer.18.ln.weight\", \"bert.encoder.layer.18.ln.bias\", \"bert.encoder.layer.18.intermediate.dense.weight\", \"bert.encoder.layer.18.intermediate.dense.bias\", \"bert.encoder.layer.18.output.dense.weight\", \"bert.encoder.layer.18.output.dense.bias\", \"bert.encoder.layer.19.attention.ln.weight\", \"bert.encoder.layer.19.attention.ln.bias\", \"bert.encoder.layer.19.attention.self.query.weight\", \"bert.encoder.layer.19.attention.self.query.bias\", \"bert.encoder.layer.19.attention.self.key.weight\", \"bert.encoder.layer.19.attention.self.key.bias\", \"bert.encoder.layer.19.attention.self.value.weight\", \"bert.encoder.layer.19.attention.self.value.bias\", \"bert.encoder.layer.19.attention.output.dense.weight\", \"bert.encoder.layer.19.attention.output.dense.bias\", \"bert.encoder.layer.19.ln.weight\", \"bert.encoder.layer.19.ln.bias\", \"bert.encoder.layer.19.intermediate.dense.weight\", \"bert.encoder.layer.19.intermediate.dense.bias\", \"bert.encoder.layer.19.output.dense.weight\", \"bert.encoder.layer.19.output.dense.bias\", \"bert.encoder.layer.20.attention.ln.weight\", \"bert.encoder.layer.20.attention.ln.bias\", \"bert.encoder.layer.20.attention.self.query.weight\", \"bert.encoder.layer.20.attention.self.query.bias\", \"bert.encoder.layer.20.attention.self.key.weight\", \"bert.encoder.layer.20.attention.self.key.bias\", \"bert.encoder.layer.20.attention.self.value.weight\", \"bert.encoder.layer.20.attention.self.value.bias\", \"bert.encoder.layer.20.attention.output.dense.weight\", \"bert.encoder.layer.20.attention.output.dense.bias\", \"bert.encoder.layer.20.ln.weight\", \"bert.encoder.layer.20.ln.bias\", \"bert.encoder.layer.20.intermediate.dense.weight\", \"bert.encoder.layer.20.intermediate.dense.bias\", \"bert.encoder.layer.20.output.dense.weight\", \"bert.encoder.layer.20.output.dense.bias\", \"bert.encoder.layer.21.attention.ln.weight\", \"bert.encoder.layer.21.attention.ln.bias\", \"bert.encoder.layer.21.attention.self.query.weight\", \"bert.encoder.layer.21.attention.self.query.bias\", \"bert.encoder.layer.21.attention.self.key.weight\", \"bert.encoder.layer.21.attention.self.key.bias\", \"bert.encoder.layer.21.attention.self.value.weight\", \"bert.encoder.layer.21.attention.self.value.bias\", \"bert.encoder.layer.21.attention.output.dense.weight\", \"bert.encoder.layer.21.attention.output.dense.bias\", \"bert.encoder.layer.21.ln.weight\", \"bert.encoder.layer.21.ln.bias\", \"bert.encoder.layer.21.intermediate.dense.weight\", \"bert.encoder.layer.21.intermediate.dense.bias\", \"bert.encoder.layer.21.output.dense.weight\", \"bert.encoder.layer.21.output.dense.bias\", \"bert.encoder.layer.22.attention.ln.weight\", \"bert.encoder.layer.22.attention.ln.bias\", \"bert.encoder.layer.22.attention.self.query.weight\", \"bert.encoder.layer.22.attention.self.query.bias\", \"bert.encoder.layer.22.attention.self.key.weight\", \"bert.encoder.layer.22.attention.self.key.bias\", \"bert.encoder.layer.22.attention.self.value.weight\", \"bert.encoder.layer.22.attention.self.value.bias\", \"bert.encoder.layer.22.attention.output.dense.weight\", \"bert.encoder.layer.22.attention.output.dense.bias\", \"bert.encoder.layer.22.ln.weight\", \"bert.encoder.layer.22.ln.bias\", \"bert.encoder.layer.22.intermediate.dense.weight\", \"bert.encoder.layer.22.intermediate.dense.bias\", \"bert.encoder.layer.22.output.dense.weight\", \"bert.encoder.layer.22.output.dense.bias\", \"bert.encoder.layer.23.attention.ln.weight\", \"bert.encoder.layer.23.attention.ln.bias\", \"bert.encoder.layer.23.attention.self.query.weight\", \"bert.encoder.layer.23.attention.self.query.bias\", \"bert.encoder.layer.23.attention.self.key.weight\", \"bert.encoder.layer.23.attention.self.key.bias\", \"bert.encoder.layer.23.attention.self.value.weight\", \"bert.encoder.layer.23.attention.self.value.bias\", \"bert.encoder.layer.23.attention.output.dense.weight\", \"bert.encoder.layer.23.attention.output.dense.bias\", \"bert.encoder.layer.23.ln.weight\", \"bert.encoder.layer.23.ln.bias\", \"bert.encoder.layer.23.intermediate.dense.weight\", \"bert.encoder.layer.23.intermediate.dense.bias\", \"bert.encoder.layer.23.output.dense.weight\", \"bert.encoder.layer.23.output.dense.bias\", \"bert.encoder.ln.weight\", \"bert.encoder.ln.bias\", \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"embedding\", \"transformer\". "
     ]
    }
   ],
   "source": [
    "megatron.load_state_dict(biomegatron_weights['model']['language_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d59239b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MegatronBERT.pt  biomegatron-bert-345m-uncased-vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/models/megatron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "03ec3b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f66d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
