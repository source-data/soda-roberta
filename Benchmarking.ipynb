{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3409e99d",
   "metadata": {},
   "source": [
    "# Benchmarking models against SoDa-RoBERTa\n",
    "\n",
    "In this document we want to benchmark the performance of different models in comparisson to [SoDa-RoBERTa](https://github.com/source-data/soda-roberta) in different `token classification` tasks. \n",
    "\n",
    "The goal behind this experiment is not to find the best overall models, but those that are more accurate in classifying text entities of interest in the [SourceData](https://sourcedata.embo.org/) context. This means in the field of molecular cell biology. With this goal in mind, we will not use typical benchmarking datasets, but the [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) and [`sd-nlp-non-tokenized`](https://huggingface.co/datasets/EMBO/sd-nlp-non-tokenized) datasets. These datasets contain annotated image captions extracted from papers on molecular biology. The data has been curated and annotated by profesionals in the field. \n",
    "\n",
    "This notebook is intended to be used with the [ðŸ¤— Datasets](https://huggingface.co/) library. \n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [Chapter 1 - SoDa-RoBERTa](#chapter1)\n",
    "    * [Section 1.1 - NER task for SoDa-RoBERTa](#section_1_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for SoDa-RoBERTa](#section_1_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for SoDa-RoBERTa](#section_1_3)\n",
    "    * [Section 1.4 - PANELIZATION task for SoDa-RoBERTa](#section_1_4)\n",
    "    * [Section 1.5 - BORING task for SoDa-RoBERTa](#section_1_5)\n",
    "* [Chapter 2 - RoBERTa](#chapter2)\n",
    "    * [Section 1.1 - NER task for RoBERTa](#section_2_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for RoBERTa](#section_2_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for RoBERTa](#section_2_3)\n",
    "    * [Section 1.4 - PANELIZATION task for RoBERTa](#section_2_4)\n",
    "    * [Section 1.5 - BORING task for RoBERTa](#section_2_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a17ae074",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from smtag.config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1156d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.15.0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3f2ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e3ed9",
   "metadata": {},
   "source": [
    "# Chapter 1 - SoDa RoBERTa <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "[SoDa-RoBERTa](https://github.com/source-data/soda-roberta) [(Liechti. et al. 2017)](https://doi.org/10.1038/nmeth.4471) is a package generated by the [SourceData](https://sourcedata.embo.org/) team. This package has been developed to improve the data curation of biomedical papers in the field of molecular and cell biology.\n",
    "\n",
    "This is the first model that we will use in our benchmarking. The data available in [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) has already been tokenized using the ðŸ¤— `roberta-base` tokenizer. This tokenizer has been pre-trained with the `roberta-base` model, which is the base model on top of which SoDa-RoBERTa has been built. \n",
    "\n",
    "Since the model is already pre-trained, we just need to fine-tune it. The basic idea is that the pre-trained model with generate a series of outputs that will be token encoders. By fine-tuning a model, FFNN is added on the top of these embeddings and connected to a `softmax` layer to classify tokens.\n",
    "\n",
    "This process is mostly automated to us by the [ðŸ¤— `AutoModelForTokenClassification`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForTokenClassification) class.\n",
    "\n",
    "# Section 1.1 - NER task for SoDa-RoBERTa <a class=\"anchor\" id=\"section_1_1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "060349b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa0871d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/440dcf19a03697fc2ce9c579ac33eca032235705ae974982f23b0275b37d3660)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a57eec257a4eac8d8d7ee3d3daa0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0,\n",
       "   1640,\n",
       "   347,\n",
       "   43,\n",
       "   4052,\n",
       "   847,\n",
       "   33101,\n",
       "   43916,\n",
       "   14868,\n",
       "   303,\n",
       "   129,\n",
       "   15,\n",
       "   15145,\n",
       "   36,\n",
       "   571,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   8,\n",
       "   1368,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   43,\n",
       "   15,\n",
       "   36475,\n",
       "   36,\n",
       "   118,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   43,\n",
       "   9,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   4,\n",
       "   735,\n",
       "   2549,\n",
       "   58,\n",
       "   1455,\n",
       "   129,\n",
       "   15,\n",
       "   5,\n",
       "   5856,\n",
       "   9,\n",
       "   5,\n",
       "   155,\n",
       "   1484,\n",
       "   25,\n",
       "   22827,\n",
       "   13,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   36,\n",
       "   267,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   322,\n",
       "   1437,\n",
       "   2],\n",
       "  [0,\n",
       "   28588,\n",
       "   3693,\n",
       "   3041,\n",
       "   44193,\n",
       "   40899,\n",
       "   16007,\n",
       "   21258,\n",
       "   2018,\n",
       "   5,\n",
       "   127,\n",
       "   523,\n",
       "   12572,\n",
       "   3551,\n",
       "   5252,\n",
       "   11,\n",
       "   364,\n",
       "   771,\n",
       "   2571,\n",
       "   9,\n",
       "   545,\n",
       "   12,\n",
       "   3583,\n",
       "   12,\n",
       "   279,\n",
       "   15540,\n",
       "   9789,\n",
       "   15,\n",
       "   10,\n",
       "   239,\n",
       "   12,\n",
       "   19987,\n",
       "   5626,\n",
       "   36,\n",
       "   725,\n",
       "   24667,\n",
       "   43,\n",
       "   36,\n",
       "   4070,\n",
       "   43,\n",
       "   8,\n",
       "   5,\n",
       "   12337,\n",
       "   11257,\n",
       "   5656,\n",
       "   36,\n",
       "   6960,\n",
       "   322,\n",
       "   20,\n",
       "   2853,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   11579,\n",
       "   1978,\n",
       "   12,\n",
       "   38683,\n",
       "   401,\n",
       "   534,\n",
       "   12,\n",
       "   4590,\n",
       "   4,\n",
       "   20,\n",
       "   1692,\n",
       "   9217,\n",
       "   311,\n",
       "   5,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   438,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   3592,\n",
       "   13418,\n",
       "   21130,\n",
       "   3443,\n",
       "   8,\n",
       "   10888,\n",
       "   401,\n",
       "   347,\n",
       "   4411,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   2544,\n",
       "   4590,\n",
       "   6,\n",
       "   4067,\n",
       "   4,\n",
       "   20,\n",
       "   795,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   10888,\n",
       "   401,\n",
       "   534,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   746,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   40936,\n",
       "   36,\n",
       "   282,\n",
       "   5457,\n",
       "   290,\n",
       "   15540,\n",
       "   322,\n",
       "   1437,\n",
       "   2]],\n",
       " 'labels': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   9,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'tag_mask': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"drAbreu/sd-nlp-2\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "train_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d1e987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97333095",
   "metadata": {},
   "source": [
    "Each of the different models we are using will use different configurations for training. We will generate them using the `config_dict` variable in the module `config.py`.\n",
    "\n",
    "This information contains topics as important as the model checkpoints to be used. \n",
    "\n",
    "SoDa-RoBERTa has generated a language model [`bio-lm`](https://huggingface.co/EMBO/bio-lm). This model has been initialized from the `roberta-base` checkpoint. It is for this reason that the tokenizer to be used is that of `roberta-base`.\n",
    "\n",
    "In the next line we will load the `bio-lm` checkpoint and the `roberta-base` tokenizer.\n",
    "\n",
    "The dataset for `sd-nlp` we have the data ready to be processed. The next step would be to organize the data into a way that it can be load into batches.\n",
    "\n",
    "This is done with data collators. There is a generic data collator known as `DataCollatorForTokenClassification` in  ðŸ¤— that will do what we need. However, we have a `DataCollatorForMaskedTokenClassification` generated that uses the `tag_mask` column to randomly mask the values. This was done by Thomas. I am assuming the reason behind was to improve the generalization of the task. But this needs to be checked  with him.\n",
    "\n",
    "After checking, it looks like for some reason only the `DataCollatorForMaskedTokenClassification` will work here, so let's keep it as it is and move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "101eec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "# Check the case of the MaskedTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "config = Config(model_type = \"Autoencoder\", from_pretrained = \"EMBO/bio-lm\", tokenizer = 'roberta-base')\n",
    "data_collator_mask = DataCollatorForMaskedTokenClassification(tokenizer=AutoTokenizer.from_pretrained(config.tokenizer), \n",
    "                                                              padding=True,\n",
    "                                                              max_length=512,\n",
    "                                                              pad_to_multiple_of=None,\n",
    "                                                              return_tensors='pt',\n",
    "                                                              masking_probability=0.0,\n",
    "                                                              replacement_probability=0.0,\n",
    "                                                              select_labels=False)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                                   padding=True,\n",
    "                                                   return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1770a2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1640,   347,    43,  4052,   847, 33101, 43916, 14868,   303,\n",
       "            129,    15, 15145,    36,   571,    35,   361, 25610,     8,  1368,\n",
       "             35,   545, 25610,    43,    15, 36475,    36,   118,    35,   361,\n",
       "          25610,    43,     9,  3186,  3082,     4,   306,     4,   735,  2549,\n",
       "             58,  1455,   129,    15,     5,  5856,     9,     5,   155,  1484,\n",
       "             25, 22827,    13,  3186,  3082,     4,   306,    36,   267,    35,\n",
       "            545, 25610,   322,  1437,     2]]),\n",
       " 'labels': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,\n",
       "           0,  0, 10,  9,  0,  0,  0,  0,  0, 10,  0,  0,  0, 12,  0,  0,  0, 12,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 'tag_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1\n",
    "batch = data_collator([data[\"train\"][i] for i in range(batch_size)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c79dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1640,   347,    43,  4052,   847, 33101, 43916, 14868,   303,\n",
       "            129,    15, 15145,    36,   571,    35,   361, 25610,     8,  1368,\n",
       "             35,   545, 25610,    43,    15, 36475,    36,   118,    35,   361,\n",
       "          25610,    43,     9,  3186,  3082,     4,   306,     4,   735,  2549,\n",
       "             58,  1455,   129,    15,     5,  5856,     9,     5,   155,  1484,\n",
       "             25, 22827,    13,  3186,  3082,     4,   306,    36,   267,    35,\n",
       "            545, 25610,   322,  1437,     2]]),\n",
       " 'labels': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,\n",
       "           0,  0, 10,  9,  0,  0,  0,  0,  0, 10,  0,  0,  0, 12,  0,  0,  0, 12,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1\n",
    "batch = data_collator_mask([data[\"train\"][i] for i in range(batch_size)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd21e1",
   "metadata": {},
   "source": [
    "After the data collector is time to define the hyperparameters needed by the `Trainer` class of ðŸ¤—. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27027f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "LM_MODEL_PATH = os.getenv('LM_MODEL_PATH')\n",
    "TOKENIZER_PATH = os.getenv('TOKENIZER_PATH')\n",
    "TOKCL_MODEL_PATH = os.getenv('TOKCL_MODEL_PATH')\n",
    "CACHE = os.getenv('CACHE')\n",
    "RUNS_DIR = os.getenv('RUNS_DIR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b684c",
   "metadata": {},
   "source": [
    "We need to do also a series of important definitions at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74ff0a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 15 features:\n",
      "O, I-SMALL_MOLECULE, B-SMALL_MOLECULE, I-GENEPROD, B-GENEPROD, I-SUBCELLULAR, B-SUBCELLULAR, I-CELL, B-CELL, I-TISSUE, B-TISSUE, I-ORGANISM, B-ORGANISM, I-EXP_ASSAY, B-EXP_ASSAY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'I-SMALL_MOLECULE': 1,\n",
       " 'B-SMALL_MOLECULE': 2,\n",
       " 'I-GENEPROD': 3,\n",
       " 'B-GENEPROD': 4,\n",
       " 'I-SUBCELLULAR': 5,\n",
       " 'B-SUBCELLULAR': 6,\n",
       " 'I-CELL': 7,\n",
       " 'B-CELL': 8,\n",
       " 'I-TISSUE': 9,\n",
       " 'B-TISSUE': 10,\n",
       " 'I-ORGANISM': 11,\n",
       " 'B-ORGANISM': 12,\n",
       " 'I-EXP_ASSAY': 13,\n",
       " 'B-EXP_ASSAY': 14}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = train_dataset.info.features['labels'].feature.num_classes\n",
    "label_list = train_dataset.info.features['labels'].feature.names\n",
    "id2label, label2id = {}, {}\n",
    "for class_, label in zip(range(num_labels), label_list):\n",
    "    id2label[class_] = label \n",
    "    label2id[label] = class_ \n",
    "print(f\"\\nTraining on {num_labels} features:\")\n",
    "print(\", \".join(label_list))\n",
    "id2label\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e64a59",
   "metadata": {},
   "source": [
    "Let us define now the metrics that will be used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0725cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.metrics import MetricsTOKCL\n",
    "compute_metrics = MetricsTOKCL(label_list=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2b1b43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='/tokcl_models', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='/tokcl_models/runs/May20_08-45-53_5719849be9d3', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1000, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=1, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='/tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=True, resume_from_checkpoint=None, hub_model_id='EMBO/SourceData-NER', hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=False, gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=None, replacement_probability=None, select_labels=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL\n",
    "\n",
    "training_args = TrainingArgumentsTOKCL(\n",
    "    output_dir = TOKCL_MODEL_PATH,\n",
    "    overwrite_output_dir = True,\n",
    "    logging_steps = 1000,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    prediction_loss_only = True,  # crucial to avoid OOM at evaluation stage!\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 10,\n",
    "    masking_probability = None,\n",
    "    replacement_probability = None,\n",
    "    select_labels = False,\n",
    "    per_gpu_train_batch_size=None, \n",
    "    per_gpu_eval_batch_size=None, \n",
    "    gradient_accumulation_steps=1, \n",
    "    eval_accumulation_steps=None, \n",
    "    weight_decay=0.0, \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.999, \n",
    "    adam_epsilon=1e-08, \n",
    "    max_grad_norm=1.0, \n",
    "    max_steps=-1, \n",
    "    lr_scheduler_type='linear', \n",
    "    warmup_ratio=0.0, \n",
    "    warmup_steps=0, \n",
    "    save_strategy='epoch', \n",
    "    save_steps=1, \n",
    "    save_total_limit=5, \n",
    "    save_on_each_node=False, \n",
    "    no_cuda=False, \n",
    "    seed=42, \n",
    "    bf16=False, \n",
    "    fp16=False, \n",
    "    fp16_opt_level='O1', \n",
    "    half_precision_backend='auto', \n",
    "    bf16_full_eval=False, \n",
    "    fp16_full_eval=False, \n",
    "    tf32=None, \n",
    "    local_rank=-1, \n",
    "    xpu_backend=None, \n",
    "    tpu_num_cores=None, \n",
    "    tpu_metrics_debug=False, \n",
    "    debug=[], \n",
    "    dataloader_drop_last=False, \n",
    "    eval_steps=1000, \n",
    "    dataloader_num_workers=0, \n",
    "    past_index=-1, \n",
    "    run_name=TOKCL_MODEL_PATH, \n",
    "    disable_tqdm=False, \n",
    "    remove_unused_columns=True, \n",
    "    label_names=None, \n",
    "    load_best_model_at_end=False, \n",
    "    metric_for_best_model=None, \n",
    "    greater_is_better=None, \n",
    "    ignore_data_skip=False, \n",
    "    sharded_ddp=[], \n",
    "    deepspeed=None, \n",
    "    label_smoothing_factor=0.0, \n",
    "    adafactor=False, \n",
    "    group_by_length=False, \n",
    "    length_column_name='length', \n",
    "    report_to=['tensorboard'], \n",
    "    ddp_find_unused_parameters=None, \n",
    "    ddp_bucket_cap_mb=None, \n",
    "    dataloader_pin_memory=True, \n",
    "    skip_memory_metrics=True, \n",
    "    use_legacy_prediction_loop=False, \n",
    "    push_to_hub=True, \n",
    "    resume_from_checkpoint=None, \n",
    "    hub_model_id=\"EMBO/SourceData-NER\", \n",
    "    hub_strategy='every_save', \n",
    "    hub_token=False, \n",
    "    gradient_checkpointing=False, \n",
    "    fp16_backend='auto', \n",
    "    mp_parameters=''\n",
    "    )\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88653093",
   "metadata": {},
   "source": [
    "Up to now, we have pre-processed data and load a model. The loaded model has been cropped at the transformer network. For the model to be able to perform a task, we need to provide the model with a model head. \n",
    "\n",
    "The model heads are usually fully connected layers on the top of the transformer network. Although at this point we could perfectly use `torch` to build our own model from the output of the transformers, it has been shown that the performance of fully connected layers is at this point good enough to perform several NLP tasks, including NER.\n",
    "\n",
    "The reason is that the transformer models already encodes several context information on its resulting embeddings. We would therefore not benefit from generating a second RNN or conditional random fields on top, as it was usually done for NER. We will therefore keep it simple and use the fully connected network provided by ðŸ¤—.\n",
    "\n",
    "The way to do so is to load our model, but now using a different class: `AutoModelForTokenClassification`. In this case we use token classification since NER belongs to this task. \n",
    "\n",
    "We need to pass the number of labels. To avoid doing this for every single checkpoint we can do it programatically. The way of getting the number of classes from the training dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09711255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "            config.from_pretrained,\n",
    "            num_labels=num_labels,\n",
    "            max_position_embeddings=config.max_length + 2,  # max_length + 2 for start/end token\n",
    "            id2label = id2label,\n",
    "            label2id = label2id\n",
    "        )\n",
    "model_config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f5ba965",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-SMALL_MOLECULE\",\n",
      "    \"2\": \"B-SMALL_MOLECULE\",\n",
      "    \"3\": \"I-GENEPROD\",\n",
      "    \"4\": \"B-GENEPROD\",\n",
      "    \"5\": \"I-SUBCELLULAR\",\n",
      "    \"6\": \"B-SUBCELLULAR\",\n",
      "    \"7\": \"I-CELL\",\n",
      "    \"8\": \"B-CELL\",\n",
      "    \"9\": \"I-TISSUE\",\n",
      "    \"10\": \"B-TISSUE\",\n",
      "    \"11\": \"I-ORGANISM\",\n",
      "    \"12\": \"B-ORGANISM\",\n",
      "    \"13\": \"I-EXP_ASSAY\",\n",
      "    \"14\": \"B-EXP_ASSAY\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CELL\": 8,\n",
      "    \"B-EXP_ASSAY\": 14,\n",
      "    \"B-GENEPROD\": 4,\n",
      "    \"B-ORGANISM\": 12,\n",
      "    \"B-SMALL_MOLECULE\": 2,\n",
      "    \"B-SUBCELLULAR\": 6,\n",
      "    \"B-TISSUE\": 10,\n",
      "    \"I-CELL\": 7,\n",
      "    \"I-EXP_ASSAY\": 13,\n",
      "    \"I-GENEPROD\": 3,\n",
      "    \"I-ORGANISM\": 11,\n",
      "    \"I-SMALL_MOLECULE\": 1,\n",
      "    \"I-SUBCELLULAR\": 5,\n",
      "    \"I-TISSUE\": 9,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=EMBO/SourceData-NER,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tokcl_models/runs/May20_08-45-53_5719849be9d3,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=None,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=/tokcl_models,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "replacement_probability=None,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=1,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining arguments for model type {config.model_type}:\")\n",
    "print(model_config)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d5def",
   "metadata": {},
   "source": [
    "#### Training Step 3: Define the `Trainer`\n",
    "\n",
    "We are ready now to define the [`Trainer` class](https://ðŸ¤—.co/docs/transformers/main_classes/trainer). This class is a basic training loop supporting a series of features defined in the documentation. However, it can be further customized. We encourage you to take a look to the documentation and try it. \n",
    "\n",
    "As it is, `trainer.train` would already train our model. However, it would offer only information about the loss during the process. We know that we want the loss to get smaller with time, and ideally, that this is true for both, training and validation datasets. Otherwise we would be incurring in overfitting.\n",
    "\n",
    "What if we want to see other information during training like the accuracy or f1 score? `Trainer` provides an argument `compute_metrics` that will help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7c8e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/EMBO/SourceData-NER into local empty directory.\n",
      "WARNING:Cloning https://huggingface.co/EMBO/SourceData-NER into local empty directory.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '/tokcl_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     subprocess.run(\n\u001b[0m\u001b[1;32m    703\u001b[0m                         \u001b[0;34mf\"{'git clone' if self.skip_lfs_files else 'git lfs clone'} {repo_url} .\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'lfs', 'clone', 'https://huggingface.co/EMBO/SourceData-NER', '.']' returned non-zero exit status 2.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m             self.repo = Repository(\n\u001b[0m\u001b[1;32m   2599\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclone_from\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: WARNING: 'git lfs clone' is deprecated and will not be updated\n          with new flags from 'git clone'\n\n'git clone' has been updated in upstream Git to have comparable\nspeeds to 'git lfs clone'.\nCloning into '.'...\nremote: Repository not found\nfatal: repository 'https://huggingface.co/EMBO/SourceData-NER/' not found\nError(s) during clone:\ngit clone failed: exit status 128\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/2379092524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoardCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# Create clone of distant repo and output directory if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;31m# In case of pull, we need to make sure every process has the latest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m                 \u001b[0;31m# Try again after wiping output_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2606\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2607\u001b[0m                 self.repo = Repository(\n\u001b[1;32m   2608\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    720\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '/tokcl_models'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ShowExampleTOKCL(AutoTokenizer.from_pretrained(config.tokenizer))]\n",
    ")\n",
    "\n",
    "# switch the Tensorboard callback to plot losses on same plot\n",
    "trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21987cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d03fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e391a",
   "metadata": {},
   "source": [
    "## Train the models using the general tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62493db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ee55566e4049f284ef69e859af7241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7f6c56ae41476cadefb4966424a57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefc05dc31cf43a391642704f9a58117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937835fdf0ac49d38b21dc1907240d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from smtag.metrics import MetricsTOKCL\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from smtag.config import Config\n",
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfc7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_label(label):\n",
    "    # If the label is B-XXX we change it to I-XX\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    Expands the NER tags once the sub-word tokenization is added.\n",
    "    Arguments\n",
    "    ---------\n",
    "    labels list[int]:\n",
    "    word_ids list[int]\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            # As far as word_id matches the index of the current word\n",
    "            # We append the same label\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            new_labels.append(shift_label(labels[word_id]))\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['words'], \n",
    "                       truncation=True,\n",
    "                       is_split_into_words=True)\n",
    "    \n",
    "    all_labels = examples['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22991ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\"bert-cased\": \"bert-base-cased\", # working\n",
    "               \"bert-uncased\": \"bert-base-uncased\", # working\n",
    "              \"biobert\": \"dmis-lab/biobert-base-cased-v1.1\", # working\n",
    "              \"pubmedbert\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"} # Working on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d724f676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset source_data_nlp/NER to /root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe1c1c55e274b2fab561efa373fd59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This line is taking place\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This line is taking place\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This line is taking place\n",
      "Dataset source_data_nlp downloaded and prepared to /root/.cache/huggingface/datasets/EMBO___source_data_nlp/NER/0.0.1/b15357fa238e627492e02f6ada34ffe2637a00d9bf27a2404602d3f052a46581. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed02082e6a9463681903f764642dcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c43d7bf77146b5bbfa5658e4f8c3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f6fdd7d60c49cc9df4126f2b33591f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b10db7f694941a5890bcd0094d387af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = checkpoints[\"biobert\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "data = load_dataset(\"EMBO/sd-nlp-non-tokenized\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "train_dataset[0:2]\n",
    "\n",
    "tokenized_data = data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=data['train'].column_names)#,\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                        return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95cf97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbbc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_labels(dataset):\n",
    "    num_labels = dataset.info.features['labels'].feature.num_classes\n",
    "    label_list = dataset.info.features['labels'].feature.names\n",
    "    id2label, label2id = {}, {}\n",
    "    print(num_labels, label_list)\n",
    "    for class_, label in zip(range(num_labels), label_list):\n",
    "        id2label[class_] = label \n",
    "        label2id[label] = class_ \n",
    "    return id2label, label2id\n",
    "id2label, label2id = define_labels(tokenized_data['train'])\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a95fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(list(id2label.keys())),\n",
    "    max_position_embeddings=max_length,  \n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")\n",
    "model_config = model.config\n",
    "compute_metrics = MetricsTOKCL(label_list=list(label2id.keys()))\n",
    "       \n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# switch the Tensorboard callback to plot losses on same plot\n",
    "trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "trainer.train()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6027819",
   "metadata": {},
   "source": [
    "# Loading BioMegatron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f51cb96",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory data/models/biomegatron/ or `from_tf` and `from_flax` set to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_377/126808532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"data/models/biomegatron/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         raise ValueError(\n\u001b[1;32m    443\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0marchive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                     raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1299\u001b[0m                         \u001b[0;34mf\"Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                         \u001b[0;34mf\"directory {pretrained_model_name_or_path} or `from_tf` and `from_flax` set to False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory data/models/biomegatron/ or `from_tf` and `from_flax` set to False."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"data/models/biomegatron/\",\n",
    "    from_tf=False,\n",
    "    from_flax=False\n",
    "#     num_labels=len(list(id2label.keys())),\n",
    "#     max_position_embeddings=max_length,  \n",
    "#     id2label = id2label,\n",
    "#     label2id = label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff583e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'str'> for this kind of AutoModel: AutoModel.\nModel type should be one of ImageGPTConfig, QDQBertConfig, FNetConfig, SegformerConfig, VisionTextDualEncoderConfig, PerceiverConfig, GPTJConfig, LayoutLMv2Config, BeitConfig, RemBertConfig, VisualBertConfig, CanineConfig, RoFormerConfig, CLIPConfig, BigBirdPegasusConfig, DeiTConfig, LukeConfig, DetrConfig, GPTNeoConfig, BigBirdConfig, Speech2TextConfig, ViTConfig, Wav2Vec2Config, M2M100Config, ConvBertConfig, LEDConfig, BlenderbotSmallConfig, RetriBertConfig, IBertConfig, MT5Config, T5Config, MobileBertConfig, DistilBertConfig, AlbertConfig, BertGenerationConfig, CamembertConfig, XLMRobertaConfig, PegasusConfig, MarianConfig, MBartConfig, MegatronBertConfig, MPNetConfig, BartConfig, BlenderbotConfig, ReformerConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, FSMTConfig, SqueezeBertConfig, HubertConfig, BertConfig, OpenAIGPTConfig, GPT2Config, TransfoXLConfig, XLNetConfig, XLMProphetNetConfig, ProphetNetConfig, XLMConfig, CTRLConfig, ElectraConfig, FunnelConfig, LxmertConfig, DPRConfig, LayoutLMConfig, TapasConfig, SplinterConfig, SEWDConfig, SEWConfig, UniSpeechSatConfig, UniSpeechConfig, WavLMConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/3779020004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMegatronBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/models/biomegatron/config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m weights = torch.load('data/models/biomegatron/MegatronBERT.pt', \n\u001b[1;32m      5\u001b[0m                                  map_location=torch.device('cpu'))\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'str'> for this kind of AutoModel: AutoModel.\nModel type should be one of ImageGPTConfig, QDQBertConfig, FNetConfig, SegformerConfig, VisionTextDualEncoderConfig, PerceiverConfig, GPTJConfig, LayoutLMv2Config, BeitConfig, RemBertConfig, VisualBertConfig, CanineConfig, RoFormerConfig, CLIPConfig, BigBirdPegasusConfig, DeiTConfig, LukeConfig, DetrConfig, GPTNeoConfig, BigBirdConfig, Speech2TextConfig, ViTConfig, Wav2Vec2Config, M2M100Config, ConvBertConfig, LEDConfig, BlenderbotSmallConfig, RetriBertConfig, IBertConfig, MT5Config, T5Config, MobileBertConfig, DistilBertConfig, AlbertConfig, BertGenerationConfig, CamembertConfig, XLMRobertaConfig, PegasusConfig, MarianConfig, MBartConfig, MegatronBertConfig, MPNetConfig, BartConfig, BlenderbotConfig, ReformerConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, FSMTConfig, SqueezeBertConfig, HubertConfig, BertConfig, OpenAIGPTConfig, GPT2Config, TransfoXLConfig, XLNetConfig, XLMProphetNetConfig, ProphetNetConfig, XLMConfig, CTRLConfig, ElectraConfig, FunnelConfig, LxmertConfig, DPRConfig, LayoutLMConfig, TapasConfig, SplinterConfig, SEWDConfig, SEWConfig, UniSpeechSatConfig, UniSpeechConfig, WavLMConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, MegatronBertConfig\n",
    "import torch\n",
    "model = AutoModel.from_config('data/models/biomegatron/config.json')\n",
    "weights = torch.load('data/models/biomegatron/MegatronBERT.pt', \n",
    "                                 map_location=torch.device('cpu'))\n",
    "model.load_state_dict(weights['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7b541b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MegatronBertModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"encoder.layer.0.attention.ln.weight\", \"encoder.layer.0.attention.ln.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.ln.weight\", \"encoder.layer.0.ln.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.1.attention.ln.weight\", \"encoder.layer.1.attention.ln.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.ln.weight\", \"encoder.layer.1.ln.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.2.attention.ln.weight\", \"encoder.layer.2.attention.ln.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.ln.weight\", \"encoder.layer.2.ln.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.3.attention.ln.weight\", \"encoder.layer.3.attention.ln.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.ln.weight\", \"encoder.layer.3.ln.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.4.attention.ln.weight\", \"encoder.layer.4.attention.ln.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.ln.weight\", \"encoder.layer.4.ln.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.5.attention.ln.weight\", \"encoder.layer.5.attention.ln.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.ln.weight\", \"encoder.layer.5.ln.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.6.attention.ln.weight\", \"encoder.layer.6.attention.ln.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.ln.weight\", \"encoder.layer.6.ln.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.7.attention.ln.weight\", \"encoder.layer.7.attention.ln.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.ln.weight\", \"encoder.layer.7.ln.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.8.attention.ln.weight\", \"encoder.layer.8.attention.ln.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.ln.weight\", \"encoder.layer.8.ln.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.9.attention.ln.weight\", \"encoder.layer.9.attention.ln.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.ln.weight\", \"encoder.layer.9.ln.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.10.attention.ln.weight\", \"encoder.layer.10.attention.ln.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.ln.weight\", \"encoder.layer.10.ln.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.11.attention.ln.weight\", \"encoder.layer.11.attention.ln.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.ln.weight\", \"encoder.layer.11.ln.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.12.attention.ln.weight\", \"encoder.layer.12.attention.ln.bias\", \"encoder.layer.12.attention.self.query.weight\", \"encoder.layer.12.attention.self.query.bias\", \"encoder.layer.12.attention.self.key.weight\", \"encoder.layer.12.attention.self.key.bias\", \"encoder.layer.12.attention.self.value.weight\", \"encoder.layer.12.attention.self.value.bias\", \"encoder.layer.12.attention.output.dense.weight\", \"encoder.layer.12.attention.output.dense.bias\", \"encoder.layer.12.ln.weight\", \"encoder.layer.12.ln.bias\", \"encoder.layer.12.intermediate.dense.weight\", \"encoder.layer.12.intermediate.dense.bias\", \"encoder.layer.12.output.dense.weight\", \"encoder.layer.12.output.dense.bias\", \"encoder.layer.13.attention.ln.weight\", \"encoder.layer.13.attention.ln.bias\", \"encoder.layer.13.attention.self.query.weight\", \"encoder.layer.13.attention.self.query.bias\", \"encoder.layer.13.attention.self.key.weight\", \"encoder.layer.13.attention.self.key.bias\", \"encoder.layer.13.attention.self.value.weight\", \"encoder.layer.13.attention.self.value.bias\", \"encoder.layer.13.attention.output.dense.weight\", \"encoder.layer.13.attention.output.dense.bias\", \"encoder.layer.13.ln.weight\", \"encoder.layer.13.ln.bias\", \"encoder.layer.13.intermediate.dense.weight\", \"encoder.layer.13.intermediate.dense.bias\", \"encoder.layer.13.output.dense.weight\", \"encoder.layer.13.output.dense.bias\", \"encoder.layer.14.attention.ln.weight\", \"encoder.layer.14.attention.ln.bias\", \"encoder.layer.14.attention.self.query.weight\", \"encoder.layer.14.attention.self.query.bias\", \"encoder.layer.14.attention.self.key.weight\", \"encoder.layer.14.attention.self.key.bias\", \"encoder.layer.14.attention.self.value.weight\", \"encoder.layer.14.attention.self.value.bias\", \"encoder.layer.14.attention.output.dense.weight\", \"encoder.layer.14.attention.output.dense.bias\", \"encoder.layer.14.ln.weight\", \"encoder.layer.14.ln.bias\", \"encoder.layer.14.intermediate.dense.weight\", \"encoder.layer.14.intermediate.dense.bias\", \"encoder.layer.14.output.dense.weight\", \"encoder.layer.14.output.dense.bias\", \"encoder.layer.15.attention.ln.weight\", \"encoder.layer.15.attention.ln.bias\", \"encoder.layer.15.attention.self.query.weight\", \"encoder.layer.15.attention.self.query.bias\", \"encoder.layer.15.attention.self.key.weight\", \"encoder.layer.15.attention.self.key.bias\", \"encoder.layer.15.attention.self.value.weight\", \"encoder.layer.15.attention.self.value.bias\", \"encoder.layer.15.attention.output.dense.weight\", \"encoder.layer.15.attention.output.dense.bias\", \"encoder.layer.15.ln.weight\", \"encoder.layer.15.ln.bias\", \"encoder.layer.15.intermediate.dense.weight\", \"encoder.layer.15.intermediate.dense.bias\", \"encoder.layer.15.output.dense.weight\", \"encoder.layer.15.output.dense.bias\", \"encoder.layer.16.attention.ln.weight\", \"encoder.layer.16.attention.ln.bias\", \"encoder.layer.16.attention.self.query.weight\", \"encoder.layer.16.attention.self.query.bias\", \"encoder.layer.16.attention.self.key.weight\", \"encoder.layer.16.attention.self.key.bias\", \"encoder.layer.16.attention.self.value.weight\", \"encoder.layer.16.attention.self.value.bias\", \"encoder.layer.16.attention.output.dense.weight\", \"encoder.layer.16.attention.output.dense.bias\", \"encoder.layer.16.ln.weight\", \"encoder.layer.16.ln.bias\", \"encoder.layer.16.intermediate.dense.weight\", \"encoder.layer.16.intermediate.dense.bias\", \"encoder.layer.16.output.dense.weight\", \"encoder.layer.16.output.dense.bias\", \"encoder.layer.17.attention.ln.weight\", \"encoder.layer.17.attention.ln.bias\", \"encoder.layer.17.attention.self.query.weight\", \"encoder.layer.17.attention.self.query.bias\", \"encoder.layer.17.attention.self.key.weight\", \"encoder.layer.17.attention.self.key.bias\", \"encoder.layer.17.attention.self.value.weight\", \"encoder.layer.17.attention.self.value.bias\", \"encoder.layer.17.attention.output.dense.weight\", \"encoder.layer.17.attention.output.dense.bias\", \"encoder.layer.17.ln.weight\", \"encoder.layer.17.ln.bias\", \"encoder.layer.17.intermediate.dense.weight\", \"encoder.layer.17.intermediate.dense.bias\", \"encoder.layer.17.output.dense.weight\", \"encoder.layer.17.output.dense.bias\", \"encoder.layer.18.attention.ln.weight\", \"encoder.layer.18.attention.ln.bias\", \"encoder.layer.18.attention.self.query.weight\", \"encoder.layer.18.attention.self.query.bias\", \"encoder.layer.18.attention.self.key.weight\", \"encoder.layer.18.attention.self.key.bias\", \"encoder.layer.18.attention.self.value.weight\", \"encoder.layer.18.attention.self.value.bias\", \"encoder.layer.18.attention.output.dense.weight\", \"encoder.layer.18.attention.output.dense.bias\", \"encoder.layer.18.ln.weight\", \"encoder.layer.18.ln.bias\", \"encoder.layer.18.intermediate.dense.weight\", \"encoder.layer.18.intermediate.dense.bias\", \"encoder.layer.18.output.dense.weight\", \"encoder.layer.18.output.dense.bias\", \"encoder.layer.19.attention.ln.weight\", \"encoder.layer.19.attention.ln.bias\", \"encoder.layer.19.attention.self.query.weight\", \"encoder.layer.19.attention.self.query.bias\", \"encoder.layer.19.attention.self.key.weight\", \"encoder.layer.19.attention.self.key.bias\", \"encoder.layer.19.attention.self.value.weight\", \"encoder.layer.19.attention.self.value.bias\", \"encoder.layer.19.attention.output.dense.weight\", \"encoder.layer.19.attention.output.dense.bias\", \"encoder.layer.19.ln.weight\", \"encoder.layer.19.ln.bias\", \"encoder.layer.19.intermediate.dense.weight\", \"encoder.layer.19.intermediate.dense.bias\", \"encoder.layer.19.output.dense.weight\", \"encoder.layer.19.output.dense.bias\", \"encoder.layer.20.attention.ln.weight\", \"encoder.layer.20.attention.ln.bias\", \"encoder.layer.20.attention.self.query.weight\", \"encoder.layer.20.attention.self.query.bias\", \"encoder.layer.20.attention.self.key.weight\", \"encoder.layer.20.attention.self.key.bias\", \"encoder.layer.20.attention.self.value.weight\", \"encoder.layer.20.attention.self.value.bias\", \"encoder.layer.20.attention.output.dense.weight\", \"encoder.layer.20.attention.output.dense.bias\", \"encoder.layer.20.ln.weight\", \"encoder.layer.20.ln.bias\", \"encoder.layer.20.intermediate.dense.weight\", \"encoder.layer.20.intermediate.dense.bias\", \"encoder.layer.20.output.dense.weight\", \"encoder.layer.20.output.dense.bias\", \"encoder.layer.21.attention.ln.weight\", \"encoder.layer.21.attention.ln.bias\", \"encoder.layer.21.attention.self.query.weight\", \"encoder.layer.21.attention.self.query.bias\", \"encoder.layer.21.attention.self.key.weight\", \"encoder.layer.21.attention.self.key.bias\", \"encoder.layer.21.attention.self.value.weight\", \"encoder.layer.21.attention.self.value.bias\", \"encoder.layer.21.attention.output.dense.weight\", \"encoder.layer.21.attention.output.dense.bias\", \"encoder.layer.21.ln.weight\", \"encoder.layer.21.ln.bias\", \"encoder.layer.21.intermediate.dense.weight\", \"encoder.layer.21.intermediate.dense.bias\", \"encoder.layer.21.output.dense.weight\", \"encoder.layer.21.output.dense.bias\", \"encoder.layer.22.attention.ln.weight\", \"encoder.layer.22.attention.ln.bias\", \"encoder.layer.22.attention.self.query.weight\", \"encoder.layer.22.attention.self.query.bias\", \"encoder.layer.22.attention.self.key.weight\", \"encoder.layer.22.attention.self.key.bias\", \"encoder.layer.22.attention.self.value.weight\", \"encoder.layer.22.attention.self.value.bias\", \"encoder.layer.22.attention.output.dense.weight\", \"encoder.layer.22.attention.output.dense.bias\", \"encoder.layer.22.ln.weight\", \"encoder.layer.22.ln.bias\", \"encoder.layer.22.intermediate.dense.weight\", \"encoder.layer.22.intermediate.dense.bias\", \"encoder.layer.22.output.dense.weight\", \"encoder.layer.22.output.dense.bias\", \"encoder.layer.23.attention.ln.weight\", \"encoder.layer.23.attention.ln.bias\", \"encoder.layer.23.attention.self.query.weight\", \"encoder.layer.23.attention.self.query.bias\", \"encoder.layer.23.attention.self.key.weight\", \"encoder.layer.23.attention.self.key.bias\", \"encoder.layer.23.attention.self.value.weight\", \"encoder.layer.23.attention.self.value.bias\", \"encoder.layer.23.attention.output.dense.weight\", \"encoder.layer.23.attention.output.dense.bias\", \"encoder.layer.23.ln.weight\", \"encoder.layer.23.ln.bias\", \"encoder.layer.23.intermediate.dense.weight\", \"encoder.layer.23.intermediate.dense.bias\", \"encoder.layer.23.output.dense.weight\", \"encoder.layer.23.output.dense.bias\", \"encoder.ln.weight\", \"encoder.ln.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"iteration\", \"model\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/2898531425.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMegatronBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMegatronBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMegatronBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMegatronBertConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.load_state_dict(torch.load('data/models/biomegatron/MegatronBERT.pt', \n\u001b[0m\u001b[1;32m      4\u001b[0m                                  map_location=torch.device('cpu')))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MegatronBertModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"encoder.layer.0.attention.ln.weight\", \"encoder.layer.0.attention.ln.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.ln.weight\", \"encoder.layer.0.ln.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.1.attention.ln.weight\", \"encoder.layer.1.attention.ln.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.ln.weight\", \"encoder.layer.1.ln.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.2.attention.ln.weight\", \"encoder.layer.2.attention.ln.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.ln.weight\", \"encoder.layer.2.ln.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.3.attention.ln.weight\", \"encoder.layer.3.attention.ln.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.ln.weight\", \"encoder.layer.3.ln.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.4.attention.ln.weight\", \"encoder.layer.4.attention.ln.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.ln.weight\", \"encoder.layer.4.ln.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.5.attention.ln.weight\", \"encoder.layer.5.attention.ln.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.ln.weight\", \"encoder.layer.5.ln.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.6.attention.ln.weight\", \"encoder.layer.6.attention.ln.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.ln.weight\", \"encoder.layer.6.ln.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.7.attention.ln.weight\", \"encoder.layer.7.attention.ln.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.ln.weight\", \"encoder.layer.7.ln.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.8.attention.ln.weight\", \"encoder.layer.8.attention.ln.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.ln.weight\", \"encoder.layer.8.ln.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.9.attention.ln.weight\", \"encoder.layer.9.attention.ln.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.ln.weight\", \"encoder.layer.9.ln.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.10.attention.ln.weight\", \"encoder.layer.10.attention.ln.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.ln.weight\", \"encoder.layer.10.ln.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.11.attention.ln.weight\", \"encoder.layer.11.attention.ln.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.ln.weight\", \"encoder.layer.11.ln.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.12.attention.ln.weight\", \"encoder.layer.12.attention.ln.bias\", \"encoder.layer.12.attention.self.query.weight\", \"encoder.layer.12.attention.self.query.bias\", \"encoder.layer.12.attention.self.key.weight\", \"encoder.layer.12.attention.self.key.bias\", \"encoder.layer.12.attention.self.value.weight\", \"encoder.layer.12.attention.self.value.bias\", \"encoder.layer.12.attention.output.dense.weight\", \"encoder.layer.12.attention.output.dense.bias\", \"encoder.layer.12.ln.weight\", \"encoder.layer.12.ln.bias\", \"encoder.layer.12.intermediate.dense.weight\", \"encoder.layer.12.intermediate.dense.bias\", \"encoder.layer.12.output.dense.weight\", \"encoder.layer.12.output.dense.bias\", \"encoder.layer.13.attention.ln.weight\", \"encoder.layer.13.attention.ln.bias\", \"encoder.layer.13.attention.self.query.weight\", \"encoder.layer.13.attention.self.query.bias\", \"encoder.layer.13.attention.self.key.weight\", \"encoder.layer.13.attention.self.key.bias\", \"encoder.layer.13.attention.self.value.weight\", \"encoder.layer.13.attention.self.value.bias\", \"encoder.layer.13.attention.output.dense.weight\", \"encoder.layer.13.attention.output.dense.bias\", \"encoder.layer.13.ln.weight\", \"encoder.layer.13.ln.bias\", \"encoder.layer.13.intermediate.dense.weight\", \"encoder.layer.13.intermediate.dense.bias\", \"encoder.layer.13.output.dense.weight\", \"encoder.layer.13.output.dense.bias\", \"encoder.layer.14.attention.ln.weight\", \"encoder.layer.14.attention.ln.bias\", \"encoder.layer.14.attention.self.query.weight\", \"encoder.layer.14.attention.self.query.bias\", \"encoder.layer.14.attention.self.key.weight\", \"encoder.layer.14.attention.self.key.bias\", \"encoder.layer.14.attention.self.value.weight\", \"encoder.layer.14.attention.self.value.bias\", \"encoder.layer.14.attention.output.dense.weight\", \"encoder.layer.14.attention.output.dense.bias\", \"encoder.layer.14.ln.weight\", \"encoder.layer.14.ln.bias\", \"encoder.layer.14.intermediate.dense.weight\", \"encoder.layer.14.intermediate.dense.bias\", \"encoder.layer.14.output.dense.weight\", \"encoder.layer.14.output.dense.bias\", \"encoder.layer.15.attention.ln.weight\", \"encoder.layer.15.attention.ln.bias\", \"encoder.layer.15.attention.self.query.weight\", \"encoder.layer.15.attention.self.query.bias\", \"encoder.layer.15.attention.self.key.weight\", \"encoder.layer.15.attention.self.key.bias\", \"encoder.layer.15.attention.self.value.weight\", \"encoder.layer.15.attention.self.value.bias\", \"encoder.layer.15.attention.output.dense.weight\", \"encoder.layer.15.attention.output.dense.bias\", \"encoder.layer.15.ln.weight\", \"encoder.layer.15.ln.bias\", \"encoder.layer.15.intermediate.dense.weight\", \"encoder.layer.15.intermediate.dense.bias\", \"encoder.layer.15.output.dense.weight\", \"encoder.layer.15.output.dense.bias\", \"encoder.layer.16.attention.ln.weight\", \"encoder.layer.16.attention.ln.bias\", \"encoder.layer.16.attention.self.query.weight\", \"encoder.layer.16.attention.self.query.bias\", \"encoder.layer.16.attention.self.key.weight\", \"encoder.layer.16.attention.self.key.bias\", \"encoder.layer.16.attention.self.value.weight\", \"encoder.layer.16.attention.self.value.bias\", \"encoder.layer.16.attention.output.dense.weight\", \"encoder.layer.16.attention.output.dense.bias\", \"encoder.layer.16.ln.weight\", \"encoder.layer.16.ln.bias\", \"encoder.layer.16.intermediate.dense.weight\", \"encoder.layer.16.intermediate.dense.bias\", \"encoder.layer.16.output.dense.weight\", \"encoder.layer.16.output.dense.bias\", \"encoder.layer.17.attention.ln.weight\", \"encoder.layer.17.attention.ln.bias\", \"encoder.layer.17.attention.self.query.weight\", \"encoder.layer.17.attention.self.query.bias\", \"encoder.layer.17.attention.self.key.weight\", \"encoder.layer.17.attention.self.key.bias\", \"encoder.layer.17.attention.self.value.weight\", \"encoder.layer.17.attention.self.value.bias\", \"encoder.layer.17.attention.output.dense.weight\", \"encoder.layer.17.attention.output.dense.bias\", \"encoder.layer.17.ln.weight\", \"encoder.layer.17.ln.bias\", \"encoder.layer.17.intermediate.dense.weight\", \"encoder.layer.17.intermediate.dense.bias\", \"encoder.layer.17.output.dense.weight\", \"encoder.layer.17.output.dense.bias\", \"encoder.layer.18.attention.ln.weight\", \"encoder.layer.18.attention.ln.bias\", \"encoder.layer.18.attention.self.query.weight\", \"encoder.layer.18.attention.self.query.bias\", \"encoder.layer.18.attention.self.key.weight\", \"encoder.layer.18.attention.self.key.bias\", \"encoder.layer.18.attention.self.value.weight\", \"encoder.layer.18.attention.self.value.bias\", \"encoder.layer.18.attention.output.dense.weight\", \"encoder.layer.18.attention.output.dense.bias\", \"encoder.layer.18.ln.weight\", \"encoder.layer.18.ln.bias\", \"encoder.layer.18.intermediate.dense.weight\", \"encoder.layer.18.intermediate.dense.bias\", \"encoder.layer.18.output.dense.weight\", \"encoder.layer.18.output.dense.bias\", \"encoder.layer.19.attention.ln.weight\", \"encoder.layer.19.attention.ln.bias\", \"encoder.layer.19.attention.self.query.weight\", \"encoder.layer.19.attention.self.query.bias\", \"encoder.layer.19.attention.self.key.weight\", \"encoder.layer.19.attention.self.key.bias\", \"encoder.layer.19.attention.self.value.weight\", \"encoder.layer.19.attention.self.value.bias\", \"encoder.layer.19.attention.output.dense.weight\", \"encoder.layer.19.attention.output.dense.bias\", \"encoder.layer.19.ln.weight\", \"encoder.layer.19.ln.bias\", \"encoder.layer.19.intermediate.dense.weight\", \"encoder.layer.19.intermediate.dense.bias\", \"encoder.layer.19.output.dense.weight\", \"encoder.layer.19.output.dense.bias\", \"encoder.layer.20.attention.ln.weight\", \"encoder.layer.20.attention.ln.bias\", \"encoder.layer.20.attention.self.query.weight\", \"encoder.layer.20.attention.self.query.bias\", \"encoder.layer.20.attention.self.key.weight\", \"encoder.layer.20.attention.self.key.bias\", \"encoder.layer.20.attention.self.value.weight\", \"encoder.layer.20.attention.self.value.bias\", \"encoder.layer.20.attention.output.dense.weight\", \"encoder.layer.20.attention.output.dense.bias\", \"encoder.layer.20.ln.weight\", \"encoder.layer.20.ln.bias\", \"encoder.layer.20.intermediate.dense.weight\", \"encoder.layer.20.intermediate.dense.bias\", \"encoder.layer.20.output.dense.weight\", \"encoder.layer.20.output.dense.bias\", \"encoder.layer.21.attention.ln.weight\", \"encoder.layer.21.attention.ln.bias\", \"encoder.layer.21.attention.self.query.weight\", \"encoder.layer.21.attention.self.query.bias\", \"encoder.layer.21.attention.self.key.weight\", \"encoder.layer.21.attention.self.key.bias\", \"encoder.layer.21.attention.self.value.weight\", \"encoder.layer.21.attention.self.value.bias\", \"encoder.layer.21.attention.output.dense.weight\", \"encoder.layer.21.attention.output.dense.bias\", \"encoder.layer.21.ln.weight\", \"encoder.layer.21.ln.bias\", \"encoder.layer.21.intermediate.dense.weight\", \"encoder.layer.21.intermediate.dense.bias\", \"encoder.layer.21.output.dense.weight\", \"encoder.layer.21.output.dense.bias\", \"encoder.layer.22.attention.ln.weight\", \"encoder.layer.22.attention.ln.bias\", \"encoder.layer.22.attention.self.query.weight\", \"encoder.layer.22.attention.self.query.bias\", \"encoder.layer.22.attention.self.key.weight\", \"encoder.layer.22.attention.self.key.bias\", \"encoder.layer.22.attention.self.value.weight\", \"encoder.layer.22.attention.self.value.bias\", \"encoder.layer.22.attention.output.dense.weight\", \"encoder.layer.22.attention.output.dense.bias\", \"encoder.layer.22.ln.weight\", \"encoder.layer.22.ln.bias\", \"encoder.layer.22.intermediate.dense.weight\", \"encoder.layer.22.intermediate.dense.bias\", \"encoder.layer.22.output.dense.weight\", \"encoder.layer.22.output.dense.bias\", \"encoder.layer.23.attention.ln.weight\", \"encoder.layer.23.attention.ln.bias\", \"encoder.layer.23.attention.self.query.weight\", \"encoder.layer.23.attention.self.query.bias\", \"encoder.layer.23.attention.self.key.weight\", \"encoder.layer.23.attention.self.key.bias\", \"encoder.layer.23.attention.self.value.weight\", \"encoder.layer.23.attention.self.value.bias\", \"encoder.layer.23.attention.output.dense.weight\", \"encoder.layer.23.attention.output.dense.bias\", \"encoder.layer.23.ln.weight\", \"encoder.layer.23.ln.bias\", \"encoder.layer.23.intermediate.dense.weight\", \"encoder.layer.23.intermediate.dense.bias\", \"encoder.layer.23.output.dense.weight\", \"encoder.layer.23.output.dense.bias\", \"encoder.ln.weight\", \"encoder.ln.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"iteration\", \"model\". "
     ]
    }
   ],
   "source": [
    "from transformers import MegatronBertModel, MegatronBertConfig\n",
    "model = MegatronBertModel(MegatronBertConfig()) \n",
    "model.load_state_dict(torch.load('data/models/biomegatron/MegatronBERT.pt', \n",
    "                                 map_location=torch.device('cpu')))\n",
    "\n",
    "# (torch.load('data/models/biomegatron/MegatronBERT.pt', \n",
    "#                                  map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c7f3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/nvidia/megatron-bert-cased-345m/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'nvidia/megatron-bert-cased-345m'. Make sure that:\n\n- 'nvidia/megatron-bert-cased-345m' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'nvidia/megatron-bert-cased-345m' is not a path to a local directory with something else, in that case)\n\n- or 'nvidia/megatron-bert-cased-345m' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             resolved_config_file = cached_path(\n\u001b[0m\u001b[1;32m    569\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1777\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1947\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1948\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1949\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/nvidia/megatron-bert-cased-345m/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_457/141831498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nvidia/megatron-bert-cased-345m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'nvidia/megatron-bert-cased-345m'. Make sure that:\n\n- 'nvidia/megatron-bert-cased-345m' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'nvidia/megatron-bert-cased-345m' is not a path to a local directory with something else, in that case)\n\n- or 'nvidia/megatron-bert-cased-345m' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/megatron-bert-cased-345m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a4584f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['language_model', 'qa_head'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0954b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embedding', 'transformer'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['model']['language_model'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecf4b7",
   "metadata": {},
   "source": [
    "## Testing a loop for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c826ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from smtag.metrics import MetricsTOKCL\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from smtag.config import Config\n",
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\"NER\", \"GENEPROD_ROLES\", \"SMALL_MOL_ROLES\", \"BORING\", \"PANELIZATION\"]\n",
    "MODELS = [\"EMBO/bio-lm\", \"roberta-base\"]\n",
    "\n",
    "ROBERTA_DATASET = \"drAbreu/sd-nlp-2\"\n",
    "GENERAL_DATASET = \"EMBO/sd-nlp-non-tokenized\"\n",
    "\n",
    "HUB_TOKEN = \"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\"\n",
    "\n",
    "HUB_USER = \"EMBO\"\n",
    "\n",
    "load_dotenv()\n",
    "LM_MODEL_PATH = os.getenv('LM_MODEL_PATH')\n",
    "TOKENIZER_PATH = os.getenv('TOKENIZER_PATH')\n",
    "TOKCL_MODEL_PATH = os.getenv('TOKCL_MODEL_PATH')\n",
    "CACHE = os.getenv('CACHE')\n",
    "RUNS_DIR = os.getenv('RUNS_DIR')\n",
    "\n",
    "TRAINING_ARGS_DICT = {\"output_dir\": TOKCL_MODEL_PATH,\n",
    "                     \"overwrite_output_dir\": True,\n",
    "                    \"logging_steps\":1000,\n",
    "                    \"evaluation_strategy\":'epoch',\n",
    "                    \"lr_scheduler_type\":'linear', \n",
    "                    \"save_strategy\":'epoch', \n",
    "                    \"save_steps\":1, \n",
    "#                     \"eval_strategy\":'epoch', \n",
    "                    \"save_total_limit\":5, \n",
    "                    \"seed\":42, \n",
    "                    \"eval_steps\":1, \n",
    "                    \"past_index\":-1, \n",
    "                    \"run_name\":TOKCL_MODEL_PATH, \n",
    "                    \"disable_tqdm\":False, \n",
    "                    \"metric_for_best_model\":'overall_f1', \n",
    "                    \"load_best_model_at_end\":True, \n",
    "                    \"greater_is_better\":True, \n",
    "                    \"length_column_name\":'length', \n",
    "                    \"report_to\":['tensorboard'], \n",
    "                    \"push_to_hub\":False, \n",
    "                    \"resume_from_checkpoint\":None,  \n",
    "                    \"hub_strategy\":'every_save', \n",
    "                    \"hub_token\":HUB_TOKEN, \n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"drAbreu/sd-nlp-2\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_labels(dataset):\n",
    "    num_labels = dataset.info.features['labels'].feature.num_classes\n",
    "    label_list = dataset.info.features['labels'].feature.names\n",
    "    id2label, label2id = {}, {}\n",
    "    for class_, label in zip(range(num_labels), label_list):\n",
    "        id2label[class_] = label \n",
    "        label2id[label] = class_ \n",
    "        return id2label, label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb049b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_label(label):\n",
    "    # If the label is B-XXX we change it to I-XX\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    Expands the NER tags once the sub-word tokenization is added.\n",
    "    Arguments\n",
    "    ---------\n",
    "    labels list[int]:\n",
    "    word_ids list[int]\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            # As far as word_id matches the index of the current word\n",
    "            # We append the same label\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            new_labels.append(shift_label(labels[word_id]))\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['words'], \n",
    "                       truncation=True,\n",
    "                       is_split_into_words=True)\n",
    "    \n",
    "    all_labels = examples['labels']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c09da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_repo_names = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    for task in TASKS:\n",
    "        TRAINING_ARGS_DICT[\"hub_model_id\"] = f\"{HUB_USER}/{model_name.replace('/','_')}_{task}\"\n",
    "        if model_name in [\"EMBO/bio-lm\", \"roberta-base\"]:\n",
    "            \n",
    "            config = Config(model_type = \"Autoencoder\", \n",
    "                            from_pretrained = model_name, \n",
    "                            tokenizer = \"roberta-base\")\n",
    "            \n",
    "            data = load_dataset(ROBERTA_DATASET, task)\n",
    "            train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "            id2label, label2id = define_labels(train_dataset)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)\n",
    "            data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                                               padding=True,\n",
    "                                                               return_tensors='pt')\n",
    "            \n",
    "            compute_metrics = MetricsTOKCL(label_list=list(label2id.keys()))\n",
    "            \n",
    "            \n",
    "            # Get the training arguments\n",
    "            training_args = TrainingArgumentsTOKCL(**TRAINING_ARGS_DICT)\n",
    "\n",
    "            # Select the model (This is for Token Classification)\n",
    "            \n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                config.from_pretrained,\n",
    "                num_labels=len(list(id2label.keys())),\n",
    "                max_position_embeddings=config.max_length + 2,  \n",
    "                id2label = id2label,\n",
    "                label2id = label2id\n",
    "            )\n",
    "            model_config = model.config\n",
    "            \n",
    "            # Define the trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[ShowExampleTOKCL(tokenizer)]\n",
    "            )\n",
    "\n",
    "            # switch the Tensorboard callback to plot losses on same plot\n",
    "            trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "            trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "#             train()\n",
    "            list_repo_names.append(TRAINING_ARGS_DICT[\"hub_model_id\"])\n",
    "    \n",
    "        elif model_name in []:\n",
    "            data = load_dataset(GENERAL_DATASET, task)\n",
    "            list_repo_names.append(TRAINING_ARGS_DICT[\"hub_model_id\"])\n",
    "        else:\n",
    "            raise ValueError(f\"The selected model ({model_name}) is not contained in our Benchmark list. Please add it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7db06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_repo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745813bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c77c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd65c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
