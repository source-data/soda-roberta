{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd3caa6",
   "metadata": {},
   "source": [
    "# Benchmarking models against SoDa-RoBERTa\n",
    "\n",
    "In this document we want to benchmark the performance of different models in comparisson to [SoDa-RoBERTa](https://github.com/source-data/soda-roberta) in different `token classification` tasks. \n",
    "\n",
    "The goal behind this experiment is not to find the best overall models, but those that are more accurate in classifying text entities of interest in the [SourceData](https://sourcedata.embo.org/) context. This means in the field of molecular cell biology. With this goal in mind, we will not use typical benchmarking datasets, but the [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) and [`sd-nlp-non-tokenized`](https://huggingface.co/datasets/EMBO/sd-nlp-non-tokenized) datasets. These datasets contain annotated image captions extracted from papers on molecular biology. The data has been curated and annotated by profesionals in the field. \n",
    "\n",
    "This notebook is intended to be used with the [ðŸ¤— Datasets](https://huggingface.co/) library. \n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* [Chapter 1 - SoDa-RoBERTa](#chapter1)\n",
    "    * [Section 1.1 - NER task for SoDa-RoBERTa](#section_1_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for SoDa-RoBERTa](#section_1_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for SoDa-RoBERTa](#section_1_3)\n",
    "    * [Section 1.4 - PANELIZATION task for SoDa-RoBERTa](#section_1_4)\n",
    "    * [Section 1.5 - BORING task for SoDa-RoBERTa](#section_1_5)\n",
    "* [Chapter 2 - RoBERTa](#chapter2)\n",
    "    * [Section 1.1 - NER task for RoBERTa](#section_2_1)\n",
    "    * [Section 1.2 - SMALL_MOL_ROLES task for RoBERTa](#section_2_2)\n",
    "    * [Section 1.3 - GENEPROD_ROLES task for RoBERTa](#section_2_3)\n",
    "    * [Section 1.4 - PANELIZATION task for RoBERTa](#section_2_4)\n",
    "    * [Section 1.5 - BORING task for RoBERTa](#section_2_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad50715",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from smtag.config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79fa4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353defb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "HUB_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fef7a",
   "metadata": {},
   "source": [
    "# Chapter 1 - SoDa RoBERTa <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n",
    "[SoDa-RoBERTa](https://github.com/source-data/soda-roberta) [(Liechti. et al. 2017)](https://doi.org/10.1038/nmeth.4471) is a package generated by the [SourceData](https://sourcedata.embo.org/) team. This package has been developed to improve the data curation of biomedical papers in the field of molecular and cell biology.\n",
    "\n",
    "This is the first model that we will use in our benchmarking. The data available in [`sd-nlp`](https://huggingface.co/datasets/EMBO/sd-nlp) has already been tokenized using the ðŸ¤— `roberta-base` tokenizer. This tokenizer has been pre-trained with the `roberta-base` model, which is the base model on top of which SoDa-RoBERTa has been built. \n",
    "\n",
    "Since the model is already pre-trained, we just need to fine-tune it. The basic idea is that the pre-trained model with generate a series of outputs that will be token encoders. By fine-tuning a model, FFNN is added on the top of these embeddings and connected to a `softmax` layer to classify tokens.\n",
    "\n",
    "This process is mostly automated to us by the [ðŸ¤— `AutoModelForTokenClassification`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForTokenClassification) class.\n",
    "\n",
    "# Section 1.1 - NER task for SoDa-RoBERTa <a class=\"anchor\" id=\"section_1_1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7822e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f4915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/drAbreu___source_data_nlp/NER/0.0.1/bc570bbe08a861e7e058193ee9cf347dbe9f132e96d86d20817bc45c49737226)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c1f0d36b364e2da3a070b833368d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0,\n",
       "   1640,\n",
       "   347,\n",
       "   43,\n",
       "   4052,\n",
       "   847,\n",
       "   33101,\n",
       "   43916,\n",
       "   14868,\n",
       "   303,\n",
       "   129,\n",
       "   15,\n",
       "   15145,\n",
       "   36,\n",
       "   571,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   8,\n",
       "   1368,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   43,\n",
       "   15,\n",
       "   36475,\n",
       "   36,\n",
       "   118,\n",
       "   35,\n",
       "   361,\n",
       "   25610,\n",
       "   43,\n",
       "   9,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   4,\n",
       "   735,\n",
       "   2549,\n",
       "   58,\n",
       "   1455,\n",
       "   129,\n",
       "   15,\n",
       "   5,\n",
       "   5856,\n",
       "   9,\n",
       "   5,\n",
       "   155,\n",
       "   1484,\n",
       "   25,\n",
       "   22827,\n",
       "   13,\n",
       "   3186,\n",
       "   3082,\n",
       "   4,\n",
       "   306,\n",
       "   36,\n",
       "   267,\n",
       "   35,\n",
       "   545,\n",
       "   25610,\n",
       "   322,\n",
       "   1437,\n",
       "   2],\n",
       "  [0,\n",
       "   28588,\n",
       "   3693,\n",
       "   3041,\n",
       "   44193,\n",
       "   40899,\n",
       "   16007,\n",
       "   21258,\n",
       "   2018,\n",
       "   5,\n",
       "   127,\n",
       "   523,\n",
       "   12572,\n",
       "   3551,\n",
       "   5252,\n",
       "   11,\n",
       "   364,\n",
       "   771,\n",
       "   2571,\n",
       "   9,\n",
       "   545,\n",
       "   12,\n",
       "   3583,\n",
       "   12,\n",
       "   279,\n",
       "   15540,\n",
       "   9789,\n",
       "   15,\n",
       "   10,\n",
       "   239,\n",
       "   12,\n",
       "   19987,\n",
       "   5626,\n",
       "   36,\n",
       "   725,\n",
       "   24667,\n",
       "   43,\n",
       "   36,\n",
       "   4070,\n",
       "   43,\n",
       "   8,\n",
       "   5,\n",
       "   12337,\n",
       "   11257,\n",
       "   5656,\n",
       "   36,\n",
       "   6960,\n",
       "   322,\n",
       "   20,\n",
       "   2853,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   11579,\n",
       "   1978,\n",
       "   12,\n",
       "   38683,\n",
       "   401,\n",
       "   534,\n",
       "   12,\n",
       "   4590,\n",
       "   4,\n",
       "   20,\n",
       "   1692,\n",
       "   9217,\n",
       "   311,\n",
       "   5,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   438,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   3592,\n",
       "   13418,\n",
       "   21130,\n",
       "   3443,\n",
       "   8,\n",
       "   10888,\n",
       "   401,\n",
       "   347,\n",
       "   4411,\n",
       "   256,\n",
       "   13459,\n",
       "   11194,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   9,\n",
       "   274,\n",
       "   306,\n",
       "   73,\n",
       "   2940,\n",
       "   2544,\n",
       "   4590,\n",
       "   6,\n",
       "   4067,\n",
       "   4,\n",
       "   20,\n",
       "   795,\n",
       "   2798,\n",
       "   924,\n",
       "   5,\n",
       "   10888,\n",
       "   401,\n",
       "   534,\n",
       "   4411,\n",
       "   7522,\n",
       "   1225,\n",
       "   428,\n",
       "   1690,\n",
       "   8173,\n",
       "   4392,\n",
       "   821,\n",
       "   1070,\n",
       "   30,\n",
       "   746,\n",
       "   7522,\n",
       "   1898,\n",
       "   2744,\n",
       "   40936,\n",
       "   36,\n",
       "   282,\n",
       "   5457,\n",
       "   290,\n",
       "   15540,\n",
       "   322,\n",
       "   1437,\n",
       "   2]],\n",
       " 'labels': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   9,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'tag_mask': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"drAbreu/sd-nlp-2\", \"NER\")\n",
    "train_dataset, eval_dataset, test_dataset = data[\"train\"], data['validation'], data['test']\n",
    "train_dataset[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b52a0",
   "metadata": {},
   "source": [
    "Each of the different models we are using will use different configurations for training. We will generate them using the `config_dict` variable in the module `config.py`.\n",
    "\n",
    "This information contains topics as important as the model checkpoints to be used. \n",
    "\n",
    "SoDa-RoBERTa has generated a language model [`bio-lm`](https://huggingface.co/EMBO/bio-lm). This model has been initialized from the `roberta-base` checkpoint. It is for this reason that the tokenizer to be used is that of `roberta-base`.\n",
    "\n",
    "In the next line we will load the `bio-lm` checkpoint and the `roberta-base` tokenizer.\n",
    "\n",
    "The dataset for `sd-nlp` we have the data ready to be processed. The next step would be to organize the data into a way that it can be load into batches.\n",
    "\n",
    "This is done with data collators. There is a generic data collator known as `DataCollatorForTokenClassification` in  ðŸ¤— that will do what we need. However, we have a `DataCollatorForMaskedTokenClassification` generated that uses the `tag_mask` column to randomly mask the values. This was done by Thomas. I am assuming the reason behind was to improve the generalization of the task. But this needs to be checked  with him.\n",
    "\n",
    "After checking, it looks like for some reason only the `DataCollatorForMaskedTokenClassification` will work here, so let's keep it as it is and move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c72e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from smtag.data_collator import DataCollatorForMaskedTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "# Check the case of the MaskedTokenClassification\n",
    "config = Config(model_type = \"Autoencoder\", from_pretrained = \"EMBO/bio-lm\", tokenizer = 'roberta-base')\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True,return_tensors='pt')\n",
    "data_collator_mask = DataCollatorForMaskedTokenClassification(tokenizer=AutoTokenizer.from_pretrained(config.tokenizer), \n",
    "                                                              padding=True,\n",
    "                                                              max_length=512,\n",
    "                                                              pad_to_multiple_of=None,\n",
    "                                                              return_tensors='pt',\n",
    "                                                              masking_probability=0.1,\n",
    "                                                              replacement_probability=0.1,\n",
    "                                                              select_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426096c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2242: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1640,   347,    43,  4052,   847, 33101, 43916, 14868,   303,\n",
       "            129,    15,  5507,    36,   571,    35,   361, 25610,     8,  1368,\n",
       "             35,   545, 25610,    43,    15, 36475,    36,   118,    35,   361,\n",
       "          25610,    43,     9, 32367,  3082,     4,   306,     4,   735, 19990,\n",
       "             58,  1455,   129,    15,     5,  5856,     9,     5,   155, 50264,\n",
       "             25, 22827,    13,  3186,  3082,     4,   306,    36,   267,    35,\n",
       "            545, 25610,   322,  1437,     2]]),\n",
       " 'labels': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,\n",
       "           0,  0, 10,  9,  0,  0,  0,  0,  0, 10,  0,  0,  0, 12,  0,  0,  0, 12,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=1\n",
    "batch = data_collator_mask([data[\"train\"][i] for i in range(batch_size)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4a12c",
   "metadata": {},
   "source": [
    "After the data collector is time to define the hyperparameters needed by the `Trainer` class of ðŸ¤—. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73ede13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# LM_MODEL_PATH = os.getenv('LM_MODEL_PATH')\n",
    "# TOKENIZER_PATH = os.getenv('TOKENIZER_PATH')\n",
    "# TOKCL_MODEL_PATH = os.getenv('TOKCL_MODEL_PATH')\n",
    "# CACHE = os.getenv('CACHE')\n",
    "# RUNS_DIR = os.getenv('RUNS_DIR')\n",
    "LM_MODEL_PATH=\"./.lm_models\"\n",
    "TOKCL_MODEL_PATH=\"./.tokcl_models\"\n",
    "CACHE=\"./.cache\"\n",
    "RUNS_DIR=\"./.runs\"\n",
    "DUMMY_DIR=\"./.dummy\"\n",
    "RUNTIME=\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc4e75",
   "metadata": {},
   "source": [
    "We need to do also a series of important definitions at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dae54e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 15 features:\n",
      "O, I-SMALL_MOLECULE, B-SMALL_MOLECULE, I-GENEPROD, B-GENEPROD, I-SUBCELLULAR, B-SUBCELLULAR, I-CELL, B-CELL, I-TISSUE, B-TISSUE, I-ORGANISM, B-ORGANISM, I-EXP_ASSAY, B-EXP_ASSAY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'I-SMALL_MOLECULE': 1,\n",
       " 'B-SMALL_MOLECULE': 2,\n",
       " 'I-GENEPROD': 3,\n",
       " 'B-GENEPROD': 4,\n",
       " 'I-SUBCELLULAR': 5,\n",
       " 'B-SUBCELLULAR': 6,\n",
       " 'I-CELL': 7,\n",
       " 'B-CELL': 8,\n",
       " 'I-TISSUE': 9,\n",
       " 'B-TISSUE': 10,\n",
       " 'I-ORGANISM': 11,\n",
       " 'B-ORGANISM': 12,\n",
       " 'I-EXP_ASSAY': 13,\n",
       " 'B-EXP_ASSAY': 14}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = train_dataset.info.features['labels'].feature.num_classes\n",
    "label_list = train_dataset.info.features['labels'].feature.names\n",
    "id2label, label2id = {}, {}\n",
    "for class_, label in zip(range(num_labels), label_list):\n",
    "    id2label[class_] = label \n",
    "    label2id[label] = class_ \n",
    "print(f\"\\nTraining on {num_labels} features:\")\n",
    "print(\", \".join(label_list))\n",
    "id2label\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670cd35",
   "metadata": {},
   "source": [
    "Let us define now the metrics that will be used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a553f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.metrics import MetricsTOKCL\n",
    "compute_metrics = MetricsTOKCL(label_list=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f9394ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='./.tokcl_models', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='./.tokcl_models/runs/May19_08-48-32_21fe288ec657', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1000, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.EPOCH: 'epoch'>, save_steps=1, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='./.tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=True, resume_from_checkpoint=None, hub_model_id='EMBO/SourceData-NER', hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token='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', gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=None, replacement_probability=None, select_labels=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smtag.train.train_tokcl import TrainingArgumentsTOKCL\n",
    "\n",
    "training_args = TrainingArgumentsTOKCL(\n",
    "    output_dir = TOKCL_MODEL_PATH,\n",
    "    overwrite_output_dir = True,\n",
    "    logging_steps = 1000,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    prediction_loss_only = True,  # crucial to avoid OOM at evaluation stage!\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 10,\n",
    "    masking_probability = None,\n",
    "    replacement_probability = None,\n",
    "    select_labels = False,\n",
    "    per_gpu_train_batch_size=None, \n",
    "    per_gpu_eval_batch_size=None, \n",
    "    gradient_accumulation_steps=1, \n",
    "    eval_accumulation_steps=None, \n",
    "    weight_decay=0.0, \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.999, \n",
    "    adam_epsilon=1e-08, \n",
    "    max_grad_norm=1.0, \n",
    "    max_steps=-1, \n",
    "    lr_scheduler_type='linear', \n",
    "    warmup_ratio=0.0, \n",
    "    warmup_steps=0, \n",
    "    save_strategy='epoch', \n",
    "    save_steps=1, \n",
    "    save_total_limit=5, \n",
    "    save_on_each_node=False, \n",
    "    no_cuda=False, \n",
    "    seed=42, \n",
    "    bf16=False, \n",
    "    fp16=False, \n",
    "    fp16_opt_level='O1', \n",
    "    half_precision_backend='auto', \n",
    "    bf16_full_eval=False, \n",
    "    fp16_full_eval=False, \n",
    "    tf32=None, \n",
    "    local_rank=-1, \n",
    "    xpu_backend=None, \n",
    "    tpu_num_cores=None, \n",
    "    tpu_metrics_debug=False, \n",
    "    debug=[], \n",
    "    dataloader_drop_last=False, \n",
    "    eval_steps=1000, \n",
    "    dataloader_num_workers=0, \n",
    "    past_index=-1, \n",
    "    run_name=TOKCL_MODEL_PATH, \n",
    "    disable_tqdm=False, \n",
    "    remove_unused_columns=True, \n",
    "    label_names=None, \n",
    "    load_best_model_at_end=False, \n",
    "    metric_for_best_model=None, \n",
    "    greater_is_better=None, \n",
    "    ignore_data_skip=False, \n",
    "    sharded_ddp=[], \n",
    "    deepspeed=None, \n",
    "    label_smoothing_factor=0.0, \n",
    "    adafactor=False, \n",
    "    group_by_length=False, \n",
    "    length_column_name='length', \n",
    "    report_to=['tensorboard'], \n",
    "    ddp_find_unused_parameters=None, \n",
    "    ddp_bucket_cap_mb=None, \n",
    "    dataloader_pin_memory=True, \n",
    "    skip_memory_metrics=True, \n",
    "    use_legacy_prediction_loop=False, \n",
    "    push_to_hub=True, \n",
    "    resume_from_checkpoint=None, \n",
    "    hub_model_id=\"EMBO/SourceData-NER\", \n",
    "    hub_strategy='every_save', \n",
    "    hub_token=HUB_TOKEN, \n",
    "    gradient_checkpointing=False, \n",
    "    fp16_backend='auto', \n",
    "    mp_parameters=''\n",
    "    )\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc7e11",
   "metadata": {},
   "source": [
    "Up to now, we have pre-processed data and load a model. The loaded model has been cropped at the transformer network. For the model to be able to perform a task, we need to provide the model with a model head. \n",
    "\n",
    "The model heads are usually fully connected layers on the top of the transformer network. Although at this point we could perfectly use `torch` to build our own model from the output of the transformers, it has been shown that the performance of fully connected layers is at this point good enough to perform several NLP tasks, including NER.\n",
    "\n",
    "The reason is that the transformer models already encodes several context information on its resulting embeddings. We would therefore not benefit from generating a second RNN or conditional random fields on top, as it was usually done for NER. We will therefore keep it simple and use the fully connected network provided by ðŸ¤—.\n",
    "\n",
    "The way to do so is to load our model, but now using a different class: `AutoModelForTokenClassification`. In this case we use token classification since NER belongs to this task. \n",
    "\n",
    "We need to pass the number of labels. To avoid doing this for every single checkpoint we can do it programatically. The way of getting the number of classes from the training dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c756a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EMBO/bio-lm/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/09fed88b4a07fe6baced126e3cdb14f2764c1bc57f62d1026a75b3ffdb3ec5f8.c781727f43e25ac5b298f775b2dd4f32f53c9890a2367bbd99ffdbd856251b85\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-SMALL_MOLECULE\",\n",
      "    \"2\": \"B-SMALL_MOLECULE\",\n",
      "    \"3\": \"I-GENEPROD\",\n",
      "    \"4\": \"B-GENEPROD\",\n",
      "    \"5\": \"I-SUBCELLULAR\",\n",
      "    \"6\": \"B-SUBCELLULAR\",\n",
      "    \"7\": \"I-CELL\",\n",
      "    \"8\": \"B-CELL\",\n",
      "    \"9\": \"I-TISSUE\",\n",
      "    \"10\": \"B-TISSUE\",\n",
      "    \"11\": \"I-ORGANISM\",\n",
      "    \"12\": \"B-ORGANISM\",\n",
      "    \"13\": \"I-EXP_ASSAY\",\n",
      "    \"14\": \"B-EXP_ASSAY\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CELL\": 8,\n",
      "    \"B-EXP_ASSAY\": 14,\n",
      "    \"B-GENEPROD\": 4,\n",
      "    \"B-ORGANISM\": 12,\n",
      "    \"B-SMALL_MOLECULE\": 2,\n",
      "    \"B-SUBCELLULAR\": 6,\n",
      "    \"B-TISSUE\": 10,\n",
      "    \"I-CELL\": 7,\n",
      "    \"I-EXP_ASSAY\": 13,\n",
      "    \"I-GENEPROD\": 3,\n",
      "    \"I-ORGANISM\": 11,\n",
      "    \"I-SMALL_MOLECULE\": 1,\n",
      "    \"I-SUBCELLULAR\": 5,\n",
      "    \"I-TISSUE\": 9,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBO/bio-lm/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f61ee36e7d211df88e7eb67c937348fbd256ab3a970d6b3c9848181ed0330c27.12bb7cf77356bf912ed8598fb9a17becaa7e7c5406692af7264c45956cde2cf3\n",
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "            config.from_pretrained,\n",
    "            num_labels=num_labels,\n",
    "            max_position_embeddings=config.max_length + 2,  # max_length + 2 for start/end token\n",
    "            id2label = id2label,\n",
    "            label2id = label2id\n",
    "        )\n",
    "model_config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8881378f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"I-SMALL_MOLECULE\",\n",
      "    \"2\": \"B-SMALL_MOLECULE\",\n",
      "    \"3\": \"I-GENEPROD\",\n",
      "    \"4\": \"B-GENEPROD\",\n",
      "    \"5\": \"I-SUBCELLULAR\",\n",
      "    \"6\": \"B-SUBCELLULAR\",\n",
      "    \"7\": \"I-CELL\",\n",
      "    \"8\": \"B-CELL\",\n",
      "    \"9\": \"I-TISSUE\",\n",
      "    \"10\": \"B-TISSUE\",\n",
      "    \"11\": \"I-ORGANISM\",\n",
      "    \"12\": \"B-ORGANISM\",\n",
      "    \"13\": \"I-EXP_ASSAY\",\n",
      "    \"14\": \"B-EXP_ASSAY\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-CELL\": 8,\n",
      "    \"B-EXP_ASSAY\": 14,\n",
      "    \"B-GENEPROD\": 4,\n",
      "    \"B-ORGANISM\": 12,\n",
      "    \"B-SMALL_MOLECULE\": 2,\n",
      "    \"B-SUBCELLULAR\": 6,\n",
      "    \"B-TISSUE\": 10,\n",
      "    \"I-CELL\": 7,\n",
      "    \"I-EXP_ASSAY\": 13,\n",
      "    \"I-GENEPROD\": 3,\n",
      "    \"I-ORGANISM\": 11,\n",
      "    \"I-SMALL_MOLECULE\": 1,\n",
      "    \"I-SUBCELLULAR\": 5,\n",
      "    \"I-TISSUE\": 9,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=EMBO/SourceData-NER,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./.tokcl_models/runs/May19_08-48-32_21fe288ec657,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=None,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./.tokcl_models,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "replacement_probability=None,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./.tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=1,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining arguments for model type {config.model_type}:\")\n",
    "print(model_config)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265c760",
   "metadata": {},
   "source": [
    "#### Training Step 3: Define the `Trainer`\n",
    "\n",
    "We are ready now to define the [`Trainer` class](https://ðŸ¤—.co/docs/transformers/main_classes/trainer). This class is a basic training loop supporting a series of features defined in the documentation. However, it can be further customized. We encourage you to take a look to the documentation and try it. \n",
    "\n",
    "As it is, `trainer.train` would already train our model. However, it would offer only information about the loss during the process. We know that we want the loss to get smaller with time, and ideally, that this is true for both, training and validation datasets. Otherwise we would be incurring in overfitting.\n",
    "\n",
    "What if we want to see other information during training like the accuracy or f1 score? `Trainer` provides an argument `compute_metrics` that will help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12777102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             lfs_version = subprocess.run(\n\u001b[0m\u001b[1;32m    588\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m\"git-lfs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--version\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m             self.repo = Repository(\n\u001b[0m\u001b[1;32m   2599\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_git_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0;34m\"Looks like you do not have git-lfs installed, please install.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             lfs_version = subprocess.run(\n\u001b[0m\u001b[1;32m    588\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m\"git-lfs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--version\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_432/3585215316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoardCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# Create clone of distant repo and output directory if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;31m# In case of pull, we need to make sure every process has the latest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2605\u001b[0m                 \u001b[0;31m# Try again after wiping output_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m                 self.repo = Repository(\n\u001b[0m\u001b[1;32m   2608\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m                     \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_git_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m             ).stdout.strip()\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0;34m\"Looks like you do not have git-lfs installed, please install.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;34m\" You can install from https://git-lfs.github.com/.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from smtag.tb_callback import MyTensorBoardCallback\n",
    "import torch\n",
    "from smtag.show import ShowExampleTOKCL\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator_mask,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ShowExampleTOKCL(AutoTokenizer.from_pretrained(config.tokenizer))]\n",
    ")\n",
    "\n",
    "# switch the Tensorboard callback to plot losses on same plot\n",
    "trainer.remove_callback(TensorBoardCallback)  # remove default Tensorboard callback\n",
    "trainer.add_callback(MyTensorBoardCallback)  # replace with customized callback\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966254d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0466de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git: 'lfs' is not a git command. See 'git --help'.\r\n",
      "\r\n",
      "The most similar command is\r\n",
      "\tlog\r\n"
     ]
    }
   ],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aae17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
