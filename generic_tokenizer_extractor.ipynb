{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92687a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc4505c",
   "metadata": {},
   "source": [
    "Let us first replicate the information taken at `__init__` by the class `PreparatorTOKCL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.encoder import XMLEncoder\n",
    "import os\n",
    "from lxml.etree import fromstring, Element\n",
    "from smtag.config import Config\n",
    "from smtag.xml2labels import SourceDataCodes as sd\n",
    "from smtag.utils import innertext\n",
    "from smtag.xml2labels import CodeMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82c0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    max_length=512,  # in tokens! # sentence-level: 64, abstracts/full fig captions 512 tokens\n",
    "    from_pretrained=\"bert-base-cased\",  # leave empty if training a language model from scratch\n",
    "    model_type=\"Autoencoder\",\n",
    "    asynchr=True,  # we need ordered examples while async returns results in non deterministic way\n",
    "    tokenizer=\"bert-base-=cased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f56ee643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/data/xml/sd_panels/train.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dir_path = '/app/data/xml/sd_panels/'\n",
    "dest_dir_path = '/app/data/json/sd_panels/'\n",
    "max_length = config.max_length\n",
    "subsets = [\"train\", \"eval\", \"test\"]\n",
    "tokenizer = config.tokenizer\n",
    "code_maps = [sd.ENTITY_TYPES, sd.GENEPROD_ROLES, sd.BORING, sd.PANELIZATION]\n",
    "source_file_path = f\"{os.path.join(source_dir_path, subsets[0])}.txt\"\n",
    "source_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b261cf8",
   "metadata": {},
   "source": [
    "For developing and debugging purposes we will select a single line instead of the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2a688dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(source_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "line = lines[9411]\n",
    "\n",
    "xml_example = fromstring(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f23f0",
   "metadata": {},
   "source": [
    "We test here the `_encode_example` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae3c967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_encoder = XMLEncoder(xml_example)\n",
    "inner_text = innertext(xml_encoder.element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1634e58",
   "metadata": {},
   "source": [
    "`inner_text` represents the text of a given XML element. Using `code_maps` we can get the label for each character of `inner_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "60b12cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -> None\n",
      "E -> None\n",
      ") -> None\n",
      "  -> None\n",
      "s -> None\n",
      "i -> None\n",
      "R -> None\n",
      "N -> None\n",
      "A -> None\n",
      "  -> None\n",
      "k -> None\n",
      "n -> None\n",
      "o -> None\n",
      "c -> None\n",
      "k -> None\n",
      "d -> None\n",
      "o -> None\n",
      "w -> None\n",
      "n -> None\n",
      "  -> None\n",
      "o -> None\n",
      "f -> None\n",
      "  -> None\n",
      "P -> 2\n",
      "a -> 2\n",
      "r -> 2\n",
      "k -> 2\n",
      "i -> 2\n",
      "n -> 2\n",
      "  -> None\n",
      "w -> None\n",
      "a -> None\n",
      "s -> None\n",
      "  -> None\n",
      "p -> None\n",
      "e -> None\n",
      "r -> None\n",
      "f -> None\n",
      "o -> None\n",
      "r -> None\n",
      "m -> None\n",
      "e -> None\n",
      "d -> None\n",
      "  -> None\n",
      "i -> None\n",
      "n -> None\n",
      "  -> None\n",
      "S -> 4\n",
      "H -> 4\n",
      "- -> 4\n",
      "S -> 4\n",
      "Y -> 4\n",
      "5 -> 4\n",
      "Y -> 4\n",
      "  -> None\n",
      "c -> None\n",
      "e -> None\n",
      "l -> None\n",
      "l -> None\n",
      "s -> None\n",
      "  -> None\n",
      "f -> None\n",
      "o -> None\n",
      "l -> None\n",
      "l -> None\n",
      "o -> None\n",
      "w -> None\n",
      "e -> None\n",
      "d -> None\n",
      "  -> None\n",
      "b -> None\n",
      "y -> None\n",
      "  -> None\n",
      "2 -> None\n",
      "5 -> None\n",
      "  -> None\n",
      "μ -> None\n",
      "M -> None\n",
      "  -> None\n",
      "A -> None\n",
      "A -> None\n",
      "  -> None\n",
      "f -> None\n",
      "o -> None\n",
      "r -> None\n",
      "  -> None\n",
      "2 -> None\n",
      "  -> None\n",
      "h -> None\n",
      "o -> None\n",
      "u -> None\n",
      "r -> None\n",
      "s -> None\n",
      "  -> None\n",
      "p -> None\n",
      "r -> None\n",
      "i -> None\n",
      "o -> None\n",
      "r -> None\n",
      "  -> None\n",
      "t -> None\n",
      "o -> None\n",
      "  -> None\n",
      "f -> None\n",
      "i -> None\n",
      "x -> None\n",
      "a -> None\n",
      "t -> None\n",
      "i -> None\n",
      "o -> None\n",
      "n -> None\n",
      "  -> None\n",
      "a -> None\n",
      "n -> None\n",
      "d -> None\n",
      "  -> None\n",
      "i -> 7\n",
      "m -> 7\n",
      "m -> 7\n",
      "u -> 7\n",
      "n -> 7\n",
      "o -> 7\n",
      "s -> 7\n",
      "t -> 7\n",
      "a -> 7\n",
      "i -> 7\n",
      "n -> 7\n",
      "i -> 7\n",
      "n -> 7\n",
      "g -> 7\n",
      "  -> None\n",
      "w -> None\n",
      "i -> None\n",
      "t -> None\n",
      "h -> None\n",
      "  -> None\n",
      "a -> None\n",
      "n -> None\n",
      "t -> None\n",
      "i -> None\n",
      "b -> None\n",
      "o -> None\n",
      "d -> None\n",
      "i -> None\n",
      "e -> None\n",
      "s -> None\n",
      "  -> None\n",
      "s -> None\n",
      "p -> None\n",
      "e -> None\n",
      "c -> None\n",
      "i -> None\n",
      "f -> None\n",
      "i -> None\n",
      "c -> None\n",
      "  -> None\n",
      "t -> None\n",
      "o -> None\n",
      "  -> None\n",
      "T -> 2\n",
      "O -> 2\n",
      "M -> 2\n",
      "2 -> 2\n",
      "0 -> 2\n",
      "  -> None\n",
      "( -> None\n",
      "g -> None\n",
      "r -> None\n",
      "e -> None\n",
      "e -> None\n",
      "n -> None\n",
      ") -> None\n",
      ", -> None\n",
      "  -> None\n",
      "T -> 2\n",
      "o -> 2\n",
      "l -> 2\n",
      "l -> 2\n",
      "i -> 2\n",
      "p -> 2\n",
      "  -> None\n",
      "( -> None\n",
      "r -> None\n",
      "e -> None\n",
      "d -> None\n",
      ") -> None\n",
      "  -> None\n",
      "a -> None\n",
      "n -> None\n",
      "d -> None\n",
      "  -> None\n",
      "C -> 2\n",
      "y -> 2\n",
      "t -> 2\n",
      "o -> 2\n",
      "c -> 2\n",
      "h -> 2\n",
      "r -> 2\n",
      "o -> 2\n",
      "m -> 2\n",
      "e -> 2\n",
      "  -> 2\n",
      "c -> 2\n",
      "  -> None\n",
      "( -> None\n",
      "b -> None\n",
      "l -> None\n",
      "u -> None\n",
      "e -> None\n",
      ") -> None\n",
      ". -> None\n",
      "  -> None\n",
      "W -> None\n",
      "e -> None\n",
      "  -> None\n",
      "o -> None\n",
      "b -> None\n",
      "s -> None\n",
      "e -> None\n",
      "r -> None\n",
      "v -> None\n",
      "e -> None\n",
      "d -> None\n",
      "  -> None\n",
      "T -> 2\n",
      "o -> 2\n",
      "l -> 2\n",
      "l -> 2\n",
      "i -> 2\n",
      "p -> 2\n",
      "  -> None\n",
      "c -> 7\n",
      "o -> 7\n",
      "l -> 7\n",
      "o -> 7\n",
      "c -> 7\n",
      "a -> 7\n",
      "l -> 7\n",
      "i -> 7\n",
      "s -> 7\n",
      "a -> 7\n",
      "t -> 7\n",
      "i -> 7\n",
      "o -> 7\n",
      "n -> 7\n",
      "  -> None\n",
      "w -> None\n",
      "i -> None\n",
      "t -> None\n",
      "h -> None\n",
      "  -> None\n",
      "T -> 2\n",
      "O -> 2\n",
      "M -> 2\n",
      "2 -> 2\n",
      "0 -> 2\n",
      "+ -> None\n",
      "v -> None\n",
      "e -> None\n",
      "/ -> None\n",
      "P -> 3\n",
      "D -> 3\n",
      "H -> 3\n",
      "- -> None\n",
      "v -> None\n",
      "e -> None\n",
      "  -> None\n",
      "M -> 3\n",
      "D -> 3\n",
      "V -> 3\n",
      "s -> 3\n",
      "  -> None\n",
      "( -> None\n",
      "d -> None\n",
      "e -> None\n",
      "n -> None\n",
      "o -> None\n",
      "t -> None\n",
      "e -> None\n",
      "d -> None\n",
      "  -> None\n",
      "b -> None\n",
      "y -> None\n",
      "  -> None\n",
      "a -> None\n",
      "r -> None\n",
      "r -> None\n",
      "o -> None\n",
      "w -> None\n",
      "h -> None\n",
      "e -> None\n",
      "a -> None\n",
      "d -> None\n",
      "s -> None\n",
      ") -> None\n",
      ", -> None\n",
      "  -> None\n",
      "w -> None\n",
      "h -> None\n",
      "i -> None\n",
      "c -> None\n",
      "h -> None\n",
      "  -> None\n",
      "w -> None\n",
      "a -> None\n",
      "s -> None\n",
      "  -> None\n",
      "s -> None\n",
      "t -> None\n",
      "i -> None\n",
      "l -> None\n",
      "l -> None\n",
      "  -> None\n",
      "m -> None\n",
      "a -> None\n",
      "i -> None\n",
      "n -> None\n",
      "t -> None\n",
      "a -> None\n",
      "i -> None\n",
      "n -> None\n",
      "e -> None\n",
      "d -> None\n",
      "  -> None\n",
      "f -> None\n",
      "o -> None\n",
      "l -> None\n",
      "l -> None\n",
      "o -> None\n",
      "w -> None\n",
      "i -> None\n",
      "n -> None\n",
      "g -> None\n",
      "  -> None\n",
      "P -> 2\n",
      "a -> 2\n",
      "r -> 2\n",
      "k -> 2\n",
      "i -> 2\n",
      "n -> 2\n",
      "  -> None\n",
      "s -> None\n",
      "i -> None\n",
      "R -> None\n",
      "N -> None\n",
      "A -> None\n",
      "  -> None\n",
      "k -> None\n",
      "n -> None\n",
      "o -> None\n",
      "c -> None\n",
      "k -> None\n",
      "d -> None\n",
      "o -> None\n",
      "w -> None\n",
      "n -> None\n",
      ". -> None\n",
      "  -> None\n"
     ]
    }
   ],
   "source": [
    "for c, l in zip(inner_text, xml_encoder.encode(code_maps[0])['label_ids']):\n",
    "    print(f\"{c} -> {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36105a7",
   "metadata": {},
   "source": [
    "The next step now is to put the characters together in words. At the same time, the labels must also be generated on a way that each label belongs to each word.\n",
    "\n",
    "We do the same for each of the code maps, with the exception of `panel_start` which gets a bit of a different treatment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a842e56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_types [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 4, 4, 4, 4, 4, 4, 4, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, None, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, None, None, None, None, None, None, 2, 2, 2, 2, 2, None, None, None, None, 3, 3, 3, None, None, None, None, 3, 3, 3, 3, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "geneprod_roles [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 2, 2, 2, 2, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "boring [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "panel_start ['B-PANEL_START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for code_map in code_maps:\n",
    "    # At this point we have a tag for each character.\n",
    "    # It is here where I should put chars together into words\n",
    "    words, label_words = [], []\n",
    "    xml_encoded = xml_encoder.encode(code_map)\n",
    "    if code_map.name != \"panel_start\":\n",
    "        char_level_labels = xml_encoded['label_ids']\n",
    "    else:\n",
    "        char_level_labels = [\"O\"] * len(xml_encoded['label_ids'])\n",
    "        offsets = xml_encoded[\"offsets\"]\n",
    "        for offset in offsets:\n",
    "            char_level_labels[offset[0]] = \"B-PANEL_START\"\n",
    "    print(code_map.name, char_level_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "150f5da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SourceDataCodes.ENTITY_TYPES: CodeMap(name='entity_types', mode='whole_entity', constraints=OrderedDict([(1, {'label': 'SMALL_MOLECULE', 'tag': 'sd-tag', 'attributes': {'entity_type': ['molecule']}}), (2, {'label': 'GENEPROD', 'tag': 'sd-tag', 'attributes': {'entity_type': ['geneprod', 'gene', 'protein']}}), (3, {'label': 'SUBCELLULAR', 'tag': 'sd-tag', 'attributes': {'entity_type': ['subcellular']}}), (4, {'label': 'CELL', 'tag': 'sd-tag', 'attributes': {'entity_type': ['cell']}}), (5, {'label': 'TISSUE', 'tag': 'sd-tag', 'attributes': {'entity_type': ['tissue']}}), (6, {'label': 'ORGANISM', 'tag': 'sd-tag', 'attributes': {'entity_type': ['organism']}}), (7, {'label': 'EXP_ASSAY', 'tag': 'sd-tag', 'attributes': {'category': ['assay']}})]))>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.ENTITY_TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a858ab",
   "metadata": {},
   "source": [
    "The next step is to go from text on a character level basis to text on a word-level basis. This is done through the method `_from_char_to_token_level_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97880a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def _labels_to_iob2(code_map: CodeMap, words: List[str], labels: List) -> List:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        code_map (CodeMap): CodeMap, each specifying The XML-to-code mapping of label codes\n",
    "                            to specific combinations of tag name and attribute values.\n",
    "        text List[str]:     List of separated words\n",
    "        labels List:        List of labels for each word inside the XML elements.\n",
    "\n",
    "    Returns:\n",
    "        List[str]           Word-level tokenized labels in IOB2 format\n",
    "\n",
    "    \"\"\"\n",
    "    iob2_labels = []\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        if code_map.name == \"panel_start\":\n",
    "            iob2_labels.append(\"O\")\n",
    "\n",
    "        if code_map.name != \"panel_start\":\n",
    "            if label == \"O\":\n",
    "                iob2_labels.append(label)\n",
    "\n",
    "            if label != \"O\":\n",
    "                if idx == 0:\n",
    "                    iob2_labels.append(code_map.iob2_labels[int(label) * 2])\n",
    "                if (idx > 0) and (labels[idx - 1] != label):\n",
    "                    iob2_labels.append(code_map.iob2_labels[int(label) * 2])\n",
    "                if (idx > 0) and (labels[idx - 1] == label):\n",
    "                    iob2_labels.append(code_map.iob2_labels[int(label) * 2 - 1])\n",
    "\n",
    "    return iob2_labels\n",
    "\n",
    "\n",
    "def _from_char_to_token_level_labels(code_map: CodeMap, text: List[str], labels: List[str]) -> List:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        code_map (CodeMap): CodeMap, each specifying Tthe XML-to-code mapping of label codes\n",
    "                            to specific combinations of tag name and attribute values.\n",
    "        text List[str]:     List of the characters inside the text of the XML elements\n",
    "        labels List:        List of labels for each character inside the XML elements. They will be\n",
    "                            a mix of integers and None\n",
    "\n",
    "    Returns:\n",
    "        List[str]           Word-level tokenized labels for the input text\n",
    "    \"\"\"\n",
    "\n",
    "    word, label_word = '', ''\n",
    "    word_level_words, word_level_labels = [], []\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char.isalnum():\n",
    "            word += char\n",
    "            label_word += str(labels[i]).replace(\"None\", \"O\")\n",
    "        elif char == \" \":\n",
    "            if word not in [\"\"]:\n",
    "                word_level_words.append(word)\n",
    "                word_level_labels.append(label_word[0])\n",
    "            word = ''\n",
    "            label_word = ''\n",
    "        else:\n",
    "            if word not in [\"\"]:\n",
    "                word_level_words.append(word)\n",
    "                word_level_labels.append(label_word[0])\n",
    "\n",
    "            word_level_words.append(char)\n",
    "            word_level_labels.append(str(labels[i]).replace(\"None\", \"O\"))\n",
    "            word = ''\n",
    "            label_word = ''\n",
    "\n",
    "    word_level_iob2_labels = _labels_to_iob2(code_map, word_level_words, word_level_labels)\n",
    "    assert len(word_level_words) == len(word_level_iob2_labels), \"Length of labels and words not identical!\"\n",
    "    return word_level_words, word_level_iob2_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7c3b3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( O\n",
      "E O\n",
      ") O\n",
      "siRNA O\n",
      "knockdown O\n",
      "of O\n",
      "Parkin B-GENEPROD\n",
      "was O\n",
      "performed O\n",
      "in O\n",
      "SH B-CELL\n",
      "- I-CELL\n",
      "SY5Y I-CELL\n",
      "cells O\n",
      "followed O\n",
      "by O\n",
      "25 O\n",
      "μM O\n",
      "AA O\n",
      "for O\n",
      "2 O\n",
      "hours O\n",
      "prior O\n",
      "to O\n",
      "fixation O\n",
      "and O\n",
      "immunostaining B-EXP_ASSAY\n",
      "with O\n",
      "antibodies O\n",
      "specific O\n",
      "to O\n",
      "TOM20 B-GENEPROD\n",
      "( O\n",
      "green O\n",
      ") O\n",
      ", O\n",
      "Tollip B-GENEPROD\n",
      "( O\n",
      "red O\n",
      ") O\n",
      "and O\n",
      "Cytochrome B-GENEPROD\n",
      "c I-GENEPROD\n",
      "( O\n",
      "blue O\n",
      ") O\n",
      ". O\n",
      "We O\n",
      "observed O\n",
      "Tollip B-GENEPROD\n",
      "colocalisation B-EXP_ASSAY\n",
      "with O\n",
      "TOM20 B-GENEPROD\n",
      "+ O\n",
      "ve O\n",
      "/ O\n",
      "PDH B-SUBCELLULAR\n",
      "- O\n",
      "ve O\n",
      "MDVs B-SUBCELLULAR\n",
      "( O\n",
      "denoted O\n",
      "by O\n",
      "arrowheads O\n",
      ") O\n",
      ", O\n",
      "which O\n",
      "was O\n",
      "still O\n",
      "maintained O\n",
      "following O\n",
      "Parkin B-GENEPROD\n",
      "siRNA O\n",
      "knockdown O\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "code_map = sd.ENTITY_TYPES\n",
    "words, iob_labels = _from_char_to_token_level_labels(code_map, inner_text, xml_encoder.encode(code_map)[\"label_ids\"])\n",
    "for w,l in zip(words, iob_labels):\n",
    "    print(w, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03380a",
   "metadata": {},
   "source": [
    " At this point we have the characters orders in words. Note that we treat words as consecutive strings of alphanumeric characters. Non alpha numeric characters will be part of different words. This is ok since the tokenizer id for non alphanumeric characters is not going to be different from them being alone or attached to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0284598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open('/data/json/sd_panels/train.jsonl') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "baa34605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( O\n",
      "A O\n",
      ") O\n",
      "Representative O\n",
      "fluorescence B-EXP_ASSAY\n",
      "images I-EXP_ASSAY\n",
      "of O\n",
      "Mock O\n",
      ", O\n",
      "SARS B-ORGANISM\n",
      "- I-ORGANISM\n",
      "CoV I-ORGANISM\n",
      "- I-ORGANISM\n",
      "2 I-ORGANISM\n",
      "infected O\n",
      "( O\n",
      "Ctrl O\n",
      ") O\n",
      ", O\n",
      "and O\n",
      "Salmeterol B-SMALL_MOLECULE\n",
      "- O\n",
      "treated O\n",
      "wells O\n",
      "analyzed O\n",
      "with O\n",
      "the O\n",
      "Multiwavelength O\n",
      "Cell O\n",
      "Scoring O\n",
      "application O\n",
      "in O\n",
      "MetaXpress O\n",
      ". O\n",
      "Grayscales O\n",
      "of O\n",
      "the O\n",
      "images B-EXP_ASSAY\n",
      "were O\n",
      "adjusted O\n",
      "to O\n",
      "enable O\n",
      "direct O\n",
      "comparison O\n",
      "of O\n",
      "the O\n",
      "relative O\n",
      "levels B-EXP_ASSAY\n",
      "of O\n",
      "fluorescence B-EXP_ASSAY\n",
      "among O\n",
      "the O\n",
      "treatments O\n",
      ": O\n",
      "Segmentation O\n",
      "images B-EXP_ASSAY\n",
      "show O\n",
      "how O\n",
      "cells O\n",
      "were O\n",
      "segmented O\n",
      "and O\n",
      "identified O\n",
      "as O\n",
      "spike B-GENEPROD\n",
      "positive O\n",
      ". O\n",
      "Purple O\n",
      ", O\n",
      "nuclei B-SUBCELLULAR\n",
      "; O\n",
      "cyan O\n",
      ", O\n",
      "spike B-GENEPROD\n",
      ". O\n",
      "Scale O\n",
      "bar O\n",
      ", O\n",
      "100 O\n",
      "μm O\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for (w, l) in zip(data[215]['words'], data[215]['label_ids']['entity_types']):\n",
    "    print(w, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ad0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b423d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d769a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772ec91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505e49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb523cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c2db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec28cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db7a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b3e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d00b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaTokenizerForListOfStrings:\n",
    "    \"\"\"\n",
    "    Uses tokenizers that work with lists of strings\n",
    "    to be able to properly run the roberta tokenizer.\n",
    "    Arguments:\n",
    "    ----------\n",
    "    tokenizer: `PreTrainedTokenizerFast`\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\"bert-base-cased\")):\n",
    "        self.roberta = AutoTokenizer.from_pretrained(\"roberta-base\", is_pretokenized=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        encoded_batch = self.tokenizer.batch_encode_plus(batch, is_split_into_words=True)\n",
    "        batch_plain_string = self.tokenizer.batch_decode(encoded_batch[\"input_ids\"], skip_special_tokens=True)\n",
    "        roberta_batch = self.roberta.batch_encode_plus(batch_plain_string)\n",
    "        return roberta_batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES  = [\"This is just a simple test\",\n",
    "              \"What about putting a-symbol like this?\"]\n",
    "SENTENCE_SPLIT = [sentence.split() for sentence in SENTENCES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ = RobertaTokenizerForListOfStrings()\n",
    "assert class_(SENTENCE_SPLIT) == roberta(SENTENCES), f\"Something went wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b514758",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SENTENCES[-1])\n",
    "encoded_batch = bert.batch_encode_plus([SENTENCES[-1].split()], is_split_into_words=True)\n",
    "bert.batch_decode(encoded_batch[\"input_ids\"], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbd301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = AutoTokenizer.from_pretrained(\"roberta-base\", is_pretokenized=True, add_prefix_space=True)\n",
    "bert = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta(\"Wha -t -about- pu-tting a - symbol like this? or here ? { } \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.decode([0, 14447, 102, 111, 90, 111, 9006, 12, 18829, 12, 90, 2577, 10, 111, 7648, 101, 42, 116, 50, 259, 17487, 25522, 35524, 1437, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a73ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta(\"Wha -t -about- pu-tting a - symbol like this? or here ? { } \".split(), is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta.decode([0, 653, 59, 2057, 10, 12, 7648, 101, 42, 116, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert(\"What about putting a- symbol like this?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5217c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.decode([101, 1327, 1164, 4518, 170, 118, 5961, 1176, 1142, 136, 102], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99544f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"EMBO/sd-nlp-non-tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ebed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", is_pretokenized=True, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _align_labels_with_tokens(labels, word_ids, word_labels):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def _shift_label(label):\n",
    "    # If the label is B-XXX we change it to I-XX\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "def _tokenize_and_align_labels(examples) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Tokenizes data split into words into sub-token tokenization parts.\n",
    "    Args:\n",
    "        examples: batch of data from a `datasets.DatasetDict`\n",
    "\n",
    "    Returns:\n",
    "        `datasets.DatasetDict` with entries tokenized to the `AutoTokenizer`\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples['words'],\n",
    "                                      truncation=True,\n",
    "                                      is_split_into_words=True,\n",
    "                                      max_length=512)\n",
    "\n",
    "    all_labels = examples['labels']\n",
    "    new_labels = []\n",
    "    tag_mask = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(_align_labels_with_tokens(labels, word_ids, all_labels))\n",
    "        tag_mask.append([0 if tag == 0 else 1 for tag in new_labels[-1]])\n",
    "\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    tokenized_inputs['tag_mask'] = tag_mask\n",
    "\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11b758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be2601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = ds.map(\n",
    "                _tokenize_and_align_labels,\n",
    "                batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ccf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(ds['train'][2]['words'],\n",
    "                                  truncation=True,\n",
    "                                  is_split_into_words=True,)\n",
    "labels = ds['train'][2]['labels']\n",
    "words = ds['train'][2]['words']\n",
    "\n",
    "for i in range(len(tokenized_inputs.word_ids())):\n",
    "    print(tokenized_inputs.word_ids()[i],\n",
    "         _align_labels_with_tokens(labels, tokenized_inputs.word_ids())[i],\n",
    "         tokenized_inputs['input_ids'][i],\n",
    "         tokenizer.decode(tokenized_inputs['input_ids'][i]))\n",
    "# new_labels.append(_align_labels_with_tokens(labels, word_ids))\n",
    "# tag_mask.append([0 if tag == 0 else 1 for tag in new_labels[-1]])\n",
    "# print(i, tokenized_inputs.word_ids(i), ds['train'][0]['words'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65462dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_data['train'][2][\"labels\"])):\n",
    "    print(tokenized_data['train'][2][\"labels\"][i], tokenizer.decode(tokenized_data['train'][2]['input_ids'][i]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad778c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", is_pretokenized=True, add_prefix_space=False)\n",
    "tokenized_data = ds.map(\n",
    "                _tokenize_and_align_labels,\n",
    "                batched=True)\n",
    "for i in range(len(tokenized_data['train'][2][\"labels\"])):\n",
    "    print(tokenized_data['train'][2][\"labels\"][i], tokenizer.decode(tokenized_data['train'][0]['input_ids'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ds['train'][0][\"labels\"])):\n",
    "    print(ds['train'][0][\"words\"][i], ds['train'][0]['labels'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base', \n",
    "                                          use_fast=True, \n",
    "                                          is_pretokenized=True, \n",
    "                                          add_prefix_space=True)\n",
    "enc = tokenizer(ds['train'][0]['words'], is_split_into_words=True)\n",
    "enc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce5eb395",
   "metadata": {},
   "source": [
    "enc.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0327667",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.word_to_tokens(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b5fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test1:\n",
    "    def __init__(self,\n",
    "                arg_1_1: int = 1,\n",
    "                arg_1_2: int = 2,\n",
    "                arg_1_3: int = 3,\n",
    "                arg_1_4: int = 4,\n",
    "                ):\n",
    "        self.arg_1_1 = arg_1_1\n",
    "        self.arg_1_2 = arg_1_2\n",
    "        self.arg_1_3 = arg_1_3\n",
    "        self.arg_1_4 = arg_1_4\n",
    "        \n",
    "class test2(test1):\n",
    "    def __init__(self, \n",
    "                 arg_2=10,\n",
    "                 **kw):\n",
    "        self.arg2 = arg_2\n",
    "        super(test2, self).__init__(**kw)\n",
    "    \n",
    "t2 = test2(arg_1_1=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c94ae44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.arg_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6954924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ray.tune.progress_reporter.CLIReporter"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.tune import CLIReporter\n",
    "\n",
    "type(CLIReporter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10801c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
