{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9e49d4",
   "metadata": {},
   "source": [
    "## Visualization of the panelizing results\n",
    "\n",
    "This notebook shows the results of the panelization model generated as part of the paper. \n",
    "\n",
    "Scroll to the end of the notebook to see some results. You can also use your own panels to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e9289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.22.2)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (64.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.19.3)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.10.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75d8bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 09:51:41.982432: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-07 09:51:42.151452: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-07 09:51:42.978050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/compat/lib.real:/opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-10-07 09:51:42.978157: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/compat/lib.real:/opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-10-07 09:51:42.978168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from typing import Optional, List, Tuple, Union, Any, Dict\n",
    "\n",
    "from transformers import (AutoModelForTokenClassification, AutoTokenizer,\n",
    "                         pipeline, Pipeline\n",
    "                         )\n",
    "from transformers.pipelines.base import ChunkPipeline\n",
    "from transformers.pipelines.token_classification import (TokenClassificationArgumentHandler, \n",
    "                                                         TokenClassificationPipeline, AggregationStrategy )\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
    "from transformers.models.bert.tokenization_bert import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bcdcddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EMBO/sd-panelization-v2\", \n",
    "                                          model_max_length=512,\n",
    "                                         add_prefix_space=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"EMBO/sd-panelization-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09596266",
   "metadata": {},
   "source": [
    "## Pipeline to tag panels in sentences > 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018ef5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongTextTokenClassificationPipeline(ChunkPipeline):\n",
    "    \"\"\"\n",
    "    Named Entity Recognition pipeline using any `ModelForTokenClassification`. See the [named entity recognition\n",
    "    examples](../task_summary#named-entity-recognition) for more information.\n",
    "    Strings of any length can be passed. If they exceed `ModelForTokenClassification.config.max_position_embeddings` tokens,\n",
    "    they will be divided into several parts text that will be passed to the `forward` method.\n",
    "    The results will then be concatenated together and be sent back.\n",
    "    *LongTextTokenClassificationPipeline* uses `offsets_mapping` and therefore is available only with `FastTokenizer`.\n",
    "    The models that this pipeline can use are models that have been fine-tuned on a token classification task. See the\n",
    "    up-to-date list of available models on\n",
    "    [huggingface.co/models](https://huggingface.co/models?filter=token-classification).\n",
    "    \"\"\"    \n",
    "    default_input_names = \"sequences\"\n",
    "\n",
    "    def __init__(self, args_parser=TokenClassificationArgumentHandler(), *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.check_model_type(\n",
    "            TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
    "            if self.framework == \"tf\"\n",
    "            else MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
    "        )\n",
    "\n",
    "        self._basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "        self._args_parser = args_parser\n",
    "        if not self.tokenizer.is_fast:\n",
    "            raise TypeError(\n",
    "            \"\"\"LongTextTokenClassificationPipeline works only with fast tokenizers.\n",
    "            Please choose a fast tokenizer.\"\"\"\n",
    "            )\n",
    "\n",
    "    def _sanitize_parameters(\n",
    "        self,\n",
    "        ignore_labels=None,\n",
    "        grouped_entities: Optional[bool] = None,\n",
    "        ignore_subwords: Optional[bool] = None,\n",
    "        aggregation_strategy: Optional[AggregationStrategy] = None,\n",
    "        offset_mapping: Optional[List[Tuple[int, int]]] = None,\n",
    "        stride: Optional[int] = None,\n",
    "    ):\n",
    "\n",
    "        preprocess_params = {}\n",
    "        if offset_mapping is not None:\n",
    "            preprocess_params[\"offset_mapping\"] = offset_mapping\n",
    "\n",
    "        postprocess_params = {}\n",
    "        if grouped_entities is not None or ignore_subwords is not None:\n",
    "            if grouped_entities and ignore_subwords:\n",
    "                aggregation_strategy = AggregationStrategy.FIRST\n",
    "            elif grouped_entities and not ignore_subwords:\n",
    "                aggregation_strategy = AggregationStrategy.SIMPLE\n",
    "            else:\n",
    "                aggregation_strategy = AggregationStrategy.NONE\n",
    "\n",
    "            if grouped_entities is not None:\n",
    "                warnings.warn(\n",
    "                    \"`grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to\"\n",
    "                    f' `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n",
    "                )\n",
    "            if ignore_subwords is not None:\n",
    "                warnings.warn(\n",
    "                    \"`ignore_subwords` is deprecated and will be removed in version v5.0.0, defaulted to\"\n",
    "                    f' `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n",
    "                )\n",
    "\n",
    "        if aggregation_strategy is not None:\n",
    "            if isinstance(aggregation_strategy, str):\n",
    "                aggregation_strategy = AggregationStrategy[aggregation_strategy.upper()]\n",
    "            if (\n",
    "                aggregation_strategy\n",
    "                in {AggregationStrategy.FIRST, AggregationStrategy.MAX, AggregationStrategy.AVERAGE}\n",
    "                and not self.tokenizer.is_fast\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Slow tokenizers cannot handle subwords. Please set the `aggregation_strategy` option\"\n",
    "                    'to `\"simple\"` or use a fast tokenizer.'\n",
    "                )\n",
    "            postprocess_params[\"aggregation_strategy\"] = aggregation_strategy\n",
    "        if ignore_labels is not None:\n",
    "            postprocess_params[\"ignore_labels\"] = ignore_labels\n",
    "            \n",
    "            \n",
    "        if stride is not None:\n",
    "            if not isinstance(stride, int): \n",
    "                raise TypeError(\n",
    "                    f\"Strides must be of type `int`. {type(stride)} was given.\"\n",
    "                )\n",
    "            postprocess_params[\"stride\"] = stride\n",
    "            preprocess_params[\"stride\"] = stride\n",
    "            \n",
    "        return preprocess_params, {}, postprocess_params\n",
    "    \n",
    "    def __call__(self, inputs: Union[str, List[str]], **kwargs):\n",
    "        \"\"\"\n",
    "        Classify each token of the text(s) given as inputs.\n",
    "        Args:\n",
    "            inputs (`str` or `List[str]`):\n",
    "                One or several texts (or one list of texts) for token classification.\n",
    "        Return:\n",
    "            A list or a list of list of `dict`: Each result comes as a list of dictionaries (one for each token in the\n",
    "            corresponding input, or each entity if this pipeline was instantiated with an aggregation_strategy) with\n",
    "            the following keys:\n",
    "            - **word** (`str`) -- The token/word classified. This is obtained by decoding the selected tokens. If you\n",
    "              want to have the exact string in the original sentence, use `start` and `stop`.\n",
    "            - **score** (`float`) -- The corresponding probability for `entity`.\n",
    "            - **entity** (`str`) -- The entity predicted for that token/word (it is named *entity_group* when\n",
    "              *aggregation_strategy* is not `\"none\"`.\n",
    "            - **index** (`int`, only present when `aggregation_strategy=\"none\"`) -- The index of the corresponding\n",
    "              token in the sentence.\n",
    "            - **start** (`int`, *optional*) -- The index of the start of the corresponding entity in the sentence. Only\n",
    "              exists if the offsets are available within the tokenizer\n",
    "            - **end** (`int`, *optional*) -- The index of the end of the corresponding entity in the sentence. Only\n",
    "              exists if the offsets are available within the tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        _inputs, offset_mapping = self._args_parser(inputs, **kwargs)\n",
    "        if offset_mapping:\n",
    "            kwargs[\"offset_mapping\"] = offset_mapping\n",
    "            \n",
    "        return super().__call__(inputs, **kwargs)\n",
    "\n",
    "    def preprocess(self, sentence, offset_mapping=None, stride=0):\n",
    "        truncation = False\n",
    "        \n",
    "        model_inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=None,\n",
    "            truncation=truncation,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_offsets_mapping=self.tokenizer.is_fast,\n",
    "        )\n",
    "\n",
    "#         sentence_chunks = self._get_sentence_chunks(model_inputs[\"input_ids\"], stride)\n",
    "                        \n",
    "        if offset_mapping:\n",
    "            model_inputs[\"offset_mapping\"] = offset_mapping\n",
    "                    \n",
    "        model_inputs[\"sentence\"] = sentence\n",
    "        \n",
    "        idx_lookup = list(range(len(model_inputs[\"input_ids\"])))[1:-1]\n",
    "        first_token = 0\n",
    "        bos_token = model_inputs[\"input_ids\"][0]\n",
    "        eos_token = model_inputs[\"input_ids\"][-1]\n",
    "        \n",
    "        chunk_inputs = {}\n",
    "        \n",
    "        while first_token < len(idx_lookup):\n",
    "            start = max(0,first_token-stride)\n",
    "            end = min(start + self.model.config.max_length - 2, len(idx_lookup))\n",
    "            \n",
    "            chunk_inputs[\"input_ids\"] = self._to_tensor(\n",
    "                [bos_token] + model_inputs[\"input_ids\"][1:-1][start:end] + [eos_token]\n",
    "                )\n",
    "            chunk_inputs[\"token_type_ids\"] = self._to_tensor(\n",
    "                [0] + model_inputs[\"token_type_ids\"][1:-1][start:end] + [0]\n",
    "                )\n",
    "            chunk_inputs[\"attention_mask\"] = self._to_tensor(\n",
    "                [1] + model_inputs[\"attention_mask\"][1:-1][start:end] + [1]\n",
    "                )\n",
    "            chunk_inputs[\"special_tokens_mask\"] = self._to_tensor(\n",
    "                [1] + model_inputs[\"special_tokens_mask\"][1:-1][start:end] + [1]\n",
    "                )\n",
    "            chunk_inputs[\"offset_mapping\"] = [(0,0)] + model_inputs[\"offset_mapping\"][1:-1][start:end] + [(0,0)]\n",
    "            chunk_inputs[\"chunk_sentence\"] = tokenizer.decode(chunk_inputs[\"input_ids\"][0])\n",
    "            chunk_inputs[\"sentence\"] = sentence\n",
    "            \n",
    "            first_token = end\n",
    "                        \n",
    "            yield {**chunk_inputs}\n",
    "            \n",
    "    def _forward(self, chunk_inputs: Dict[str, Any]) -> List[dict]:\n",
    "        # Forward\n",
    "        special_tokens_mask = chunk_inputs.pop(\"special_tokens_mask\")\n",
    "        offset_mapping = chunk_inputs.pop(\"offset_mapping\", None)\n",
    "        sentence = chunk_inputs.pop(\"sentence\")\n",
    "        chunk_sentence = chunk_inputs.pop(\"chunk_sentence\")\n",
    "        if self.framework == \"tf\":\n",
    "            logits = self.model(chunk_inputs.data)[0]\n",
    "        else:\n",
    "            logits = self.model(**chunk_inputs)[0]\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"special_tokens_mask\": special_tokens_mask,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"sentence\": sentence,\n",
    "            \"chunk_sentence\": chunk_sentence,\n",
    "            **chunk_inputs,\n",
    "        }\n",
    "    \n",
    "    def postprocess(self, model_outputs: List[Dict[str, Any]], \n",
    "                    aggregation_strategy=AggregationStrategy.NONE, \n",
    "                    ignore_labels=None, \n",
    "                    stride=0):\n",
    "        sentence = model_outputs[0][\"sentence\"]\n",
    "        aggregated_tokenizer_outputs = tokenizer(sentence,\n",
    "            return_tensors=self.framework,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_offsets_mapping=self.tokenizer.is_fast,\n",
    "        )\n",
    "        input_ids = aggregated_tokenizer_outputs[\"input_ids\"]\n",
    "        offset_mapping = aggregated_tokenizer_outputs[\"offset_mapping\"]\n",
    "        special_tokens_mask = aggregated_tokenizer_outputs[\"special_tokens_mask\"]\n",
    "        \n",
    "        logits = self._aggregate_chunk_outputs(model_outputs, stride)\n",
    "        \n",
    "        if ignore_labels is None:\n",
    "            ignore_labels = [\"O\"]\n",
    "        logits = logits.numpy()\n",
    "        input_ids = input_ids[0]\n",
    "        offset_mapping = offset_mapping[0] if offset_mapping is not None else None\n",
    "        special_tokens_mask = special_tokens_mask[0].numpy()\n",
    "\n",
    "        maxes = np.max(logits, axis=-1, keepdims=True)\n",
    "        shifted_exp = np.exp(logits - maxes)\n",
    "        scores = shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\n",
    "        \n",
    "        pre_entities = self.gather_pre_entities(\n",
    "            sentence, input_ids, scores, offset_mapping, special_tokens_mask, aggregation_strategy\n",
    "        )\n",
    "        grouped_entities = self.aggregate(pre_entities, aggregation_strategy)\n",
    "        # Filter anything that is in self.ignore_labels\n",
    "        entities = [\n",
    "            entity\n",
    "            for entity in grouped_entities\n",
    "            if entity.get(\"entity\", None) not in ignore_labels\n",
    "            and entity.get(\"entity_group\", None) not in ignore_labels\n",
    "        ]\n",
    "        return entities                \n",
    "        \n",
    "    def _to_tensor(self, inputs: List[Any]) -> Union[tf.Tensor, torch.tensor, np.ndarray]:\n",
    "        if self.framework == \"pt\":\n",
    "            return torch.tensor(inputs).unsqueeze(0)\n",
    "        if self.framework == \"tf\":\n",
    "            return tf.reshape(tf.convert_to_tensor(inputs), (1,-1))\n",
    "        if self.framework == \"np\":\n",
    "            return np.array(inputs).reshape(1,-1)\n",
    "\n",
    "    def _aggregate_chunk_outputs(self, outputs: \n",
    "                                 List[Dict[str, Any]], \n",
    "                                 stride: int) -> Union[tf.Tensor, torch.tensor, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Change this to numpy or lits to save cuda space\n",
    "        \"\"\"\n",
    "        for iter_, chunk_output in enumerate(outputs):\n",
    "            is_first = (iter_ == 0)\n",
    "            is_last = (iter_ == len(outputs)-1)\n",
    "            if is_first:\n",
    "                logits = chunk_output[\"logits\"][0][:-1]\n",
    "            elif is_last:\n",
    "                logits = self._concat(logits, chunk_output[\"logits\"][0][stride+1:])\n",
    "            else:\n",
    "                logits = self._concat(logits, chunk_output[\"logits\"][0][stride+1:-1])\n",
    "                \n",
    "        return logits\n",
    "            \n",
    "    def _concat(self, \n",
    "                 t1: Union[tf.Tensor, torch.tensor, np.ndarray],\n",
    "                 t2: Union[tf.Tensor, torch.tensor, np.ndarray],\n",
    "                 axis: int  = 0\n",
    "                ) -> Union[tf.Tensor, torch.tensor, np.ndarray]:\n",
    "        if self.framework == \"pt\":\n",
    "            concat = torch.concat([t1, t2], axis=axis)\n",
    "        if self.framework == \"tf\":\n",
    "            concat = tf.concat([t1, t2], axis=axis)\n",
    "        if self.framework == \"np\":\n",
    "            concat = np.concatenate([t1, t2], axis=axis)\n",
    "        return concat\n",
    "    \n",
    "    def gather_pre_entities(\n",
    "        self,\n",
    "        sentence: str,\n",
    "        input_ids: np.ndarray,\n",
    "        scores: np.ndarray,\n",
    "        offset_mapping: Optional[List[Tuple[int, int]]],\n",
    "        special_tokens_mask: np.ndarray,\n",
    "        aggregation_strategy: AggregationStrategy,\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\"\n",
    "        pre_entities = []\n",
    "        for idx, token_scores in enumerate(scores):\n",
    "            # Filter special_tokens, they should only occur\n",
    "            # at the sentence boundaries since we're not encoding pairs of\n",
    "            # sentences so we don't have to keep track of those.\n",
    "            if special_tokens_mask[idx]:\n",
    "                continue\n",
    "\n",
    "            word = self.tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n",
    "            if offset_mapping is not None:\n",
    "                start_ind, end_ind = offset_mapping[idx]\n",
    "                if not isinstance(start_ind, int):\n",
    "                    if self.framework == \"pt\":\n",
    "                        start_ind = start_ind.item()\n",
    "                        end_ind = end_ind.item()\n",
    "                    else:\n",
    "                        start_ind = int(start_ind.numpy())\n",
    "                        end_ind = int(end_ind.numpy())\n",
    "                word_ref = sentence[start_ind:end_ind]\n",
    "                if getattr(self.tokenizer._tokenizer.model, \"continuing_subword_prefix\", None):\n",
    "                    # This is a BPE, word aware tokenizer, there is a correct way\n",
    "                    # to fuse tokens\n",
    "                    is_subword = len(word) != len(word_ref)\n",
    "                else:\n",
    "                    # This is a fallback heuristic. This will fail most likely on any kind of text + punctuation mixtures that will be considered \"words\". Non word aware models cannot do better than this unfortunately.\n",
    "                    if aggregation_strategy in {\n",
    "                        AggregationStrategy.FIRST,\n",
    "                        AggregationStrategy.AVERAGE,\n",
    "                        AggregationStrategy.MAX,\n",
    "                    }:\n",
    "                        warnings.warn(\"Tokenizer does not support real words, using fallback heuristic\", UserWarning)\n",
    "                    is_subword = start_ind > 0 and \" \" not in sentence[start_ind - 1 : start_ind + 1]\n",
    "\n",
    "                if int(input_ids[idx]) == self.tokenizer.unk_token_id:\n",
    "                    word = word_ref\n",
    "                    is_subword = False\n",
    "            else:\n",
    "                start_ind = None\n",
    "                end_ind = None\n",
    "                is_subword = False\n",
    "\n",
    "            pre_entity = {\n",
    "                \"word\": word,\n",
    "                \"scores\": token_scores,\n",
    "                \"start\": start_ind,\n",
    "                \"end\": end_ind,\n",
    "                \"index\": idx,\n",
    "                \"is_subword\": is_subword,\n",
    "            }\n",
    "            pre_entities.append(pre_entity)\n",
    "        return pre_entities\n",
    "\n",
    "    def aggregate(self, pre_entities: List[dict], aggregation_strategy: AggregationStrategy) -> List[dict]:\n",
    "        if aggregation_strategy in {AggregationStrategy.NONE, AggregationStrategy.SIMPLE}:\n",
    "            entities = []\n",
    "            for pre_entity in pre_entities:\n",
    "                entity_idx = pre_entity[\"scores\"].argmax()\n",
    "                score = pre_entity[\"scores\"][entity_idx]\n",
    "                entity = {\n",
    "                    \"entity\": self.model.config.id2label[entity_idx],\n",
    "                    \"score\": score,\n",
    "                    \"index\": pre_entity[\"index\"],\n",
    "                    \"word\": pre_entity[\"word\"],\n",
    "                    \"start\": pre_entity[\"start\"],\n",
    "                    \"end\": pre_entity[\"end\"],\n",
    "                }\n",
    "                entities.append(entity)\n",
    "        else:\n",
    "            entities = self.aggregate_words(pre_entities, aggregation_strategy)\n",
    "\n",
    "        if aggregation_strategy == AggregationStrategy.NONE:\n",
    "            return entities\n",
    "        return self.group_entities(entities)\n",
    "\n",
    "    def aggregate_word(self, entities: List[dict], aggregation_strategy: AggregationStrategy) -> dict:\n",
    "        word = self.tokenizer.convert_tokens_to_string([entity[\"word\"] for entity in entities])\n",
    "        if aggregation_strategy == AggregationStrategy.FIRST:\n",
    "            scores = entities[0][\"scores\"]\n",
    "            idx = scores.argmax()\n",
    "            score = scores[idx]\n",
    "            entity = self.model.config.id2label[idx]\n",
    "        elif aggregation_strategy == AggregationStrategy.MAX:\n",
    "            max_entity = max(entities, key=lambda entity: entity[\"scores\"].max())\n",
    "            scores = max_entity[\"scores\"]\n",
    "            idx = scores.argmax()\n",
    "            score = scores[idx]\n",
    "            entity = self.model.config.id2label[idx]\n",
    "        elif aggregation_strategy == AggregationStrategy.AVERAGE:\n",
    "            scores = np.stack([entity[\"scores\"] for entity in entities])\n",
    "            average_scores = np.nanmean(scores, axis=0)\n",
    "            entity_idx = average_scores.argmax()\n",
    "            entity = self.model.config.id2label[entity_idx]\n",
    "            score = average_scores[entity_idx]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation_strategy\")\n",
    "        new_entity = {\n",
    "            \"entity\": entity,\n",
    "            \"score\": score,\n",
    "            \"word\": word,\n",
    "            \"start\": entities[0][\"start\"],\n",
    "            \"end\": entities[-1][\"end\"],\n",
    "        }\n",
    "        return new_entity\n",
    "\n",
    "    def aggregate_words(self, entities: List[dict], aggregation_strategy: AggregationStrategy) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Override tokens from a given word that disagree to force agreement on word boundaries.\n",
    "        Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft|\n",
    "        company| B-ENT I-ENT\n",
    "        \"\"\"\n",
    "        if aggregation_strategy in {\n",
    "            AggregationStrategy.NONE,\n",
    "            AggregationStrategy.SIMPLE,\n",
    "        }:\n",
    "            raise ValueError(\"NONE and SIMPLE strategies are invalid for word aggregation\")\n",
    "\n",
    "        word_entities = []\n",
    "        word_group = None\n",
    "        for entity in entities:\n",
    "            if word_group is None:\n",
    "                word_group = [entity]\n",
    "            elif entity[\"is_subword\"]:\n",
    "                word_group.append(entity)\n",
    "            else:\n",
    "                word_entities.append(self.aggregate_word(word_group, aggregation_strategy))\n",
    "                word_group = [entity]\n",
    "        # Last item\n",
    "        word_entities.append(self.aggregate_word(word_group, aggregation_strategy))\n",
    "        return word_entities\n",
    "\n",
    "    def group_sub_entities(self, entities: List[dict]) -> dict:\n",
    "        \"\"\"\n",
    "        Group together the adjacent tokens with the same entity predicted.\n",
    "        Args:\n",
    "            entities (`dict`): The entities predicted by the pipeline.\n",
    "        \"\"\"\n",
    "        # Get the first entity in the entity group\n",
    "        entity = entities[0][\"entity\"].split(\"-\")[-1]\n",
    "        scores = np.nanmean([entity[\"score\"] for entity in entities])\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "\n",
    "        entity_group = {\n",
    "            \"entity_group\": entity,\n",
    "            \"score\": np.mean(scores),\n",
    "            \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"start\": entities[0][\"start\"],\n",
    "            \"end\": entities[-1][\"end\"],\n",
    "        }\n",
    "        return entity_group\n",
    "\n",
    "    def get_tag(self, entity_name: str) -> Tuple[str, str]:\n",
    "        if entity_name.startswith(\"B-\"):\n",
    "            bi = \"B\"\n",
    "            tag = entity_name[2:]\n",
    "        elif entity_name.startswith(\"I-\"):\n",
    "            bi = \"I\"\n",
    "            tag = entity_name[2:]\n",
    "        else:\n",
    "            # It's not in B-, I- format\n",
    "            # Default to I- for continuation.\n",
    "            bi = \"I\"\n",
    "            tag = entity_name\n",
    "        return bi, tag\n",
    "\n",
    "    def group_entities(self, entities: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Find and group together the adjacent tokens with the same entity predicted.\n",
    "        Args:\n",
    "            entities (`dict`): The entities predicted by the pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "\n",
    "        for entity in entities:\n",
    "            if not entity_group_disagg:\n",
    "                entity_group_disagg.append(entity)\n",
    "                continue\n",
    "\n",
    "            # If the current entity is similar and adjacent to the previous entity,\n",
    "            # append it to the disaggregated entity group\n",
    "            # The split is meant to account for the \"B\" and \"I\" prefixes\n",
    "            # Shouldn't merge if both entities are B-type\n",
    "            bi, tag = self.get_tag(entity[\"entity\"])\n",
    "            last_bi, last_tag = self.get_tag(entity_group_disagg[-1][\"entity\"])\n",
    "\n",
    "            if tag == last_tag and bi != \"B\":\n",
    "                # Modify subword type to be previous_type\n",
    "                entity_group_disagg.append(entity)\n",
    "            else:\n",
    "                # If the current entity is different from the previous entity\n",
    "                # aggregate the disaggregated entity group\n",
    "                entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "                entity_group_disagg = [entity]\n",
    "        if entity_group_disagg:\n",
    "            # it's the last entity, add it to the entity groups\n",
    "            entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "\n",
    "        return entity_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf59a6",
   "metadata": {},
   "source": [
    "## Function to put color tags in HTML text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789c61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML as html_print\n",
    "\n",
    "def color_string(s, color='black'):\n",
    "    return \"<text style=background-color:{};weight:b>{}</text>\".format(color, s)\n",
    "def normal_string(s, color='black'):\n",
    "    return \"<text style=color:{};weight:b>{}</text>\".format(color, s)\n",
    "\n",
    "def get_predicted_panels(example):\n",
    "    generation = tagger(\" \".join(example[\"words\"]))\n",
    "    list_ = []\n",
    "    for item in generation:\n",
    "        if item[\"entity\"] == \"B-PANEL_START\":\n",
    "            list_.append(item)\n",
    "    return list_\n",
    "\n",
    "def get_labeled_panels(example):\n",
    "    chars = 0\n",
    "    list_ = []\n",
    "    for idx, i in enumerate(example[\"labels\"]):\n",
    "        word = example[\"words\"][idx]\n",
    "        if i == 1:\n",
    "            list_.append({\n",
    "                \"index\": idx,\n",
    "                \"start\": chars\n",
    "            })\n",
    "        chars += len(word) + 1\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d61cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(idx):\n",
    "    example = ds[idx]\n",
    "\n",
    "    predictions = get_predicted_panels(example)\n",
    "    labels = get_labeled_panels(example)\n",
    "    text = \" \".join(example[\"words\"])\n",
    "\n",
    "    chars = 0\n",
    "    text = \"\"\n",
    "    for word in example[\"words\"]:\n",
    "        if chars in [d['start'] for d in predictions if 'start' in d] and chars not in [d['start'] for d in labels if 'start' in d]:\n",
    "            text += color_string(word, color='cyan')\n",
    "        elif chars in [d['start'] for d in predictions if 'start' in d] and chars in [d['start'] for d in labels if 'start' in d]:\n",
    "            text += color_string(word, color='lime')\n",
    "        elif chars not in [d['start'] for d in predictions if 'start' in d] and chars in [d['start'] for d in labels if 'start' in d]:\n",
    "            text += color_string(word, color='red')\n",
    "        else:\n",
    "            text += normal_string(word, color='black')\n",
    "        text += \" \"\n",
    "        chars += len(word) + 1\n",
    "        \n",
    "    return text, predictions, labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056edc8c",
   "metadata": {},
   "source": [
    "## Visualizing the panelization task\n",
    "\n",
    "Below can be shown the prediction of our machine learning model on the figure legend when the panelization task is applied. Color codes are as following:\n",
    "\n",
    "* <font color='lime'>Green (True positive)</font> - Correct prediction\n",
    "* <font color='red'>Red (False negative)</font>   - The prediction missed a panel start present in the labelled data\n",
    "* <font color='cyan'>Cyan (False positive or bad labelling)</font>  - The prediction shows a positive panel not shown in the labelled data\n",
    "* Black - true negatives :-)\n",
    "\n",
    "Modifying `EXAMPLE` will show how the model works in different examples of our test dataset. Any integer number < 307 will work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29b60ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 10:02:07.788029 140395753617216 builder.py:532] Reusing dataset source_data_nlp (/root/.cache/huggingface/datasets/EMBO___source_data_nlp/PANELIZATION/2.0.0/697847190b4f17eb8b2bf15419fdd4e8cacc32bc57734ea9909016d809b4eb55)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0313f01387054affb738f9fd1232eecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"EMBO/sd-nlp-v2\", \"PANELIZATION\")[\"test\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EMBO/sd-panelization-v2\", \n",
    "                                          model_max_length=512,\n",
    "                                         add_prefix_space=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"EMBO/sd-panelization-v2\")\n",
    "\n",
    "tagger = LongTextTokenClassificationPipeline(task=\"token-classification\", \n",
    "                     model=model, \n",
    "                     tokenizer=tokenizer,\n",
    "                     device=0,\n",
    "                     aggregation_strategy=\"none\")\n",
    "\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512,'return_tensors':'pt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9414d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb9bdf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:black;weight:b>LC3</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>accumulate</text> <text style=color:black;weight:b>in</text> <text style=color:black;weight:b>IBMPFD</text> <text style=color:black;weight:b>muscle</text> <text style=color:black;weight:b>tissue</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>Figure</text> <text style=color:black;weight:b>1</text> <text style=color:black;weight:b>.</text> <text style=background-color:lime;weight:b>(</text> <text style=color:black;weight:b>A</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>B</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>Immunoblot</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>9</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>mo</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>old</text> <text style=color:black;weight:b>quadricepsmuscle</text> <text style=color:black;weight:b>lysates</text> <text style=color:black;weight:b>from</text> <text style=color:black;weight:b>control</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>cont</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>VCP</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>WT</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>RH9</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>RH12</text> <text style=color:black;weight:b>mutant</text> <text style=color:black;weight:b>transgenic</text> <text style=color:black;weight:b>lines</text> <text style=color:black;weight:b>with</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>A</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>LC3</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>B</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>antibodies</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>Note</text> <text style=color:black;weight:b>the</text> <text style=color:black;weight:b>increase</text> <text style=color:black;weight:b>in</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>LC3II</text> <text style=color:black;weight:b>isoforms</text> <text style=color:black;weight:b>in</text> <text style=color:black;weight:b>mutant</text> <text style=color:black;weight:b>animals</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>Densitometric</text> <text style=color:black;weight:b>quantification</text> <text style=color:black;weight:b>is</text> <text style=color:black;weight:b>from</text> <text style=color:black;weight:b>more</text> <text style=color:black;weight:b>than</text> <text style=color:black;weight:b>six</text> <text style=color:black;weight:b>9</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>mo</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>old</text> <text style=color:black;weight:b>animals</text> <text style=color:black;weight:b>/</text> <text style=color:black;weight:b>group</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>LC3</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>levels</text> <text style=color:black;weight:b>are</text> <text style=color:black;weight:b>normalized</text> <text style=color:black;weight:b>to</text> <text style=color:black;weight:b>loading</text> <text style=color:black;weight:b>control</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>Error</text> <text style=color:black;weight:b>bars</text> <text style=color:black;weight:b>represent</text> <text style=color:black;weight:b>the</text> <text style=color:black;weight:b>standard</text> <text style=color:black;weight:b>error</text> <text style=color:black;weight:b>from</text> <text style=color:black;weight:b>six</text> <text style=color:black;weight:b>independent</text> <text style=color:black;weight:b>experiments</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>*</text> <text style=color:black;weight:b>*</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>P</text> <text style=color:black;weight:b><</text> <text style=color:black;weight:b>0</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>001</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>AU</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>arbitrary</text> <text style=color:black;weight:b>unit</text> <text style=color:black;weight:b>.</text> <text style=background-color:lime;weight:b>(</text> <text style=color:black;weight:b>C</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>immunostaining</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>tibialis</text> <text style=color:black;weight:b>anterior</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>TA</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>quadriceps</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>Quad</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>muscle</text> <text style=color:black;weight:b>from</text> <text style=color:black;weight:b>9</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>mo</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>old</text> <text style=color:black;weight:b>control</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>VCP</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>WT</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>one</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>two</text> <text style=color:black;weight:b>VCP</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>RH</text> <text style=color:black;weight:b>transgenic</text> <text style=color:black;weight:b>lines</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>RH9</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>RH12</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>Note</text> <text style=color:black;weight:b>accumulations</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>in</text> <text style=color:black;weight:b>the</text> <text style=color:black;weight:b>middle</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>myofibers</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>along</text> <text style=color:black;weight:b>subsarcolemmal</text> <text style=color:black;weight:b>regions</text> <text style=color:black;weight:b>.</text> <text style=background-color:lime;weight:b>(</text> <text style=color:black;weight:b>D</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>Histochemistry</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>immunohistochemistry</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>quadriceps</text> <text style=color:black;weight:b>muscle</text> <text style=color:black;weight:b>from</text> <text style=color:black;weight:b>VCP</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>RH</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>expressing</text> <text style=color:black;weight:b>transgenic</text> <text style=color:black;weight:b>mice</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>Hematoxylin</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>eosin</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>H</text> <text style=color:black;weight:b>&</text> <text style=color:black;weight:b>amp</text> <text style=color:black;weight:b>;</text> <text style=color:black;weight:b>amp</text> <text style=color:black;weight:b>;</text> <text style=color:black;weight:b>E</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>staining</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>a</text> <text style=color:black;weight:b>15</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>mo</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>old</text> <text style=color:black;weight:b>animal</text> <text style=color:black;weight:b>with</text> <text style=color:black;weight:b>an</text> <text style=color:black;weight:b>RV</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>immunostaining</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>an</text> <text style=color:black;weight:b>RV</text> <text style=color:black;weight:b>from</text> <text style=color:black;weight:b>a</text> <text style=color:black;weight:b>12</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>mo</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>old</text> <text style=color:black;weight:b>animal</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>LC3</text> <text style=color:black;weight:b>immunostaining</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>an</text> <text style=color:black;weight:b>RV</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>subsarcolemmal</text> <text style=color:black;weight:b>accumulations</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>LC3</text> <text style=color:black;weight:b>from</text> <text style=color:black;weight:b>a</text> <text style=color:black;weight:b>12</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>mo</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>old</text> <text style=color:black;weight:b>animal</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>The</text> <text style=color:black;weight:b>bracket</text> <text style=color:black;weight:b>highlights</text> <text style=color:black;weight:b>one</text> <text style=color:black;weight:b>LC3</text> <text style=color:black;weight:b>-</text> <text style=color:black;weight:b>positive</text> <text style=color:black;weight:b>myofiber</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>A</text> <text style=color:black;weight:b>single</text> <text style=color:black;weight:b>muscle</text> <text style=color:black;weight:b>fiber</text> <text style=color:black;weight:b>is</text> <text style=color:black;weight:b>outlined</text> <text style=color:black;weight:b>in</text> <text style=color:black;weight:b>white</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>(</text> <text style=color:black;weight:b>C</text> <text style=color:black;weight:b>and</text> <text style=color:black;weight:b>D</text> <text style=color:black;weight:b>)</text> <text style=color:black;weight:b>Arrows</text> <text style=color:black;weight:b>denote</text> <text style=color:black;weight:b>vacuoles</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>accumulations</text> <text style=color:black;weight:b>of</text> <text style=color:black;weight:b>p62</text> <text style=color:black;weight:b>or</text> <text style=color:black;weight:b>LC3</text> <text style=color:black;weight:b>.</text> <text style=color:black;weight:b>Bars</text> <text style=color:black;weight:b>,</text> <text style=color:black;weight:b>30</text> <text style=color:black;weight:b>µm</text> <text style=color:black;weight:b>.</text> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, predictions_transformer, labels = show_example(EXAMPLE)\n",
    "print(len(tokenizer(ds[EXAMPLE][\"words\"], is_split_into_words=True)[\"input_ids\"]))\n",
    "html_print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61beac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda643c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438d7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
