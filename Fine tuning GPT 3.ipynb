{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa6c81e",
   "metadata": {},
   "source": [
    "Notebook based on the [oficial example](https://github.com/openai/openai-python/blob/main/examples/finetuning/finetuning-classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524e7cf",
   "metadata": {},
   "source": [
    "## Step 1: Use smtag to generate your Seq2seq file into GPT3 supported format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "505965ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong number of task--input in line. Possible lack of \n",
      "                                    delimiter. Skipping line to next one. Tag entities, find causality, and experimental methods:  H, I Blood glucose during  \n",
      "\n",
      "Wrong number of task--input in line. Possible lack of \n",
      "                                    delimiter. Skipping line to next one. Tag entities, find causality, and experimental methods: Figure 5. The anaphase rDNA loop is condensin dependent.\n",
      "\n",
      "Parameter 'function'=<function Gpt3FineTuner._convert_to_gpt at 0x7f720a38c9d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████████████████████████████| 16355/16355 [00:01<00:00, 10079.75ex/s]\n",
      "100%|████████████████████████████████████| 2069/2069 [00:00<00:00, 10181.90ex/s]\n",
      "100%|████████████████████████████████████| 2068/2068 [00:00<00:00, 10169.31ex/s]\n",
      "/app/smtag/train/train_seq2seq.py:199: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use DatasetDict.remove_columns instead.\n",
      "  self.dataset.remove_columns_([\"input\", \"target\"])\n",
      "/data/seq2seq/sd-seq2seq-with-html-tags.csv\n",
      "Creating json from Arrow format: 100%|████████████| 2/2 [00:00<00:00, 12.50ba/s]\n",
      "Creating json from Arrow format: 100%|████████████| 1/1 [00:00<00:00, 58.75ba/s]\n",
      "Creating json from Arrow format: 100%|████████████| 1/1 [00:00<00:00, 59.41ba/s]\n",
      "{'prompt': 'Tag entities, find causality, and experimental methods:  Schematic comparison of mUcp1L and hUcp1. Poly(A) signal (Hex, open box) and CPE (black box) are denoted. The RT-PCR  results from human, WT (Ucp1+/+) and UCP1-KO (Ucp1-/-) BAT RNA samples \\n\\n###\\n\\n ', 'completion': ' <geneprod> ucp1 </geneprod> was tested for its influence on <geneprod> ucp1 </geneprod> by <experiment> rt-pcr </experiment> [END]'}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m smtag.cli.seq2seq.gpt3_finetune \"/data/seq2seq/sd-seq2seq-with-html-tags.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873b0df",
   "metadata": {},
   "source": [
    "### Use the data preparation tool of openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbd90609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1301e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a33b6e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 16355 prompt-completion pairs\n",
      "- There are 44 duplicated prompt-completion sets. These are rows: [256, 400, 804, 1014, 1434, 1559, 2911, 3290, 4152, 4575, 4611, 5220, 5320, 6337, 6344, 6864, 7226, 7928, 8608, 10120, 10409, 11015, 11102, 11483, 11603, 11605, 11894, 11895, 11896, 11909, 11918, 12792, 12801, 13016, 13154, 13636, 14337, 14573, 14777, 15037, 15051, 15300, 15452, 15676]\n",
      "- All prompts end with suffix ` \\n\\n###\\n\\n `\n",
      "- All completions end with suffix ` </experiment> [END]`. This suffix seems very long. Consider replacing with a shorter suffix, such as `\\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 44 duplicate rows [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified file to `/data/seq2seq/gpt_ready_train_prepared.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/data/seq2seq/gpt_ready_train_prepared.jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` \\n\\n###\\n\\n ` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" </experiment> [END]\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 13.2 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f /data/seq2seq/gpt_ready_train.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09aedd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 2069 prompt-completion pairs\n",
      "- There are 1 duplicated prompt-completion sets. These are rows: [1363]\n",
      "- All prompts end with suffix ` \\n\\n###\\n\\n `\n",
      "- All prompts start with prefix `Tag entities, find causality, and experimental methods: `. Fine-tuning doesn't require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion\n",
      "- All completions end with suffix ` </experiment> [END]`. This suffix seems very long. Consider replacing with a shorter suffix, such as `\\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 1 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Remove prefix `Tag entities, find causality, and experimental methods: ` from all prompts [Y/n]: Y\n",
      "/opt/conda/lib/python3.8/site-packages/openai/validators.py:272: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"prompt\"] = x[\"prompt\"].str[len(prefix) :]\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified file to `/data/seq2seq/gpt_ready_eval_prepared.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/data/seq2seq/gpt_ready_eval_prepared.jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` \\n\\n###\\n\\n ` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" </experiment> [END]\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 1.69 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f /data/seq2seq/gpt_ready_eval.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0e6a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /data/seq2seq/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9dac8",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create \\\n",
    "        -t /data/seq2seq/gpt_ready_train_prepared.jsonl \\\n",
    "        -v /data/seq2seq/gpt_ready_eval_prepared.jsonl \\\n",
    "        -m text-davinci-002 \\\n",
    "        --suffix 'sd-gpt-model-' \\\n",
    "        --n_epochs 2 \\\n",
    "        --batch_size 256 \\\n",
    "        --learning_rate_multiplier 0.2 \\\n",
    "        --prompt_loss_weight PROMPT_LOSS_WEIGHT \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.results -i ft-2zaA7qi0rxJduWQpdvOvmGn3 > /data/seq2seq/sd-gpt3-model-result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('/data/seq2seq/sd-gpt3-model-result.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908229a",
   "metadata": {},
   "source": [
    "### Using the model\n",
    "\n",
    "\n",
    "\n",
    "We need to use the same separator following the prompt which we used during fine-tuning. In this case it is \\n\\n###\\n\\n. Since we're concerned with classification, we want the temperature to be as low as possible, and we only require one token completion to determine the prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = 'NAME HERE'\n",
    "res = openai.Completion.create( model=ft_model, \n",
    "                                prompt=test['prompt'][0] + '\\n\\n###\\n\\n', \n",
    "                                max_tokens=1, \n",
    "                                temperature=0,\n",
    "                                top_p=1,\n",
    "                                n=1,\n",
    "                                stream=False,\n",
    "                                echo=False,\n",
    "                                frequency_penalt=0,\n",
    "                                presence_penalty=0,\n",
    "                                stop=[\"[END]\"],\n",
    "                                best_of=1,\n",
    "                                logit_bias=None\n",
    "                            )\n",
    "res['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6d71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94003d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = openai.Completion.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\n",
    "res['choices'][0]['logprobs']['top_logprobs'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
